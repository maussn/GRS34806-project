{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Bioinformatics Project 2025 - Motif CNN & GO Prediction\n",
        "\n",
        "**Course:** GRS34806 Deep Learning\n",
        "\n",
        "**Authors:** ............\n",
        "\n",
        "**Date:**\n",
        "\n"
      ],
      "metadata": {
        "id": "HuP5FaUF6YLo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries"
      ],
      "metadata": {
        "id": "YbtfKAzz6ct5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "! git clone https://git.wur.nl/bioinformatics/grs34806-deep-learning-project-data.git -q\n",
        "! git clone https://github.com/maussn/GRS34806-project.git -q\n",
        "os.chdir(Path('grs34806-deep-learning-project-data'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Emlqnf_rAIWr",
        "outputId": "20ef00e4-950e-46ba-9f37-e407c394e60d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'grs34806-deep-learning-project-data' already exists and is not an empty directory.\n",
            "fatal: destination path 'GRS34806-project' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from datetime import datetime\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import random\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "3ctb6CE_bWXP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data I/O & Tokenisation"
      ],
      "metadata": {
        "id": "g96CRPCRCbdx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. read() --------------------------------------------------------------------\n",
        "def read(seqfile: str, posfile: str) -> tuple[list, list]:\n",
        "    \"\"\"Read file with sequences and file with positive cases into lists.\n",
        "\n",
        "    :param seqfile: file with sequences\n",
        "    :type seqfile: str\n",
        "    :param posfile: file with positive cases (annotated with function)\n",
        "    :type posfile: str\n",
        "    :return: two lists, first with sequences and second with boolean labels\n",
        "    :rtype: tuple[list, list]\n",
        "    \"\"\"\n",
        "    idlist = []\n",
        "    datalist = []\n",
        "    labellist = []\n",
        "    with open(seqfile, 'r') as f:\n",
        "        for line in f.readlines():\n",
        "            line = line.rstrip().split('\\t')\n",
        "            idlist.append(line[0])\n",
        "            datalist.append(line[1])\n",
        "            labellist.append(False)\n",
        "    with open(posfile, 'r') as f:\n",
        "        for line in f.readlines():\n",
        "            id = line.rstrip()\n",
        "            try:\n",
        "                i = idlist.index(id)\n",
        "                labellist[i] = True\n",
        "            except ValueError:\n",
        "                continue\n",
        "    return datalist, labellist\n",
        "\n",
        "\n",
        "# 2. split_labelled() ----------------------------------------------------------\n",
        "def split_labelled(datalist: list, labellist: list):\n",
        "    \"\"\"Return two separate sequence lists: positives & negatives.\"\"\"\n",
        "    pos_datalist = []\n",
        "    neg_datalist = []\n",
        "    for i, label in enumerate(labellist):\n",
        "        if label:\n",
        "            pos_datalist.append(datalist[i])\n",
        "        else:\n",
        "            neg_datalist.append(datalist[i])\n",
        "    return pos_datalist, neg_datalist\n",
        "\n",
        "\n",
        "# 3. remove_sequences() -----\n",
        "def remove_sequences(datalist: list, fraction=0.5):\n",
        "    \"\"\"Randomly keeps half of the list\"\"\"\n",
        "    random.shuffle(datalist)\n",
        "    keep = round(len(datalist) * fraction)\n",
        "    return datalist[:keep]\n",
        "\n",
        "\n",
        "# 4. fuse_sequence_lists() ------------\n",
        "def fuse_sequence_lists(pos_datalist: list, neg_datalist: list):\n",
        "    \"\"\"Merge postives and negetaves into one list + label\"\"\"\n",
        "    pos_labels = [True for _ in pos_datalist]\n",
        "    neg_labels = [False for _ in neg_datalist]\n",
        "    datalist = pos_datalist + neg_datalist\n",
        "    labellist = pos_labels + neg_labels\n",
        "    return datalist, labellist\n",
        "\n",
        "\n",
        "# 5. generate_train_test() --------\n",
        "def generate_train_test(datalist: list, labellist: list, fraction: float=0.8):\n",
        "    \"\"\"Split up dataset in training set and test set\n",
        "\n",
        "    :param datalist: list with sequences\n",
        "    :type datalist: list\n",
        "    :param labellist: list with labels\n",
        "    :type labellist: list\n",
        "    :param ratio: fraction to be added to the training set, remainder is added to the test set, defaults to 0.8\n",
        "    :type ratio: float, optional\n",
        "    :return: four lists, first two the training data and labels, second two the test data and labels\n",
        "    :rtype: tuple[list, list, list, list]\n",
        "    \"\"\"\n",
        "    c = list(zip(datalist, labellist))\n",
        "    random.shuffle(c)\n",
        "    datalist[:], labellist[:] = zip(*c)\n",
        "    i = round(len(datalist) * fraction)\n",
        "    traindatalist = datalist[:i]\n",
        "    trainlabellist = labellist[:i]\n",
        "    testdatalist = datalist[i:]\n",
        "    testlabellist = labellist[i:]\n",
        "    return traindatalist, trainlabellist,testdatalist,testlabellist\n",
        "\n",
        "\n",
        "# 6. Tokenisation & Padding --------\n",
        "def tokenize(data: list, map2num: dict, non_aa_num: int=20) -> list:\n",
        "    \"\"\"Tokenize all sequences in a list\n",
        "\n",
        "    :param data: list of sequences to tokenize\n",
        "    :type data: list\n",
        "    :param map2num: ammino acid -> integer token mapping\n",
        "    :type map2num: dict\n",
        "    :param non_aa_num: token for non amino acid characters, defaults to 20\n",
        "    :type non_aa_num: int, optional\n",
        "    :return: list of tokenized sequences\n",
        "    :rtype: list\n",
        "    \"\"\"\n",
        "    seq = []\n",
        "    for count, i in enumerate(data):\n",
        "        seq.append([map2num.get(j,non_aa_num) for j in list(i)])\n",
        "    return seq\n",
        "\n",
        "\n",
        "def truncate_pad(line: list, num_steps: int, padding_token: int) -> list:\n",
        "    \"\"\"Truncate or pad a tokenized sequence\n",
        "\n",
        "    :param line: tokenized sequence\n",
        "    :type line: list\n",
        "    :param num_steps: maximum sequence length\n",
        "    :type num_steps: int\n",
        "    :param padding_token: token to be used for padding\n",
        "    :type padding_token: int\n",
        "    :return: truncated/padded sequence\n",
        "    :rtype: list\n",
        "    \"\"\"\n",
        "    if len(line) > num_steps:\n",
        "        return line[:num_steps] # Truncate\n",
        "    return line + [padding_token] * (num_steps - len(line)) # Pad\n",
        "\n",
        "\n",
        "def build_seq_array(lines: list, num_steps: int, non_aa_num: int=20) -> torch.tensor:\n",
        "    \"\"\"Truncate or pad tokenized sequences and convert to tensor\n",
        "\n",
        "    :param lines: tokenized sequences\n",
        "    :type lines: list\n",
        "    :param num_steps: maximum sequence length\n",
        "    :type num_steps: int\n",
        "    :param non_aa_num: token for padding, defaults to 20\n",
        "    :type non_aa_num: int, optional\n",
        "    :return: tensor with truncated/padded tokenized sequences\n",
        "    :rtype: torch.tensor\n",
        "    \"\"\"\n",
        "    return torch.tensor([truncate_pad(l, num_steps, non_aa_num) for l in lines], dtype=torch.long)\n",
        "\n",
        "\n",
        "# 7. load_array() & load_data()\n",
        "def load_array(data_arrays: tuple[torch.tensor, torch.tensor], batch_size: int, is_train: bool=True) -> torch.utils.data.DataLoader:\n",
        "    \"\"\"Construct a PyTorch data iterator.\n",
        "\n",
        "    Taken from d2l package\"\"\"\n",
        "    dataset = torch.utils.data.TensorDataset(*data_arrays)\n",
        "    return torch.utils.data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
        "\n",
        "\n",
        "def load_data(batch_size: int, num_steps: int, dataset: tuple[list, list]) -> torch.utils.data.DataLoader:\n",
        "    \"\"\"Tokenize sequence/label dataset and load into dataloader.\n",
        "\n",
        "    :param batch_size: size of each batch\n",
        "    :type batch_size: int\n",
        "    :param num_steps: maximum sequence length\n",
        "    :type num_steps: int\n",
        "    :param dataset: first list contains sequences, second labels\n",
        "    :type dataset: tuple[list, list]\n",
        "    :return: torch dataloader which gives a tensor of sequences in a batch and a tensor with their labels\n",
        "    :rtype: torch.utils.data.DataLoader\n",
        "    \"\"\"\n",
        "    mapaa2num = {aa: i for (i, aa) in enumerate(list(\"ACDEFGHIKLMNPQRSTVWY\"))}\n",
        "    seq,lab = dataset\n",
        "    seq = tokenize(seq, mapaa2num)\n",
        "    seq_array = build_seq_array(seq, num_steps)\n",
        "    data_arrays = (seq_array, torch.tensor(lab, dtype=torch.long))\n",
        "    data_iter = load_array(data_arrays, batch_size)\n",
        "    return data_iter"
      ],
      "metadata": {
        "id": "eHpedW0hqx1V"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset selector\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ChrVqNAqKXEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_id = \"len100_200_n1000\"  # select any of the 6's IDs\n",
        "REMOVE = None                    # None | \"neg\" | \"pos\"\n",
        "\n",
        "seq_path  = f\"{dataset_id}.seq\"\n",
        "pos_path  = f\"{dataset_id}.pos\"\n",
        "datalist, labellist = read(seq_path, pos_path)\n",
        "\n",
        "# Removing half operation\n",
        "if REMOVE is not None:\n",
        "    pos_datalist, neg_datalist = split_labelled(datalist, labellist)\n",
        "    if REMOVE == \"neg\":\n",
        "        neg_datalist = remove_sequences(neg_datalist, 0.5)\n",
        "    elif REMOVE == \"pos\":\n",
        "        pos_datalist = remove_sequences(pos_datalist, 0.5)\n",
        "    datalist, labellist = fuse_sequence_lists(pos_datalist, neg_datalist)"
      ],
      "metadata": {
        "id": "yDEFWAwbK_s3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loader"
      ],
      "metadata": {
        "id": "_b5Kq5jEKZwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "num_steps = 500 # max sequence length\n",
        "\n",
        "traindatalist, trainlabellist, testdatalist, testlabellist = generate_train_test(datalist, labellist, 0.8)\n",
        "traindataset = [traindatalist, trainlabellist]\n",
        "testdataset = [testdatalist, testlabellist]\n",
        "\n",
        "# Set batch_size and num_steps (maximum sequence length)\n",
        "train_iter = load_data(batch_size, num_steps, traindataset)\n",
        "test_iter = load_data(batch_size, num_steps, testdataset)\n",
        "\n",
        "print(\"batch shape  :\", next(iter(train_iter))[0].shape)\n",
        "print(\"labels shape :\", next(iter(train_iter))[1].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyRvhoyiwv_g",
        "outputId": "1ac5de2b-4efc-43d2-b437-f815b0c6b00b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch shape  : torch.Size([10, 500])\n",
            "labels shape : torch.Size([10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training / Evaluation"
      ],
      "metadata": {
        "id": "BhuHUN6TRLun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "YwR8PJG9SYbo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(layer):\n",
        "    if type(layer) == nn.Linear or type(layer) == nn.Conv1d:\n",
        "        nn.init.xavier_uniform_(layer.weight)\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model, loss_fn, optimizer, device='cuda'):\n",
        "        self.model = model.to(device)\n",
        "        self.loss_fn = loss_fn\n",
        "        self.optimizer = optimizer\n",
        "        self.device = device\n",
        "\n",
        "    # one epoch\n",
        "    def _train_one_epoch(self, train_iter):\n",
        "        \"\"\"\n",
        "        Run one epoch and return mean epoch loss.\n",
        "        \"\"\"\n",
        "        self.model.train()\n",
        "        epoch_loss= 0.0\n",
        "        n_steps = 0\n",
        "\n",
        "        for inputs, labels in train_iter:\n",
        "            inputs = inputs.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            outputs = self.model(inputs)\n",
        "            loss = self.loss_fn(outputs, labels)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            n_steps += 1\n",
        "\n",
        "        return epoch_loss / n_steps   # mean epoch loss\n",
        "\n",
        "\n",
        "    # full training\n",
        "\n",
        "    def train(self, epochs, train_iter, test_iter):\n",
        "        \"\"\"\n",
        "        Finds the best accuracy observed.\n",
        "        \"\"\"\n",
        "        best_acc = 0.0\n",
        "\n",
        "        for epoch in range(1, epochs + 1):\n",
        "            train_loss = self._train_one_epoch(train_iter)\n",
        "\n",
        "            test_loss, test_acc, test_prec, test_rec, test_f1 = \\\n",
        "                    self.evaluate(test_iter, calc_loss=True)\n",
        "\n",
        "            print(f\"[epoch {epoch:02d}] \"\n",
        "                  f\"train‑loss={train_loss:.4f} | \"\n",
        "                  f\"test‑loss={test_loss:.4f} | \"\n",
        "                  f\"test‑acc={test_acc:.4f} | \"\n",
        "                  f\"P={test_prec:.4f} | R={test_rec:.4f} | F1={test_f1:.4f}\")\n",
        "\n",
        "            if test_acc > best_acc:\n",
        "                best_acc = test_acc\n",
        "\n",
        "        return best_acc\n",
        "\n",
        "\n",
        "##### dit is gemaakt door AI!########### CHANGE\n",
        "    def evaluate(self, data_iter, calc_loss=False):\n",
        "        \"\"\"\n",
        "        Returns (loss, acc, prec, rec, f1) **if** calc_loss is True\n",
        "        otherwise returns (0.0, acc, prec, rec, f1)\n",
        "        so the caller can always unpack 5 values.\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        all_pred, all_gold = [], []\n",
        "        loss_total, n_batches = 0.0, 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for x, y in data_iter:\n",
        "                x, y          = x.to(self.device), y.to(self.device)\n",
        "                logits        = self.model(x)\n",
        "                pred          = logits.argmax(dim=1)\n",
        "\n",
        "                all_pred.append(pred.cpu())\n",
        "                all_gold.append(y.cpu())\n",
        "\n",
        "                if calc_loss:\n",
        "                    loss_total += self.loss_fn(logits, y).item()\n",
        "                    n_batches  += 1\n",
        "\n",
        "        all_pred = torch.cat(all_pred)\n",
        "        all_gold = torch.cat(all_gold)\n",
        "\n",
        "        acc  = (all_pred == all_gold).float().mean().item()\n",
        "        prec, rec, f1, _ = precision_recall_fscore_support(\n",
        "            all_gold, all_pred, average='micro'\n",
        "        )\n",
        "\n",
        "        avg_loss = loss_total / n_batches if (calc_loss and n_batches) else 0.0\n",
        "        return avg_loss, acc, prec, rec, f1"
      ],
      "metadata": {
        "id": "3Btdmdhj36zM"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BerryCNN1D(nn.Module):\n",
        "    \"\"\"1D Convolutional Neural Network for protein function classification\"\"\"\n",
        "    def __init__(self, context_size: int, vocab_size: int = 21,\n",
        "                 dropout_rate = 0, conv_channels: int = 128,\n",
        "                 use_bias: bool = False):\n",
        "\n",
        "        super().__init__()\n",
        "        assert context_size % 2 == 0, f'Invalid context_size, {context_size} is not an even number'\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "        # CNN model for binary classification\n",
        "        self.conv1 = nn.Sequential(\n",
        "            # conv block 1\n",
        "            nn.Conv1d(in_channels=vocab_size, out_channels=conv_channels, kernel_size=3, padding='same', bias=use_bias),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            # conv block 2\n",
        "            nn.Conv1d(in_channels=conv_channels, out_channels=conv_channels, kernel_size=3, padding='same', bias=use_bias),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveMaxPool1d(1)\n",
        "        )\n",
        "\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Sequential(\n",
        "            # flatten + classification head\n",
        "            nn.Flatten(1, -1),\n",
        "            nn.LazyLinear(out_features=2, bias=use_bias)  # binary classification\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.tensor, targets: torch.tensor = None):\n",
        "        \"\"\"Predict protein function class (0 or 1)\"\"\"\n",
        "        x = self.embedding(x).permute(0,2,1)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.dropout(x)\n",
        "        return self.fc(x)\n"
      ],
      "metadata": {
        "id": "2V5v63DoLaLr"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting the CNN\n",
        "net = BerryCNN1D(context_size=num_steps, vocab_size=21, conv_channels=128)"
      ],
      "metadata": {
        "id": "5Tq5vt3a_afn"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Model"
      ],
      "metadata": {
        "id": "UeAeB8Wo79Oj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "param_grid = {\n",
        "    'dropout_rate': [0, 0.3, 0.6],\n",
        "    'lr': [0.01, 0.001],\n",
        "    'momentum': [0, 0.5, 0.9]\n",
        "}\n",
        "\n",
        "grid = list(ParameterGrid(param_grid))\n",
        "\n",
        "best_acc = 0\n",
        "best_params = None\n",
        "\n",
        "for params in grid:\n",
        "    model = BerryCNN1D(\n",
        "        context_size=num_steps,\n",
        "        vocab_size=21,\n",
        "        dropout_rate=params['dropout_rate'],\n",
        "        conv_channels=128\n",
        "    )\n",
        "    model.apply(init_weights)\n",
        "\n",
        "    optimizer = torch.optim.SGD(\n",
        "        model.parameters(),\n",
        "        lr=params['lr'],\n",
        "        momentum=params['momentum']\n",
        "    )\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    trainer = Trainer(model, loss_fn, optimizer)\n",
        "\n",
        "    acc = trainer.train(epochs=5, train_iter=train_iter,\n",
        "                    test_iter=test_iter)\n",
        "\n",
        "    if acc > best_acc:\n",
        "        best_acc    = acc\n",
        "        best_params = params\n",
        "        print(f\"New best accuracy {best_acc:.4f} with {best_params}\")\n",
        "\n",
        "print(\"Best hyper‑parameters found:\", best_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2M-vmMR4EuP",
        "outputId": "9eec0690-56db-4b4b-9245-66460610448e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 01] train‑loss=0.5849 | test‑loss=0.3866 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 02] train‑loss=0.1818 | test‑loss=0.0800 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 03] train‑loss=0.0550 | test‑loss=0.0346 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 04] train‑loss=0.0284 | test‑loss=0.0213 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 05] train‑loss=0.0183 | test‑loss=0.0137 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "New best accuracy 1.0000 with {'dropout_rate': 0, 'lr': 0.01, 'momentum': 0}\n",
            "[epoch 01] train‑loss=0.5275 | test‑loss=0.1575 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 02] train‑loss=0.0638 | test‑loss=0.0278 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 03] train‑loss=0.0176 | test‑loss=0.0130 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 04] train‑loss=0.0093 | test‑loss=0.0082 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 05] train‑loss=0.0062 | test‑loss=0.0058 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 01] train‑loss=0.2813 | test‑loss=0.0027 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 02] train‑loss=0.0019 | test‑loss=0.0016 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 03] train‑loss=0.0011 | test‑loss=0.0010 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 04] train‑loss=0.0008 | test‑loss=0.0008 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 05] train‑loss=0.0007 | test‑loss=0.0006 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 01] train‑loss=0.6990 | test‑loss=0.6919 | test‑acc=0.4950 | P=0.4950 | R=0.4950 | F1=0.4950\n",
            "[epoch 02] train‑loss=0.6924 | test‑loss=0.7025 | test‑acc=0.4500 | P=0.4500 | R=0.4500 | F1=0.4500\n",
            "[epoch 03] train‑loss=0.6894 | test‑loss=0.6878 | test‑acc=0.5400 | P=0.5400 | R=0.5400 | F1=0.5400\n",
            "[epoch 04] train‑loss=0.6862 | test‑loss=0.6855 | test‑acc=0.5450 | P=0.5450 | R=0.5450 | F1=0.5450\n",
            "[epoch 05] train‑loss=0.6826 | test‑loss=0.6844 | test‑acc=0.5850 | P=0.5850 | R=0.5850 | F1=0.5850\n",
            "[epoch 01] train‑loss=0.7008 | test‑loss=0.6776 | test‑acc=0.6150 | P=0.6150 | R=0.6150 | F1=0.6150\n",
            "[epoch 02] train‑loss=0.6764 | test‑loss=0.6620 | test‑acc=0.6500 | P=0.6500 | R=0.6500 | F1=0.6500\n",
            "[epoch 03] train‑loss=0.6409 | test‑loss=0.6386 | test‑acc=0.5800 | P=0.5800 | R=0.5800 | F1=0.5800\n",
            "[epoch 04] train‑loss=0.5893 | test‑loss=0.5416 | test‑acc=0.9950 | P=0.9950 | R=0.9950 | F1=0.9950\n",
            "[epoch 05] train‑loss=0.4958 | test‑loss=0.4395 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 01] train‑loss=0.6805 | test‑loss=0.6046 | test‑acc=0.7800 | P=0.7800 | R=0.7800 | F1=0.7800\n",
            "[epoch 02] train‑loss=0.3949 | test‑loss=0.1856 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 03] train‑loss=0.0943 | test‑loss=0.0527 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 04] train‑loss=0.0336 | test‑loss=0.0261 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 05] train‑loss=0.0191 | test‑loss=0.0167 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 01] train‑loss=0.7158 | test‑loss=0.6879 | test‑acc=0.4500 | P=0.4500 | R=0.4500 | F1=0.4500\n",
            "[epoch 02] train‑loss=0.5787 | test‑loss=0.3884 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 03] train‑loss=0.2998 | test‑loss=0.1378 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 04] train‑loss=0.1619 | test‑loss=0.0651 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 05] train‑loss=0.1123 | test‑loss=0.0390 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 01] train‑loss=0.6248 | test‑loss=0.2836 | test‑acc=0.9850 | P=0.9850 | R=0.9850 | F1=0.9850\n",
            "[epoch 02] train‑loss=0.1820 | test‑loss=0.0319 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 03] train‑loss=0.0625 | test‑loss=0.0146 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 04] train‑loss=0.0276 | test‑loss=0.0056 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 05] train‑loss=0.0268 | test‑loss=0.0049 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 01] train‑loss=0.3655 | test‑loss=0.0048 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 02] train‑loss=0.0126 | test‑loss=0.0017 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 03] train‑loss=0.0126 | test‑loss=0.0039 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 04] train‑loss=0.0037 | test‑loss=0.0010 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 05] train‑loss=0.0056 | test‑loss=0.0008 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 01] train‑loss=0.7432 | test‑loss=0.6876 | test‑acc=0.5450 | P=0.5450 | R=0.5450 | F1=0.5450\n",
            "[epoch 02] train‑loss=0.7168 | test‑loss=0.6894 | test‑acc=0.5350 | P=0.5350 | R=0.5350 | F1=0.5350\n",
            "[epoch 03] train‑loss=0.7231 | test‑loss=0.7047 | test‑acc=0.4650 | P=0.4650 | R=0.4650 | F1=0.4650\n",
            "[epoch 04] train‑loss=0.7145 | test‑loss=0.6925 | test‑acc=0.5300 | P=0.5300 | R=0.5300 | F1=0.5300\n",
            "[epoch 05] train‑loss=0.7157 | test‑loss=0.6871 | test‑acc=0.5800 | P=0.5800 | R=0.5800 | F1=0.5800\n",
            "[epoch 01] train‑loss=0.7172 | test‑loss=0.6816 | test‑acc=0.5750 | P=0.5750 | R=0.5750 | F1=0.5750\n",
            "[epoch 02] train‑loss=0.7058 | test‑loss=0.6773 | test‑acc=0.6100 | P=0.6100 | R=0.6100 | F1=0.6100\n",
            "[epoch 03] train‑loss=0.7032 | test‑loss=0.6699 | test‑acc=0.5600 | P=0.5600 | R=0.5600 | F1=0.5600\n",
            "[epoch 04] train‑loss=0.6526 | test‑loss=0.6286 | test‑acc=0.8250 | P=0.8250 | R=0.8250 | F1=0.8250\n",
            "[epoch 05] train‑loss=0.6166 | test‑loss=0.5748 | test‑acc=0.9800 | P=0.9800 | R=0.9800 | F1=0.9800\n",
            "[epoch 01] train‑loss=0.7423 | test‑loss=0.6801 | test‑acc=0.5550 | P=0.5550 | R=0.5550 | F1=0.5550\n",
            "[epoch 02] train‑loss=0.6790 | test‑loss=0.6197 | test‑acc=0.8850 | P=0.8850 | R=0.8850 | F1=0.8850\n",
            "[epoch 03] train‑loss=0.5041 | test‑loss=0.3359 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 04] train‑loss=0.2761 | test‑loss=0.1104 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 05] train‑loss=0.1302 | test‑loss=0.0498 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 01] train‑loss=0.7405 | test‑loss=0.6594 | test‑acc=0.6650 | P=0.6650 | R=0.6650 | F1=0.6650\n",
            "[epoch 02] train‑loss=0.6151 | test‑loss=0.4574 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 03] train‑loss=0.4589 | test‑loss=0.2443 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 04] train‑loss=0.3536 | test‑loss=0.1378 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 05] train‑loss=0.2560 | test‑loss=0.0774 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 01] train‑loss=0.7498 | test‑loss=0.6229 | test‑acc=0.5550 | P=0.5550 | R=0.5550 | F1=0.5550\n",
            "[epoch 02] train‑loss=0.4580 | test‑loss=0.1394 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 03] train‑loss=0.2313 | test‑loss=0.0546 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 04] train‑loss=0.1248 | test‑loss=0.0363 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 05] train‑loss=0.0731 | test‑loss=0.0222 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 01] train‑loss=0.4392 | test‑loss=0.0165 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 02] train‑loss=0.0405 | test‑loss=0.0033 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 03] train‑loss=0.0218 | test‑loss=0.0033 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 04] train‑loss=0.0304 | test‑loss=0.0020 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 05] train‑loss=0.0166 | test‑loss=0.0009 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 01] train‑loss=0.7893 | test‑loss=0.6935 | test‑acc=0.5250 | P=0.5250 | R=0.5250 | F1=0.5250\n",
            "[epoch 02] train‑loss=0.8005 | test‑loss=0.6951 | test‑acc=0.4850 | P=0.4850 | R=0.4850 | F1=0.4850\n",
            "[epoch 03] train‑loss=0.7574 | test‑loss=0.6946 | test‑acc=0.4900 | P=0.4900 | R=0.4900 | F1=0.4900\n",
            "[epoch 04] train‑loss=0.7333 | test‑loss=0.6926 | test‑acc=0.5000 | P=0.5000 | R=0.5000 | F1=0.5000\n",
            "[epoch 05] train‑loss=0.7272 | test‑loss=0.6934 | test‑acc=0.5050 | P=0.5050 | R=0.5050 | F1=0.5050\n",
            "[epoch 01] train‑loss=0.7746 | test‑loss=0.6835 | test‑acc=0.5550 | P=0.5550 | R=0.5550 | F1=0.5550\n",
            "[epoch 02] train‑loss=0.7427 | test‑loss=0.7204 | test‑acc=0.4450 | P=0.4450 | R=0.4450 | F1=0.4450\n",
            "[epoch 03] train‑loss=0.7383 | test‑loss=0.6792 | test‑acc=0.6450 | P=0.6450 | R=0.6450 | F1=0.6450\n",
            "[epoch 04] train‑loss=0.7179 | test‑loss=0.6847 | test‑acc=0.5150 | P=0.5150 | R=0.5150 | F1=0.5150\n",
            "[epoch 05] train‑loss=0.6997 | test‑loss=0.6613 | test‑acc=0.7350 | P=0.7350 | R=0.7350 | F1=0.7350\n",
            "[epoch 01] train‑loss=0.8005 | test‑loss=0.6857 | test‑acc=0.5550 | P=0.5550 | R=0.5550 | F1=0.5550\n",
            "[epoch 02] train‑loss=0.7087 | test‑loss=0.8153 | test‑acc=0.4450 | P=0.4450 | R=0.4450 | F1=0.4450\n",
            "[epoch 03] train‑loss=0.6576 | test‑loss=0.5151 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 04] train‑loss=0.4858 | test‑loss=0.2569 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 05] train‑loss=0.3124 | test‑loss=0.1159 | test‑acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "Best hyper‑parameters found: {'dropout_rate': 0, 'lr': 0.01, 'momentum': 0}\n"
          ]
        }
      ]
    }
  ]
}