{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Bioinformatics Project 2025 - Motif CNN & GO Prediction\n",
        "\n",
        "**Course:** GRS34806 Deep Learning\n",
        "\n",
        "**Authors:** Berkay Helvaci & Maurits Naber\n",
        "\n",
        "**Date:**\n",
        "\n"
      ],
      "metadata": {
        "id": "HuP5FaUF6YLo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries"
      ],
      "metadata": {
        "id": "YbtfKAzz6ct5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clean the code last time! also the redundant libraries."
      ],
      "metadata": {
        "id": "amk7R7B2kcma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "! git clone https://git.wur.nl/bioinformatics/grs34806-deep-learning-project-data.git -q\n",
        "! git clone https://github.com/maussn/GRS34806-project.git -q\n",
        "os.chdir(Path('grs34806-deep-learning-project-data'))"
      ],
      "metadata": {
        "id": "Emlqnf_rAIWr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from datetime import datetime\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import random\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "3ctb6CE_bWXP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data I/O & Tokenisation"
      ],
      "metadata": {
        "id": "g96CRPCRCbdx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. read() --------------------------------------------------------------------\n",
        "def read(seqfile: str, posfile: str) -> tuple[list, list]:\n",
        "    \"\"\"Read file with sequences and file with positive cases into lists.\n",
        "\n",
        "    :param seqfile: file with sequences\n",
        "    :type seqfile: str\n",
        "    :param posfile: file with positive cases (annotated with function)\n",
        "    :type posfile: str\n",
        "    :return: two lists, first with sequences and second with boolean labels\n",
        "    :rtype: tuple[list, list]\n",
        "    \"\"\"\n",
        "    idlist = []\n",
        "    datalist = []\n",
        "    labellist = []\n",
        "    with open(seqfile, 'r') as f:\n",
        "        for line in f.readlines():\n",
        "            line = line.rstrip().split('\\t')\n",
        "            idlist.append(line[0])\n",
        "            datalist.append(line[1])\n",
        "            labellist.append(False)\n",
        "    with open(posfile, 'r') as f:\n",
        "        for line in f.readlines():\n",
        "            id = line.rstrip()\n",
        "            try:\n",
        "                i = idlist.index(id)\n",
        "                labellist[i] = True\n",
        "            except ValueError:\n",
        "                continue\n",
        "    return datalist, labellist\n",
        "\n",
        "\n",
        "# 2. split_labelled() ----------------------------------------------------------\n",
        "def split_labelled(datalist: list, labellist: list):\n",
        "    \"\"\"Return two separate sequence lists: positives & negatives.\"\"\"\n",
        "    pos_datalist = []\n",
        "    neg_datalist = []\n",
        "    for i, label in enumerate(labellist):\n",
        "        if label:\n",
        "            pos_datalist.append(datalist[i])\n",
        "        else:\n",
        "            neg_datalist.append(datalist[i])\n",
        "    return pos_datalist, neg_datalist\n",
        "\n",
        "\n",
        "# 3. remove_sequences() -----\n",
        "def remove_sequences(datalist: list, fraction=0.5):\n",
        "    \"\"\"Randomly keeps half of the list\"\"\"\n",
        "    random.shuffle(datalist)\n",
        "    keep = round(len(datalist) * fraction)\n",
        "    return datalist[:keep]\n",
        "\n",
        "\n",
        "# 4. fuse_sequence_lists() ------------\n",
        "def fuse_sequence_lists(pos_datalist: list, neg_datalist: list):\n",
        "    \"\"\"Merge postives and negetaves into one list + label\"\"\"\n",
        "    pos_labels = [True for _ in pos_datalist]\n",
        "    neg_labels = [False for _ in neg_datalist]\n",
        "    datalist = pos_datalist + neg_datalist\n",
        "    labellist = pos_labels + neg_labels\n",
        "    return datalist, labellist\n",
        "\n",
        "\n",
        "# 5. generate_train_test() --------\n",
        "def generate_train_test(datalist: list, labellist: list, fraction: float=0.8):\n",
        "    \"\"\"Split up dataset in training set and test set\n",
        "\n",
        "    :param datalist: list with sequences\n",
        "    :type datalist: list\n",
        "    :param labellist: list with labels\n",
        "    :type labellist: list\n",
        "    :param ratio: fraction to be added to the training set, remainder is added to the test set, defaults to 0.8\n",
        "    :type ratio: float, optional\n",
        "    :return: four lists, first two the training data and labels, second two the test data and labels\n",
        "    :rtype: tuple[list, list, list, list]\n",
        "    \"\"\"\n",
        "    c = list(zip(datalist, labellist))\n",
        "    random.shuffle(c)\n",
        "    datalist[:], labellist[:] = zip(*c)\n",
        "    i = round(len(datalist) * fraction)\n",
        "    traindatalist = datalist[:i]\n",
        "    trainlabellist = labellist[:i]\n",
        "    testdatalist = datalist[i:]\n",
        "    testlabellist = labellist[i:]\n",
        "    return traindatalist, trainlabellist,testdatalist,testlabellist\n",
        "\n",
        "\n",
        "# 6. Tokenisation & Padding --------\n",
        "def tokenize(data: list, map2num: dict, non_aa_num: int=20) -> list:\n",
        "    \"\"\"Tokenize all sequences in a list\n",
        "\n",
        "    :param data: list of sequences to tokenize\n",
        "    :type data: list\n",
        "    :param map2num: ammino acid -> integer token mapping\n",
        "    :type map2num: dict\n",
        "    :param non_aa_num: token for non amino acid characters, defaults to 20\n",
        "    :type non_aa_num: int, optional\n",
        "    :return: list of tokenized sequences\n",
        "    :rtype: list\n",
        "    \"\"\"\n",
        "    seq = []\n",
        "    for count, i in enumerate(data):\n",
        "        seq.append([map2num.get(j,non_aa_num) for j in list(i)])\n",
        "    return seq\n",
        "\n",
        "\n",
        "def truncate_pad(line: list, num_steps: int, padding_token: int) -> list:\n",
        "    \"\"\"Truncate or pad a tokenized sequence\n",
        "\n",
        "    :param line: tokenized sequence\n",
        "    :type line: list\n",
        "    :param num_steps: maximum sequence length\n",
        "    :type num_steps: int\n",
        "    :param padding_token: token to be used for padding\n",
        "    :type padding_token: int\n",
        "    :return: truncated/padded sequence\n",
        "    :rtype: list\n",
        "    \"\"\"\n",
        "    if len(line) > num_steps:\n",
        "        return line[:num_steps] # Truncate\n",
        "    return line + [padding_token] * (num_steps - len(line)) # Pad\n",
        "\n",
        "\n",
        "def build_seq_array(lines: list, num_steps: int, non_aa_num: int=20) -> torch.tensor:\n",
        "    \"\"\"Truncate or pad tokenized sequences and convert to tensor\n",
        "\n",
        "    :param lines: tokenized sequences\n",
        "    :type lines: list\n",
        "    :param num_steps: maximum sequence length\n",
        "    :type num_steps: int\n",
        "    :param non_aa_num: token for padding, defaults to 20\n",
        "    :type non_aa_num: int, optional\n",
        "    :return: tensor with truncated/padded tokenized sequences\n",
        "    :rtype: torch.tensor\n",
        "    \"\"\"\n",
        "    return torch.tensor([truncate_pad(l, num_steps, non_aa_num) for l in lines], dtype=torch.long)\n",
        "\n",
        "\n",
        "# 7. load_array() & load_data()\n",
        "def load_array(data_arrays: tuple[torch.tensor, torch.tensor], batch_size: int, is_train: bool=True) -> torch.utils.data.DataLoader:\n",
        "    \"\"\"Construct a PyTorch data iterator.\n",
        "\n",
        "    Taken from d2l package\"\"\"\n",
        "    dataset = torch.utils.data.TensorDataset(*data_arrays)\n",
        "    return torch.utils.data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
        "\n",
        "\n",
        "def load_data(batch_size: int, num_steps: int, dataset: tuple[list, list]) -> torch.utils.data.DataLoader:\n",
        "    \"\"\"Tokenize sequence/label dataset and load into dataloader.\n",
        "\n",
        "    :param batch_size: size of each batch\n",
        "    :type batch_size: int\n",
        "    :param num_steps: maximum sequence length\n",
        "    :type num_steps: int\n",
        "    :param dataset: first list contains sequences, second labels\n",
        "    :type dataset: tuple[list, list]\n",
        "    :return: torch dataloader which gives a tensor of sequences in a batch and a tensor with their labels\n",
        "    :rtype: torch.utils.data.DataLoader\n",
        "    \"\"\"\n",
        "    mapaa2num = {aa: i for (i, aa) in enumerate(list(\"ACDEFGHIKLMNPQRSTVWY\"))}\n",
        "    seq,lab = dataset\n",
        "    seq = tokenize(seq, mapaa2num)\n",
        "    seq_array = build_seq_array(seq, num_steps)\n",
        "    data_arrays = (seq_array, torch.tensor(lab, dtype=torch.long))\n",
        "    data_iter = load_array(data_arrays, batch_size)\n",
        "    return data_iter"
      ],
      "metadata": {
        "id": "eHpedW0hqx1V"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset selector\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ChrVqNAqKXEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For the GO dataset\n",
        "REMOVE = \"neg\"                   # None | \"neg\" | \"pos\"\n",
        "\n",
        "seq_path  = \"expr5Tseq_filtGO_100-1000.lis\"\n",
        "pos_path  = \"GO_3A0005739.annotprot\"\n",
        "datalist, labellist = read(seq_path, pos_path)\n",
        "\n",
        "# Removing half dataset\n",
        "if REMOVE is not None:\n",
        "    pos_datalist, neg_datalist = split_labelled(datalist, labellist)\n",
        "    if REMOVE == \"neg\":\n",
        "        neg_datalist = remove_sequences(neg_datalist, 0.5)\n",
        "    elif REMOVE == \"pos\":\n",
        "        pos_datalist = remove_sequences(pos_datalist, 0.5)\n",
        "    datalist, labellist = fuse_sequence_lists(pos_datalist, neg_datalist)"
      ],
      "metadata": {
        "id": "yDEFWAwbK_s3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For the synthetic dataset\n",
        "dataset_id = \"len100_200_n1000\"\n",
        "REMOVE = None                   # None | \"neg\" | \"pos\"\n",
        "\n",
        "seq_path  = f\"{dataset_id}.seq\"\n",
        "pos_path  = f\"{dataset_id}.pos\"\n",
        "datalist, labellist = read(seq_path, pos_path)\n",
        "\n",
        "# Removing half dataset\n",
        "if REMOVE is not None:\n",
        "    pos_datalist, neg_datalist = split_labelled(datalist, labellist)\n",
        "    if REMOVE == \"neg\":\n",
        "        neg_datalist = remove_sequences(neg_datalist, 0.8)\n",
        "    elif REMOVE == \"pos\":\n",
        "        pos_datalist = remove_sequences(pos_datalist, 0.8)\n",
        "    datalist, labellist = fuse_sequence_lists(pos_datalist, neg_datalist)"
      ],
      "metadata": {
        "id": "ick2N8sPGoDV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loader"
      ],
      "metadata": {
        "id": "_b5Kq5jEKZwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "num_steps = 1000 # max sequence length\n",
        "\n",
        "traindatalist, trainlabellist, testdatalist, testlabellist = generate_train_test(datalist, labellist, 0.8)\n",
        "traindataset = [traindatalist, trainlabellist]\n",
        "testdataset = [testdatalist, testlabellist]\n",
        "\n",
        "# Set batch_size and num_steps (maximum sequence length)\n",
        "train_iter = load_data(batch_size, num_steps, traindataset)\n",
        "test_iter = load_data(batch_size, num_steps, testdataset)\n",
        "\n",
        "print(\"batch shape  :\", next(iter(train_iter))[0].shape)\n",
        "print(\"labels shape :\", next(iter(train_iter))[1].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyRvhoyiwv_g",
        "outputId": "d3ef7409-8db6-4b62-cfca-13a5d63bb0b4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch shape  : torch.Size([10, 1000])\n",
            "labels shape : torch.Size([10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training / Evaluation"
      ],
      "metadata": {
        "id": "BhuHUN6TRLun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "YwR8PJG9SYbo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(layer):\n",
        "    if type(layer) == nn.Linear or type(layer) == nn.Conv1d:\n",
        "        nn.init.xavier_uniform_(layer.weight)\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model, loss_fn, optimizer, device='cuda'):\n",
        "        self.model = model.to(device)\n",
        "        self.loss_fn = loss_fn\n",
        "        self.optimizer = optimizer\n",
        "        self.device = device\n",
        "        self.columns = [\n",
        "            'epoch',\n",
        "            'train_accuracy',\n",
        "            'train_precision',\n",
        "            'train_recall',\n",
        "            'train_fscore',\n",
        "            'train_loss',\n",
        "            'test_accuracy',\n",
        "            'test_precision',\n",
        "            'test_recall',\n",
        "            'test_fscore',\n",
        "            'test_loss'\n",
        "        ]\n",
        "        self.df = pd.DataFrame(columns=self.columns)\n",
        "\n",
        "\n",
        "    # One training epoch -------------------------------------------------------\n",
        "    def _train_one_epoch(self, train_iter):\n",
        "        result_loss = 0\n",
        "        tpos = fpos = tneg = fneg = 0\n",
        "        self.model.train(True)\n",
        "\n",
        "        for i, (inputs, labels) in enumerate(train_iter):\n",
        "            inputs = inputs.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            outputs = self.model(inputs)\n",
        "            loss = self.loss_fn(outputs, labels)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            result_loss += loss.item()\n",
        "\n",
        "            # Confusion matrix calculation\n",
        "            for j, l in enumerate(labels):\n",
        "                o = outputs[j].tolist().index(max(outputs[j]))\n",
        "                l = l.item()\n",
        "                if o == 1 and l == 1:\n",
        "                    tpos += 1\n",
        "                elif o == 1 and l == 0:\n",
        "                    fpos += 1\n",
        "                elif o == 0 and l == 0:\n",
        "                    tneg += 1\n",
        "                elif o == 0 and l == 1:\n",
        "                    fneg += 1\n",
        "                else:\n",
        "                    raise ValueError(f'Output or label is not rounded to zero: {o = } {l = }')\n",
        "\n",
        "        accuracy = (tpos + tneg) / (tpos + fpos + tneg + fneg)\n",
        "        precision = tpos / (tpos + fpos) if (tpos + fpos) > 0 else 0\n",
        "        recall = tpos / (tpos + fneg) if (tpos + fneg) > 0 else 0\n",
        "        fscore = 2 * tpos / (2*tpos + fpos + fneg) if (2*tpos + fpos + fneg) > 0 else 0\n",
        "\n",
        "        return accuracy, precision, recall, fscore, result_loss / (i+1)\n",
        "\n",
        "    # Evaluation epoch ---------------------------------------------------------\n",
        "    def _test_one_epoch(self, test_iter):\n",
        "        result_loss = 0\n",
        "        tpos = fpos = tneg = fneg = 0\n",
        "        self.model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, (inputs, labels) in enumerate(test_iter, start=1):\n",
        "                inputs = inputs.to(self.device)\n",
        "                labels = labels.to(self.device)\n",
        "\n",
        "                outputs = self.model(inputs)\n",
        "                loss = self.loss_fn(input=outputs, target=labels)\n",
        "                result_loss += loss.item()\n",
        "\n",
        "                # Confusion matrix calculation\n",
        "                for j, l in enumerate(labels):\n",
        "                    o = outputs[j].tolist().index(max(outputs[j]))\n",
        "                    l = l.item()\n",
        "                    if o == 1 and l == 1:\n",
        "                        tpos += 1\n",
        "                    elif o == 1 and l == 0:\n",
        "                        fpos += 1\n",
        "                    elif o == 0 and l == 0:\n",
        "                        tneg += 1\n",
        "                    elif o == 0 and l == 1:\n",
        "                        fneg += 1\n",
        "                    else:\n",
        "                        raise ValueError(f'Output or label is not rounded to zero: {o = } {l = }')\n",
        "\n",
        "        accuracy = (tpos + tneg) / (tpos + fpos + tneg + fneg)\n",
        "        precision = tpos / (tpos + fpos) if (tpos + fpos) > 0 else 0\n",
        "        recall = tpos / (tpos + fneg) if (tpos + fneg) > 0 else 0\n",
        "        fscore = 2 * tpos / (2*tpos + fpos + fneg) if (2*tpos + fpos + fneg) > 0 else 0\n",
        "\n",
        "        return accuracy, precision, recall, fscore, result_loss / (i+1)\n",
        "\n",
        "\n",
        "    def _load_into_dict(self, epoch, train_stats, test_stats):\n",
        "        row = [epoch] + list(train_stats) + list(test_stats)\n",
        "        row = pd.DataFrame(row, index=self.columns).T\n",
        "        self.df = pd.concat([self.df, row], axis=0)\n",
        "\n",
        "\n",
        "    def train(self, epochs, train_iter, test_iter):\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            train_stats = self._train_one_epoch(train_iter)\n",
        "\n",
        "            test_stats = self._test_one_epoch(test_iter)\n",
        "            self._load_into_dict(epoch, train_stats, test_stats)\n",
        "            print(f\"[epoch {epoch:02d}] \"\n",
        "                  f\"train-loss={train_stats[-1]:.4f} | \"\n",
        "                  f\"test-loss={test_stats[-1]:.4f} | \"\n",
        "                  f\"train-acc={train_stats[0]:.4f} | \"\n",
        "                  f\"test-acc={test_stats[0]:.4f} | \"\n",
        "                  f\"P={test_stats[1]:.4f} | R={test_stats[2]:.4f} | F1={test_stats[3]:.4f}\")\n",
        "\n",
        "        return self.df"
      ],
      "metadata": {
        "id": "C0xPrmp3jHjD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BerryCNN1D(nn.Module):\n",
        "    \"\"\"1D Convolutional Neural Network for protein function classification\"\"\"\n",
        "    def __init__(self, vocab_size: int = 21,\n",
        "                 dropout_rate = 0, conv_channels: int = 128,\n",
        "                 use_bias: bool = False):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "        # CNN model for binary classification\n",
        "        self.conv1 = nn.Sequential(\n",
        "            # conv block 1\n",
        "            nn.Conv1d(in_channels=vocab_size, out_channels=conv_channels, kernel_size=3, padding='same', bias=use_bias),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            # conv block 2\n",
        "            nn.Conv1d(in_channels=conv_channels, out_channels=conv_channels, kernel_size=3, padding='same', bias=use_bias),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveMaxPool1d(1)\n",
        "        )\n",
        "\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Sequential(\n",
        "            # flatten + classification head\n",
        "            nn.Flatten(1, -1),\n",
        "            nn.LazyLinear(out_features=2, bias=use_bias)  # binary classification\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.tensor, targets: torch.tensor = None):\n",
        "        \"\"\"Predict protein function class (0 or 1)\"\"\"\n",
        "        x = self.embedding(x).permute(0,2,1)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.dropout(x)\n",
        "        return self.fc(x)\n"
      ],
      "metadata": {
        "id": "2V5v63DoLaLr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Model"
      ],
      "metadata": {
        "id": "UeAeB8Wo79Oj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "param_grid = {\n",
        "    'dropout_rate': [0, 0.3],\n",
        "    'lr': [0.01, 0.001],\n",
        "    'momentum': [0, 0.5],\n",
        "    'conv_channels': [64, 128]\n",
        "}\n",
        "\n",
        "grid = list(ParameterGrid(param_grid))\n",
        "\n",
        "best_acc = 0\n",
        "best_params = None\n",
        "\n",
        "df = None\n",
        "\n",
        "for params in grid:\n",
        "    print(\"Current hyper-parameters:\", params)\n",
        "    model = BerryCNN1D(\n",
        "        vocab_size=21,\n",
        "        dropout_rate=params['dropout_rate'],\n",
        "        conv_channels=params['conv_channels']\n",
        "    )\n",
        "    model.apply(init_weights)\n",
        "\n",
        "    optimizer = torch.optim.SGD(\n",
        "        model.parameters(),\n",
        "        lr=params['lr'],\n",
        "        momentum=params['momentum']\n",
        "    )\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    trainer = Trainer(model, loss_fn, optimizer, device)\n",
        "\n",
        "    out_df = trainer.train(epochs=10, train_iter=train_iter, test_iter=test_iter)\n",
        "\n",
        "    for p in params.keys():\n",
        "        out_df[p] = params[p]\n",
        "\n",
        "    acc = out_df['test_accuracy'].max()\n",
        "\n",
        "    if acc >= best_acc:\n",
        "        best_acc = []\n",
        "        best_acc = acc\n",
        "        best_params = params\n",
        "        print(f\"New best accuracy {best_acc:.4f} with {best_params} \\n\")\n",
        "\n",
        "    if type(df) != pd.DataFrame:\n",
        "        df = out_df\n",
        "    else:\n",
        "        df = pd.concat([df, out_df], axis=0)\n",
        "\n",
        "print(\"Best hyper‑parameters found:\", best_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-HESI-RwU5o",
        "outputId": "92cc2d0d-3201-4a4a-b8d2-34fb27f8c89c",
        "collapsed": true
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current hyper-parameters: {'conv_channels': 64, 'dropout_rate': 0, 'lr': 0.01, 'momentum': 0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-683adbe3301c>:107: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  self.df = pd.concat([self.df, row], axis=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 00] train-loss=0.6706 | test-loss=0.5830 | train-acc=0.5900 | test-acc=0.7600 | P=0.8889 | R=0.6486 | F1=0.7500\n",
            "[epoch 01] train-loss=0.4676 | test-loss=0.2590 | train-acc=0.8562 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 02] train-loss=0.1479 | test-loss=0.0759 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 03] train-loss=0.0536 | test-loss=0.0363 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 04] train-loss=0.0283 | test-loss=0.0217 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 05] train-loss=0.0185 | test-loss=0.0152 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 06] train-loss=0.0133 | test-loss=0.0113 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 07] train-loss=0.0102 | test-loss=0.0089 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 08] train-loss=0.0082 | test-loss=0.0073 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 09] train-loss=0.0068 | test-loss=0.0063 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "New best accuracy 1.0000 with {'conv_channels': 64, 'dropout_rate': 0, 'lr': 0.01, 'momentum': 0} \n",
            "\n",
            "Current hyper-parameters: {'conv_channels': 64, 'dropout_rate': 0, 'lr': 0.01, 'momentum': 0.5}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-683adbe3301c>:107: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  self.df = pd.concat([self.df, row], axis=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 00] train-loss=0.4736 | test-loss=0.1145 | train-acc=0.7800 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 01] train-loss=0.0547 | test-loss=0.0247 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 02] train-loss=0.0178 | test-loss=0.0122 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 03] train-loss=0.0100 | test-loss=0.0076 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 04] train-loss=0.0068 | test-loss=0.0056 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 05] train-loss=0.0051 | test-loss=0.0043 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 06] train-loss=0.0040 | test-loss=0.0035 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 07] train-loss=0.0033 | test-loss=0.0029 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 08] train-loss=0.0028 | test-loss=0.0025 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 09] train-loss=0.0024 | test-loss=0.0022 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "New best accuracy 1.0000 with {'conv_channels': 64, 'dropout_rate': 0, 'lr': 0.01, 'momentum': 0.5} \n",
            "\n",
            "Current hyper-parameters: {'conv_channels': 64, 'dropout_rate': 0, 'lr': 0.001, 'momentum': 0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-683adbe3301c>:107: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  self.df = pd.concat([self.df, row], axis=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 00] train-loss=0.7100 | test-loss=0.6608 | train-acc=0.4900 | test-acc=0.5400 | P=0.5683 | R=0.7117 | F1=0.6320\n",
            "[epoch 01] train-loss=0.6932 | test-loss=0.6641 | train-acc=0.5125 | test-acc=0.4900 | P=0.5616 | R=0.3694 | F1=0.4457\n",
            "[epoch 02] train-loss=0.6866 | test-loss=0.6664 | train-acc=0.5337 | test-acc=0.4850 | P=0.6053 | R=0.2072 | F1=0.3087\n",
            "[epoch 03] train-loss=0.6795 | test-loss=0.6487 | train-acc=0.5687 | test-acc=0.5950 | P=0.6172 | R=0.7117 | F1=0.6611\n",
            "[epoch 04] train-loss=0.6698 | test-loss=0.6377 | train-acc=0.6100 | test-acc=0.5950 | P=0.6071 | R=0.7658 | F1=0.6773\n",
            "[epoch 05] train-loss=0.6573 | test-loss=0.6376 | train-acc=0.6713 | test-acc=0.5950 | P=0.7679 | R=0.3874 | F1=0.5150\n",
            "[epoch 06] train-loss=0.6425 | test-loss=0.6244 | train-acc=0.7225 | test-acc=0.6550 | P=0.8750 | R=0.4414 | F1=0.5868\n",
            "[epoch 07] train-loss=0.6222 | test-loss=0.5930 | train-acc=0.8013 | test-acc=0.7600 | P=0.7266 | R=0.9099 | F1=0.8080\n",
            "[epoch 08] train-loss=0.6002 | test-loss=0.5732 | train-acc=0.8263 | test-acc=0.8300 | P=0.8130 | R=0.9009 | F1=0.8547\n",
            "[epoch 09] train-loss=0.5735 | test-loss=0.5512 | train-acc=0.9150 | test-acc=0.9100 | P=0.9515 | R=0.8829 | F1=0.9159\n",
            "Current hyper-parameters: {'conv_channels': 64, 'dropout_rate': 0, 'lr': 0.001, 'momentum': 0.5}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-683adbe3301c>:107: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  self.df = pd.concat([self.df, row], axis=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 00] train-loss=0.6976 | test-loss=0.6626 | train-acc=0.4925 | test-acc=0.4900 | P=0.5634 | R=0.3604 | F1=0.4396\n",
            "[epoch 01] train-loss=0.6863 | test-loss=0.6566 | train-acc=0.5587 | test-acc=0.5200 | P=0.6027 | R=0.3964 | F1=0.4783\n",
            "[epoch 02] train-loss=0.6763 | test-loss=0.6482 | train-acc=0.5962 | test-acc=0.5750 | P=0.6912 | R=0.4234 | F1=0.5251\n",
            "[epoch 03] train-loss=0.6627 | test-loss=0.6231 | train-acc=0.6425 | test-acc=0.6600 | P=0.6319 | R=0.9279 | F1=0.7518\n",
            "[epoch 04] train-loss=0.6397 | test-loss=0.6017 | train-acc=0.7488 | test-acc=0.7700 | P=0.8037 | R=0.7748 | F1=0.7890\n",
            "[epoch 05] train-loss=0.6003 | test-loss=0.5810 | train-acc=0.8450 | test-acc=0.7050 | P=1.0000 | R=0.4685 | F1=0.6380\n",
            "[epoch 06] train-loss=0.5501 | test-loss=0.4977 | train-acc=0.9137 | test-acc=0.9500 | P=0.9174 | R=1.0000 | F1=0.9569\n",
            "[epoch 07] train-loss=0.4819 | test-loss=0.4324 | train-acc=0.9825 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 08] train-loss=0.4029 | test-loss=0.3529 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 09] train-loss=0.3233 | test-loss=0.2793 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "New best accuracy 1.0000 with {'conv_channels': 64, 'dropout_rate': 0, 'lr': 0.001, 'momentum': 0.5} \n",
            "\n",
            "Current hyper-parameters: {'conv_channels': 64, 'dropout_rate': 0.3, 'lr': 0.01, 'momentum': 0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-683adbe3301c>:107: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  self.df = pd.concat([self.df, row], axis=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 00] train-loss=0.7258 | test-loss=0.6390 | train-acc=0.5038 | test-acc=0.5550 | P=0.8929 | R=0.2252 | F1=0.3597\n",
            "[epoch 01] train-loss=0.6471 | test-loss=0.4851 | train-acc=0.6212 | test-acc=0.9650 | P=0.9407 | R=1.0000 | F1=0.9694\n",
            "[epoch 02] train-loss=0.4707 | test-loss=0.2464 | train-acc=0.8113 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 03] train-loss=0.2688 | test-loss=0.1143 | train-acc=0.9450 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 04] train-loss=0.2090 | test-loss=0.0700 | train-acc=0.9500 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 05] train-loss=0.1721 | test-loss=0.0540 | train-acc=0.9600 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 06] train-loss=0.1582 | test-loss=0.0428 | train-acc=0.9550 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 07] train-loss=0.1519 | test-loss=0.0379 | train-acc=0.9425 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 08] train-loss=0.1296 | test-loss=0.0297 | train-acc=0.9575 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 09] train-loss=0.0977 | test-loss=0.0251 | train-acc=0.9712 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "New best accuracy 1.0000 with {'conv_channels': 64, 'dropout_rate': 0.3, 'lr': 0.01, 'momentum': 0} \n",
            "\n",
            "Current hyper-parameters: {'conv_channels': 64, 'dropout_rate': 0.3, 'lr': 0.01, 'momentum': 0.5}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-683adbe3301c>:107: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  self.df = pd.concat([self.df, row], axis=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 00] train-loss=0.6844 | test-loss=0.3853 | train-acc=0.5663 | test-acc=0.9850 | P=0.9737 | R=1.0000 | F1=0.9867\n",
            "[epoch 01] train-loss=0.2560 | test-loss=0.0690 | train-acc=0.9275 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 02] train-loss=0.0777 | test-loss=0.0202 | train-acc=0.9838 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 03] train-loss=0.0508 | test-loss=0.0104 | train-acc=0.9925 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 04] train-loss=0.0360 | test-loss=0.0067 | train-acc=0.9938 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 05] train-loss=0.0404 | test-loss=0.0075 | train-acc=0.9888 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 06] train-loss=0.0175 | test-loss=0.0040 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 07] train-loss=0.0196 | test-loss=0.0049 | train-acc=0.9950 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 08] train-loss=0.0161 | test-loss=0.0043 | train-acc=0.9988 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 09] train-loss=0.0122 | test-loss=0.0022 | train-acc=0.9975 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "New best accuracy 1.0000 with {'conv_channels': 64, 'dropout_rate': 0.3, 'lr': 0.01, 'momentum': 0.5} \n",
            "\n",
            "Current hyper-parameters: {'conv_channels': 64, 'dropout_rate': 0.3, 'lr': 0.001, 'momentum': 0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-683adbe3301c>:107: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  self.df = pd.concat([self.df, row], axis=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 00] train-loss=0.7427 | test-loss=0.6706 | train-acc=0.4988 | test-acc=0.4750 | P=0.5625 | R=0.2432 | F1=0.3396\n",
            "[epoch 01] train-loss=0.7246 | test-loss=0.6607 | train-acc=0.4750 | test-acc=0.5150 | P=0.5455 | R=0.7568 | F1=0.6340\n",
            "[epoch 02] train-loss=0.7261 | test-loss=0.6644 | train-acc=0.5100 | test-acc=0.5000 | P=0.5632 | R=0.4414 | F1=0.4949\n",
            "[epoch 03] train-loss=0.7210 | test-loss=0.6619 | train-acc=0.4888 | test-acc=0.4650 | P=0.5189 | R=0.4955 | F1=0.5069\n",
            "[epoch 04] train-loss=0.7190 | test-loss=0.6605 | train-acc=0.4738 | test-acc=0.4850 | P=0.5370 | R=0.5225 | F1=0.5297\n",
            "[epoch 05] train-loss=0.7178 | test-loss=0.6569 | train-acc=0.5088 | test-acc=0.5300 | P=0.5603 | R=0.7117 | F1=0.6270\n",
            "[epoch 06] train-loss=0.7070 | test-loss=0.6589 | train-acc=0.5150 | test-acc=0.5300 | P=0.5977 | R=0.4685 | F1=0.5253\n",
            "[epoch 07] train-loss=0.7068 | test-loss=0.6532 | train-acc=0.5100 | test-acc=0.5650 | P=0.5833 | R=0.7568 | F1=0.6588\n",
            "[epoch 08] train-loss=0.6999 | test-loss=0.6541 | train-acc=0.5225 | test-acc=0.5700 | P=0.6437 | R=0.5045 | F1=0.5657\n",
            "[epoch 09] train-loss=0.6919 | test-loss=0.6553 | train-acc=0.5350 | test-acc=0.5400 | P=0.7209 | R=0.2793 | F1=0.4026\n",
            "Current hyper-parameters: {'conv_channels': 64, 'dropout_rate': 0.3, 'lr': 0.001, 'momentum': 0.5}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-683adbe3301c>:107: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  self.df = pd.concat([self.df, row], axis=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 00] train-loss=0.7409 | test-loss=0.6469 | train-acc=0.5150 | test-acc=0.5950 | P=0.6705 | R=0.5315 | F1=0.5930\n",
            "[epoch 01] train-loss=0.7464 | test-loss=0.6396 | train-acc=0.5075 | test-acc=0.6400 | P=0.7826 | R=0.4865 | F1=0.6000\n",
            "[epoch 02] train-loss=0.7109 | test-loss=0.6212 | train-acc=0.5625 | test-acc=0.7500 | P=0.8144 | R=0.7117 | F1=0.7596\n",
            "[epoch 03] train-loss=0.6743 | test-loss=0.5987 | train-acc=0.5925 | test-acc=0.8700 | P=0.9048 | R=0.8559 | F1=0.8796\n",
            "[epoch 04] train-loss=0.6517 | test-loss=0.5646 | train-acc=0.6262 | test-acc=0.9150 | P=0.8790 | R=0.9820 | F1=0.9277\n",
            "[epoch 05] train-loss=0.6143 | test-loss=0.5228 | train-acc=0.6713 | test-acc=0.9800 | P=0.9652 | R=1.0000 | F1=0.9823\n",
            "[epoch 06] train-loss=0.5546 | test-loss=0.4578 | train-acc=0.7388 | test-acc=0.9750 | P=0.9569 | R=1.0000 | F1=0.9780\n",
            "[epoch 07] train-loss=0.5132 | test-loss=0.3963 | train-acc=0.7588 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 08] train-loss=0.4497 | test-loss=0.3439 | train-acc=0.8300 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 09] train-loss=0.4005 | test-loss=0.2776 | train-acc=0.8712 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "New best accuracy 1.0000 with {'conv_channels': 64, 'dropout_rate': 0.3, 'lr': 0.001, 'momentum': 0.5} \n",
            "\n",
            "Current hyper-parameters: {'conv_channels': 128, 'dropout_rate': 0, 'lr': 0.01, 'momentum': 0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-683adbe3301c>:107: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  self.df = pd.concat([self.df, row], axis=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 00] train-loss=0.6530 | test-loss=0.4700 | train-acc=0.6212 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 01] train-loss=0.2833 | test-loss=0.1156 | train-acc=0.9812 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 02] train-loss=0.0722 | test-loss=0.0420 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 03] train-loss=0.0314 | test-loss=0.0233 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 04] train-loss=0.0185 | test-loss=0.0149 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 05] train-loss=0.0127 | test-loss=0.0109 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 06] train-loss=0.0095 | test-loss=0.0084 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 07] train-loss=0.0075 | test-loss=0.0067 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 08] train-loss=0.0061 | test-loss=0.0056 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 09] train-loss=0.0051 | test-loss=0.0048 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "New best accuracy 1.0000 with {'conv_channels': 128, 'dropout_rate': 0, 'lr': 0.01, 'momentum': 0} \n",
            "\n",
            "Current hyper-parameters: {'conv_channels': 128, 'dropout_rate': 0, 'lr': 0.01, 'momentum': 0.5}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-683adbe3301c>:107: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  self.df = pd.concat([self.df, row], axis=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 00] train-loss=0.6616 | test-loss=0.4021 | train-acc=0.5863 | test-acc=0.9950 | P=0.9911 | R=1.0000 | F1=0.9955\n",
            "[epoch 01] train-loss=0.1540 | test-loss=0.0440 | train-acc=0.9925 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 02] train-loss=0.0246 | test-loss=0.0164 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 03] train-loss=0.0117 | test-loss=0.0096 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 04] train-loss=0.0075 | test-loss=0.0068 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 05] train-loss=0.0053 | test-loss=0.0050 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 06] train-loss=0.0041 | test-loss=0.0040 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 07] train-loss=0.0033 | test-loss=0.0033 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 08] train-loss=0.0028 | test-loss=0.0028 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 09] train-loss=0.0023 | test-loss=0.0024 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "New best accuracy 1.0000 with {'conv_channels': 128, 'dropout_rate': 0, 'lr': 0.01, 'momentum': 0.5} \n",
            "\n",
            "Current hyper-parameters: {'conv_channels': 128, 'dropout_rate': 0, 'lr': 0.001, 'momentum': 0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-683adbe3301c>:107: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  self.df = pd.concat([self.df, row], axis=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 00] train-loss=0.7329 | test-loss=0.6424 | train-acc=0.5150 | test-acc=0.6300 | P=0.6149 | R=0.8919 | F1=0.7279\n",
            "[epoch 01] train-loss=0.6831 | test-loss=0.6418 | train-acc=0.5737 | test-acc=0.5950 | P=0.6471 | R=0.5946 | F1=0.6197\n",
            "[epoch 02] train-loss=0.6771 | test-loss=0.6396 | train-acc=0.6125 | test-acc=0.6500 | P=0.7808 | R=0.5135 | F1=0.6196\n",
            "[epoch 03] train-loss=0.6696 | test-loss=0.6348 | train-acc=0.6538 | test-acc=0.6900 | P=0.8769 | R=0.5135 | F1=0.6477\n",
            "[epoch 04] train-loss=0.6615 | test-loss=0.6264 | train-acc=0.6913 | test-acc=0.7250 | P=0.9242 | R=0.5495 | F1=0.6893\n",
            "[epoch 05] train-loss=0.6488 | test-loss=0.6062 | train-acc=0.7262 | test-acc=0.7700 | P=0.7152 | R=0.9730 | F1=0.8244\n",
            "[epoch 06] train-loss=0.6352 | test-loss=0.5949 | train-acc=0.8263 | test-acc=0.8750 | P=0.8772 | R=0.9009 | F1=0.8889\n",
            "[epoch 07] train-loss=0.6154 | test-loss=0.5741 | train-acc=0.8725 | test-acc=0.9050 | P=0.8710 | R=0.9730 | F1=0.9191\n",
            "[epoch 08] train-loss=0.5921 | test-loss=0.5556 | train-acc=0.9287 | test-acc=0.9550 | P=0.9811 | R=0.9369 | F1=0.9585\n",
            "[epoch 09] train-loss=0.5624 | test-loss=0.5246 | train-acc=0.9725 | test-acc=0.9800 | P=0.9820 | R=0.9820 | F1=0.9820\n",
            "Current hyper-parameters: {'conv_channels': 128, 'dropout_rate': 0, 'lr': 0.001, 'momentum': 0.5}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-683adbe3301c>:107: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  self.df = pd.concat([self.df, row], axis=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 00] train-loss=0.6978 | test-loss=0.6717 | train-acc=0.5038 | test-acc=0.4200 | P=0.3810 | R=0.0721 | F1=0.1212\n",
            "[epoch 01] train-loss=0.6902 | test-loss=0.6574 | train-acc=0.5325 | test-acc=0.4850 | P=0.5645 | R=0.3153 | F1=0.4046\n",
            "[epoch 02] train-loss=0.6737 | test-loss=0.6522 | train-acc=0.6025 | test-acc=0.5000 | P=0.7895 | R=0.1351 | F1=0.2308\n",
            "[epoch 03] train-loss=0.6514 | test-loss=0.6177 | train-acc=0.6863 | test-acc=0.7800 | P=0.7913 | R=0.8198 | F1=0.8053\n",
            "[epoch 04] train-loss=0.6195 | test-loss=0.5852 | train-acc=0.8425 | test-acc=0.9150 | P=0.9608 | R=0.8829 | F1=0.9202\n",
            "[epoch 05] train-loss=0.5717 | test-loss=0.5315 | train-acc=0.9163 | test-acc=0.9850 | P=0.9821 | R=0.9910 | F1=0.9865\n",
            "[epoch 06] train-loss=0.5051 | test-loss=0.4708 | train-acc=0.9950 | test-acc=0.9950 | P=1.0000 | R=0.9910 | F1=0.9955\n",
            "[epoch 07] train-loss=0.4249 | test-loss=0.3777 | train-acc=0.9975 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 08] train-loss=0.3370 | test-loss=0.2925 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 09] train-loss=0.2569 | test-loss=0.2214 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "New best accuracy 1.0000 with {'conv_channels': 128, 'dropout_rate': 0, 'lr': 0.001, 'momentum': 0.5} \n",
            "\n",
            "Current hyper-parameters: {'conv_channels': 128, 'dropout_rate': 0.3, 'lr': 0.01, 'momentum': 0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-683adbe3301c>:107: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  self.df = pd.concat([self.df, row], axis=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 00] train-loss=0.6672 | test-loss=0.5411 | train-acc=0.5800 | test-acc=0.8900 | P=0.8346 | R=1.0000 | F1=0.9098\n",
            "[epoch 01] train-loss=0.4747 | test-loss=0.2270 | train-acc=0.8013 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 02] train-loss=0.2237 | test-loss=0.0808 | train-acc=0.9450 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 03] train-loss=0.1277 | test-loss=0.0479 | train-acc=0.9712 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 04] train-loss=0.0845 | test-loss=0.0248 | train-acc=0.9838 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 05] train-loss=0.0585 | test-loss=0.0250 | train-acc=0.9838 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 06] train-loss=0.0453 | test-loss=0.0125 | train-acc=0.9900 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 07] train-loss=0.0327 | test-loss=0.0095 | train-acc=0.9950 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 08] train-loss=0.0300 | test-loss=0.0096 | train-acc=0.9925 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 09] train-loss=0.0223 | test-loss=0.0051 | train-acc=0.9950 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "New best accuracy 1.0000 with {'conv_channels': 128, 'dropout_rate': 0.3, 'lr': 0.01, 'momentum': 0} \n",
            "\n",
            "Current hyper-parameters: {'conv_channels': 128, 'dropout_rate': 0.3, 'lr': 0.01, 'momentum': 0.5}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-683adbe3301c>:107: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  self.df = pd.concat([self.df, row], axis=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 00] train-loss=0.6266 | test-loss=0.2355 | train-acc=0.6438 | test-acc=0.9950 | P=0.9911 | R=1.0000 | F1=0.9955\n",
            "[epoch 01] train-loss=0.1873 | test-loss=0.0406 | train-acc=0.9463 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 02] train-loss=0.0622 | test-loss=0.0132 | train-acc=0.9862 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 03] train-loss=0.0278 | test-loss=0.0049 | train-acc=0.9962 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 04] train-loss=0.0221 | test-loss=0.0039 | train-acc=0.9962 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 05] train-loss=0.0148 | test-loss=0.0040 | train-acc=0.9950 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 06] train-loss=0.0127 | test-loss=0.0032 | train-acc=0.9975 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 07] train-loss=0.0103 | test-loss=0.0076 | train-acc=0.9988 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 08] train-loss=0.0060 | test-loss=0.0017 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 09] train-loss=0.0100 | test-loss=0.0030 | train-acc=0.9975 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "New best accuracy 1.0000 with {'conv_channels': 128, 'dropout_rate': 0.3, 'lr': 0.01, 'momentum': 0.5} \n",
            "\n",
            "Current hyper-parameters: {'conv_channels': 128, 'dropout_rate': 0.3, 'lr': 0.001, 'momentum': 0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-683adbe3301c>:107: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  self.df = pd.concat([self.df, row], axis=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 00] train-loss=0.7627 | test-loss=0.6818 | train-acc=0.4900 | test-acc=0.4450 | P=0.5000 | R=0.0090 | F1=0.0177\n",
            "[epoch 01] train-loss=0.7321 | test-loss=0.6683 | train-acc=0.4938 | test-acc=0.4750 | P=0.5682 | R=0.2252 | F1=0.3226\n",
            "[epoch 02] train-loss=0.7140 | test-loss=0.6651 | train-acc=0.5150 | test-acc=0.5050 | P=0.6111 | R=0.2973 | F1=0.4000\n",
            "[epoch 03] train-loss=0.7179 | test-loss=0.6526 | train-acc=0.5062 | test-acc=0.5650 | P=0.5606 | R=1.0000 | F1=0.7184\n",
            "[epoch 04] train-loss=0.7176 | test-loss=0.6671 | train-acc=0.5050 | test-acc=0.4800 | P=0.8182 | R=0.0811 | F1=0.1475\n",
            "[epoch 05] train-loss=0.7061 | test-loss=0.6588 | train-acc=0.5275 | test-acc=0.5250 | P=0.7000 | R=0.2523 | F1=0.3709\n",
            "[epoch 06] train-loss=0.7113 | test-loss=0.6610 | train-acc=0.5100 | test-acc=0.4950 | P=0.9167 | R=0.0991 | F1=0.1789\n",
            "[epoch 07] train-loss=0.6904 | test-loss=0.6403 | train-acc=0.5613 | test-acc=0.6350 | P=0.6234 | R=0.8649 | F1=0.7245\n",
            "[epoch 08] train-loss=0.6866 | test-loss=0.6301 | train-acc=0.5487 | test-acc=0.6700 | P=0.6398 | R=0.9279 | F1=0.7574\n",
            "[epoch 09] train-loss=0.6806 | test-loss=0.6191 | train-acc=0.5763 | test-acc=0.7500 | P=0.7103 | R=0.9279 | F1=0.8047\n",
            "Current hyper-parameters: {'conv_channels': 128, 'dropout_rate': 0.3, 'lr': 0.001, 'momentum': 0.5}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-683adbe3301c>:107: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  self.df = pd.concat([self.df, row], axis=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 00] train-loss=0.7365 | test-loss=0.6534 | train-acc=0.4963 | test-acc=0.5700 | P=0.5776 | R=0.8378 | F1=0.6838\n",
            "[epoch 01] train-loss=0.7160 | test-loss=0.6558 | train-acc=0.5138 | test-acc=0.5150 | P=0.5614 | R=0.5766 | F1=0.5689\n",
            "[epoch 02] train-loss=0.7089 | test-loss=0.6752 | train-acc=0.5262 | test-acc=0.4450 | P=0.5000 | R=0.0180 | F1=0.0348\n",
            "[epoch 03] train-loss=0.7002 | test-loss=0.6584 | train-acc=0.5437 | test-acc=0.5150 | P=0.6167 | R=0.3333 | F1=0.4327\n",
            "[epoch 04] train-loss=0.6985 | test-loss=0.6527 | train-acc=0.5300 | test-acc=0.5550 | P=0.5887 | R=0.6577 | F1=0.6213\n",
            "[epoch 05] train-loss=0.7013 | test-loss=0.6519 | train-acc=0.5325 | test-acc=0.5550 | P=0.6170 | R=0.5225 | F1=0.5659\n",
            "[epoch 06] train-loss=0.6915 | test-loss=0.6522 | train-acc=0.5350 | test-acc=0.5550 | P=0.6833 | R=0.3694 | F1=0.4795\n",
            "[epoch 07] train-loss=0.6857 | test-loss=0.6388 | train-acc=0.5463 | test-acc=0.6400 | P=0.6168 | R=0.9279 | F1=0.7410\n",
            "[epoch 08] train-loss=0.6759 | test-loss=0.6344 | train-acc=0.5775 | test-acc=0.6900 | P=0.8182 | R=0.5676 | F1=0.6702\n",
            "[epoch 09] train-loss=0.6585 | test-loss=0.6192 | train-acc=0.6150 | test-acc=0.7350 | P=0.8919 | R=0.5946 | F1=0.7135\n",
            "Best hyper‑parameters found: {'conv_channels': 128, 'dropout_rate': 0.3, 'lr': 0.01, 'momentum': 0.5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('hyperparameter_grid.csv')"
      ],
      "metadata": {
        "id": "XcpLkzHd_k6G"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot()"
      ],
      "metadata": {
        "id": "OAcU2yXQCgD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verander bovenstaande gegevens in een pandas dataframe\n",
        "\n",
        "\n",
        "\n",
        "len200_500_n5000nr4 = Best hyper‑parameters found: {'conv_channels': 64, 'dropout_rate': 0, 'lr': 0.01, 'momentum': 0}"
      ],
      "metadata": {
        "id": "aiodFhyizlh1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-label prediction"
      ],
      "metadata": {
        "id": "1w0ZU-T_u2yd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from datetime import datetime\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import random\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "from functools import total_ordering\n",
        "def init_weights(layer):\n",
        "    if type(layer) == nn.Linear or type(layer) == nn.Conv1d:\n",
        "        nn.init.xavier_uniform_(layer.weight)"
      ],
      "metadata": {
        "id": "NpXTnCJ63_6D"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data multiple labels"
      ],
      "metadata": {
        "id": "i1r1FMrdxNj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_multiple_pos(seqfile: str, posfiles: list[str]) -> tuple[list, list]:\n",
        "    \"\"\"Read file with sequences and file with positive cases into lists.\n",
        "\n",
        "    :param seqfile: file with sequences\n",
        "    :type seqfile: str\n",
        "    :param posfiles: files with positive cases (annotated with function)\n",
        "    :type posfile: str\n",
        "    :return: two lists, first with sequences and second with integer labels\n",
        "    :rtype: tuple[list, list]\n",
        "    \"\"\"\n",
        "    idlist = []\n",
        "    datalist = []\n",
        "    labellist = []\n",
        "    with open(seqfile, 'r') as f:\n",
        "        for line in f.readlines():\n",
        "            line = line.rstrip().split('\\t')\n",
        "            idlist.append(line[0])\n",
        "            datalist.append(line[1])\n",
        "            labellist.append([0 for _ in posfiles])\n",
        "    for l, posfile in enumerate(posfiles):\n",
        "        with open(posfile, 'r') as f:\n",
        "            for line in f.readlines():\n",
        "                id = line.rstrip()\n",
        "                try:\n",
        "                    i = idlist.index(id)\n",
        "                    # if labellist[i] != 0:\n",
        "                    #     raise Exception(f'Sequence with multiple labels. {id = }, {labellist[i] = }, {l = }')\n",
        "                    labellist[i][l] = 1\n",
        "                except ValueError:\n",
        "                    continue\n",
        "    return datalist, labellist\n",
        "\n",
        "\n",
        "def split_labelled_multiple_pos(datalist: list, labellist: list):\n",
        "    pos_datalist = []\n",
        "    pos_labellist = []\n",
        "    neg_datalist = []\n",
        "    neg_labellist = []\n",
        "    for i, labels in enumerate(labellist):\n",
        "        is_pos = False\n",
        "        for label in labels:\n",
        "            if label:\n",
        "                is_pos = True\n",
        "        if is_pos:\n",
        "            pos_datalist.append(datalist[i])\n",
        "            pos_labellist.append(labels)\n",
        "        else:\n",
        "            neg_datalist.append(datalist[i])\n",
        "            neg_labellist.append(labels)\n",
        "    return pos_datalist, pos_labellist, neg_datalist, neg_labellist\n",
        "\n",
        "\n",
        "def zip_n_shuffle(list1: list, list2: list) -> tuple[list, list]:\n",
        "    assert len(list1) == len(list2)\n",
        "    combined = list(zip(list1, list2))\n",
        "    random.shuffle(combined)\n",
        "    list1, list2 = zip(*combined)\n",
        "    return list(list1), list(list2)\n",
        "\n",
        "\n",
        "def remove_sequences_multiple_pos(datalist: list, labellist, fraction=0.5):\n",
        "    datalist, labellist = zip_n_shuffle(datalist, labellist)\n",
        "    i = round(len(datalist) * fraction)\n",
        "    return datalist[:i], labellist[:i]\n",
        "\n",
        "\n",
        "def remove_sequences_equal_multiple_pos(reduced_datalist: list, reduced_labellist: list, compared_datalist: list):\n",
        "    reduced_datalist, reduced_labellist = zip_n_shuffle(reduced_datalist, reduced_labellist)\n",
        "    reduced_datalist = reduced_datalist[:len(compared_datalist)]\n",
        "    reduced_labellist = reduced_labellist[:len(compared_datalist)]\n",
        "    if len(compared_datalist) != len(reduced_datalist) or len(compared_datalist) != len(reduced_labellist):\n",
        "        raise ValueError\n",
        "    return reduced_datalist, reduced_labellist\n",
        "\n",
        "\n",
        "def fuse_sequence_lists_multiple_pos(pos_datalist: list, pos_labellist:list, neg_datalist: list, neg_labellist):\n",
        "    datalist = pos_datalist + neg_datalist\n",
        "    labellist = pos_labellist + neg_labellist\n",
        "    return datalist, labellist\n",
        "\n",
        "\n",
        "def calculate_pos_weights(labellist: list):\n",
        "    total_samples = len(labellist)\n",
        "    label_counts = torch.Tensor(labellist).sum(0)\n",
        "    pos_weights = (total_samples - label_counts) / (label_counts + 1e-5)\n",
        "    return pos_weights"
      ],
      "metadata": {
        "id": "g9TF3MfTr1o6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trainer multiple labels"
      ],
      "metadata": {
        "id": "o0TJEbhQxRLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "class TrainerMultipleClasses:\n",
        "    def __init__(self, model, loss_fn, optimizer, device):\n",
        "        self.model = model.to(device)\n",
        "        self.loss_fn = loss_fn\n",
        "        self.optimizer = optimizer\n",
        "        self.device = device\n",
        "        self.columns = [\n",
        "            'epoch',\n",
        "            'train_accuracy',\n",
        "            'train_precision',\n",
        "            'train_recall',\n",
        "            'train_fscore',\n",
        "            'train_loss',\n",
        "            'test_accuracy',\n",
        "            'test_precision',\n",
        "            'test_recall',\n",
        "            'test_fscore',\n",
        "            'test_loss'\n",
        "        ]\n",
        "        self.df = pd.DataFrame(columns=self.columns)\n",
        "\n",
        "\n",
        "    def _train_one_epoch(self, train_iter):\n",
        "        result_loss = 0\n",
        "        tpos = 0\n",
        "        fpos = 0\n",
        "        tneg = 0\n",
        "        fneg = 0\n",
        "        self.model.train(True)\n",
        "        for i, (inputs, labels) in enumerate(train_iter):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            outputs = self.model(inputs).to('cpu')\n",
        "            labels = labels.type(torch.float32)\n",
        "\n",
        "            loss = loss_fn(input=outputs, target=labels)\n",
        "            result_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            self.optimizer.step()\n",
        "            for b, lab in enumerate(labels):\n",
        "                out = torch.round(torch.sigmoid(outputs[b]))\n",
        "\n",
        "                for j, o in enumerate(out):\n",
        "                    # print(f'{o=}\\t{l=}')\n",
        "                    l = lab[j]\n",
        "                    if o == 1 and l == 1:\n",
        "                        tpos += 1\n",
        "                    elif o == 1 and l == 0:\n",
        "                        fpos += 1\n",
        "                    elif o == 0 and l == 0:\n",
        "                        tneg += 1\n",
        "                    elif o == 0 and l == 1:\n",
        "                        fneg += 1\n",
        "                    else:\n",
        "                        raise ValueError(f'Output or label is not rounded to zero: {o = } {l = }')\n",
        "                    # print(f'{tpos=}\\t{fpos=}\\t{tneg=}\\t{fneg=}')\n",
        "        accuracy = (tpos + tneg) / (tpos + fpos + tneg + fneg)\n",
        "        precision = tpos / (tpos + fpos) if (tpos + fpos) > 0 else 0\n",
        "        recall = tpos / (tpos + fneg) if (tpos + fneg) > 0 else 0\n",
        "        fscore = 2 * tpos / (2*tpos + fpos + fneg) if (2*tpos + fpos + fneg) > 0 else 0\n",
        "        return accuracy, precision, recall, fscore, result_loss / (i+1)\n",
        "\n",
        "\n",
        "    def _test_one_epoch(self, test_iter):\n",
        "        result_loss = 0\n",
        "        tpos = 0\n",
        "        fpos = 0\n",
        "        tneg = 0\n",
        "        fneg = 0\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, (inputs, labels) in enumerate(test_iter):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels\n",
        "                outputs = self.model(inputs).to('cpu')\n",
        "                labels = labels.type(torch.float32)\n",
        "                loss = loss_fn(input=outputs, target=labels)\n",
        "                result_loss += loss.item()\n",
        "                for b, lab in enumerate(labels):\n",
        "                    out = torch.round(torch.sigmoid(outputs[b]))\n",
        "                    for j, o in enumerate(out):\n",
        "                        l = lab[j]\n",
        "                        if o == 1 and l == 1:\n",
        "                            tpos += 1\n",
        "                        elif o == 1 and l == 0:\n",
        "                            fpos += 1\n",
        "                        elif o == 0 and l == 0:\n",
        "                            tneg += 1\n",
        "                        elif o == 0 and l == 1:\n",
        "                            fneg += 1\n",
        "                        else:\n",
        "                            raise ValueError(f'Output or label is not rounded to zero: {o = } {l = }')\n",
        "        # print(f'{tpos=}\\t{fpos=}\\t{tneg=}\\t{fneg=}')\n",
        "        accuracy = (tpos + tneg) / (tpos + fpos + tneg + fneg)\n",
        "        precision = tpos / (tpos + fpos) if (tpos + fpos) > 0 else 0\n",
        "        recall = tpos / (tpos + fneg) if (tpos + fneg) > 0 else 0\n",
        "        fscore = 2 * tpos / (2*tpos + fpos + fneg) if (2*tpos + fpos + fneg) > 0 else 0\n",
        "        return accuracy, precision, recall, fscore, result_loss / (i+1)\n",
        "\n",
        "\n",
        "    def _load_into_dict(self, epoch, train_stats, test_stats):\n",
        "        row = [epoch] + list(train_stats) + list(test_stats)\n",
        "        row = pd.DataFrame(row, index=self.columns).T\n",
        "        self.df = pd.concat([self.df, row], axis=0)\n",
        "\n",
        "\n",
        "    def train(self, epochs, train_iter, test_iter):\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            train_stats = self._train_one_epoch(train_iter)\n",
        "\n",
        "            test_stats = self._test_one_epoch(test_iter)\n",
        "            self._load_into_dict(epoch, train_stats, test_stats)\n",
        "            print(f\"[epoch {epoch:02d}] \"\n",
        "                  f\"train-loss={train_stats[-1]:.4f} | \"\n",
        "                  f\"test-loss={test_stats[-1]:.4f} | \"\n",
        "                  f\"train-acc={train_stats[0]:.4f} | \"\n",
        "                  f\"test-acc={test_stats[0]:.4f} | \"\n",
        "                  f\"P={test_stats[1]:.4f} | R={test_stats[2]:.4f} | F1={test_stats[3]:.4f}\")\n",
        "\n",
        "        return self.df"
      ],
      "metadata": {
        "id": "7xssSozK0uO3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model multiple labels"
      ],
      "metadata": {
        "id": "xsJ8Xb1zxVYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiClassifierCNN1D(nn.Module):\n",
        "    \"\"\"1D Convolutional Neural Network for protein function classification\"\"\"\n",
        "    def __init__(self, num_classes: int, vocab_size: int = 21,  conv_channels: int = 128, use_bias: bool = False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "        # CNN model for binary classification\n",
        "        self.conv1 = nn.Sequential(\n",
        "            # conv block 1\n",
        "            nn.Conv1d(in_channels=vocab_size, out_channels=conv_channels, kernel_size=3, padding='same', bias=use_bias),\n",
        "            nn.BatchNorm1d(conv_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
        "            nn.Dropout(0)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            # conv block 2\n",
        "            nn.Conv1d(in_channels=conv_channels, out_channels=conv_channels, kernel_size=3, padding='same', bias=use_bias),\n",
        "            nn.BatchNorm1d(conv_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(1),\n",
        "            nn.Dropout(0)\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            # conv block 3\n",
        "            nn.Conv1d(in_channels=conv_channels, out_channels=conv_channels, kernel_size=3, padding='same', bias=use_bias),\n",
        "            nn.BatchNorm1d(conv_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveMaxPool1d(1)\n",
        "        )\n",
        "        self.fc1 = nn.Sequential(\n",
        "            # flatten + classification head\n",
        "            nn.Flatten(1, -1),\n",
        "            nn.LazyLinear(out_features=64, bias=use_bias),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0)\n",
        "        )\n",
        "        self.fc2 = nn.Sequential(\n",
        "            # flatten + classification head\n",
        "            nn.LazyLinear(out_features=num_classes, bias=use_bias)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.tensor, targets: torch.tensor = None):\n",
        "        \"\"\"Predict protein function class (0 or 1)\"\"\"\n",
        "        x = self.embedding(x).permute(0,2,1)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.fc1(x)\n",
        "        return self.fc2(x)\n"
      ],
      "metadata": {
        "id": "DvXk1P5yzsRZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load GO data multiple labels"
      ],
      "metadata": {
        "id": "5H-PP8bUxpCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "annot_files = [\n",
        "    \"GO_3A0005576.annotprot\",\n",
        "    \"GO_3A0005739.annotprot\",\n",
        "    \"GO_3A0007165.annotprot\",\n",
        "    \"GO_3A0043066.annotprot\",\n",
        "    \"GO_3A0055085.annotprot\"\n",
        "]\n",
        "\n",
        "batch_size = 10\n",
        "num_steps = 1000\n",
        "\n",
        "dl, ll = read_multiple_pos(\"expr5Tseq_filtGO_100-1000.lis\", annot_files)\n",
        "\n",
        "p_dl, p_ll, n_dl, n_ll = split_labelled_multiple_pos(dl, ll)\n",
        "n_dl, n_ll = remove_sequences_multiple_pos(n_dl, n_ll, 0.1)\n",
        "dl, ll = fuse_sequence_lists_multiple_pos(p_dl, p_ll, n_dl, n_ll)\n",
        "\n",
        "train_dl, train_ll, test_dl, test_ll = generate_train_test(dl, ll, 0.6)\n",
        "train_ds = [train_dl, train_ll]\n",
        "test_ds = [test_dl, test_ll]\n",
        "\n",
        "traindataloader = load_data(batch_size, num_steps, train_ds)\n",
        "testdataloader = load_data(batch_size, num_steps, test_ds)\n",
        "\n",
        "pos_weights = calculate_pos_weights(train_ll)\n",
        "\n",
        "print(next(iter(traindataloader)))"
      ],
      "metadata": {
        "id": "CId5XjpFtW82",
        "outputId": "8e80adb5-3f9b-4b9e-9199-8d4f9c2a9be6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[10,  0, 14,  ..., 20, 20, 20],\n",
            "        [10, 19, 14,  ..., 20, 20, 20],\n",
            "        [10,  0,  6,  ..., 20, 20, 20],\n",
            "        ...,\n",
            "        [10, 17,  6,  ..., 20, 20, 20],\n",
            "        [10, 17,  6,  ..., 20, 20, 20],\n",
            "        [10,  3,  8,  ..., 20, 20, 20]]), tensor([[1, 0, 0, 0, 0],\n",
            "        [0, 1, 0, 0, 0],\n",
            "        [0, 1, 0, 0, 0],\n",
            "        [1, 0, 1, 0, 0],\n",
            "        [0, 0, 1, 0, 0],\n",
            "        [0, 0, 1, 1, 0],\n",
            "        [0, 0, 0, 0, 0],\n",
            "        [1, 0, 0, 0, 0],\n",
            "        [0, 1, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len_list = []\n",
        "for s in dl:\n",
        "    len_list.append(len(s))\n",
        "sns.histplot(len_list)\n",
        "\n",
        "p = 0\n",
        "n = 0\n",
        "for labels in ll:\n",
        "    found_pos = False\n",
        "    for l in labels:\n",
        "        if l:\n",
        "            p += 1\n",
        "            found_pos = True\n",
        "            break\n",
        "    if not found_pos:\n",
        "        n+=1\n",
        "print(f'{p = }\\n{n = }')"
      ],
      "metadata": {
        "id": "JrwN-K7Syzod",
        "outputId": "45c03b8a-9efd-4b65-a98b-6355f74bac2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p = 1454\n",
            "n = 533\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train model multiple labels"
      ],
      "metadata": {
        "id": "bcZoEI7-xuop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultiClassifierCNN1D(num_classes=5, conv_channels=256, use_bias=True)\n",
        "model.apply(init_weights)\n",
        "loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9)\n",
        "model"
      ],
      "metadata": {
        "id": "towK5DH_147q",
        "outputId": "522fb760-6dc9-401e-8bb6-f15194d6fe1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultiClassifierCNN1D(\n",
              "  (embedding): Embedding(21, 21)\n",
              "  (conv1): Sequential(\n",
              "    (0): Conv1d(21, 256, kernel_size=(3,), stride=(1,), padding=same)\n",
              "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (4): Dropout(p=0, inplace=False)\n",
              "  )\n",
              "  (conv2): Sequential(\n",
              "    (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=same)\n",
              "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
              "    (4): Dropout(p=0, inplace=False)\n",
              "  )\n",
              "  (conv3): Sequential(\n",
              "    (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=same)\n",
              "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): AdaptiveMaxPool1d(output_size=1)\n",
              "  )\n",
              "  (fc1): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): LazyLinear(in_features=0, out_features=64, bias=True)\n",
              "    (2): ReLU()\n",
              "    (3): Dropout(p=0, inplace=False)\n",
              "  )\n",
              "  (fc2): Sequential(\n",
              "    (0): LazyLinear(in_features=0, out_features=5, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = TrainerMultipleClasses(model, loss_fn, optimizer, device)\n",
        "df = trainer.train(epochs=50, train_iter=traindataloader, test_iter=testdataloader)"
      ],
      "metadata": {
        "id": "xv-k3v3e2Afo",
        "outputId": "04d5e80a-4bb9-490c-8796-3a3b0b878db6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:823: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:180.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "<ipython-input-6-1a27aa51fb67>:112: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  self.df = pd.concat([self.df, row], axis=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 00] train-loss=1.1920 | test-loss=1.1838 | train-acc=0.4854 | test-acc=0.4060 | P=0.1438 | R=0.5181 | F1=0.2251\n",
            "[epoch 01] train-loss=1.1665 | test-loss=1.1852 | train-acc=0.4055 | test-acc=0.4060 | P=0.1438 | R=0.5181 | F1=0.2251\n",
            "[epoch 02] train-loss=1.1685 | test-loss=1.1838 | train-acc=0.4878 | test-acc=0.4060 | P=0.1438 | R=0.5181 | F1=0.2251\n",
            "[epoch 03] train-loss=1.1643 | test-loss=1.1831 | train-acc=0.4173 | test-acc=0.5572 | P=0.1547 | R=0.3716 | F1=0.2185\n",
            "[epoch 04] train-loss=1.1660 | test-loss=1.1844 | train-acc=0.5163 | test-acc=0.4196 | P=0.1551 | R=0.5589 | F1=0.2429\n",
            "[epoch 05] train-loss=1.1647 | test-loss=1.1842 | train-acc=0.4210 | test-acc=0.5572 | P=0.1547 | R=0.3716 | F1=0.2185\n",
            "[epoch 06] train-loss=1.1691 | test-loss=1.1840 | train-acc=0.4653 | test-acc=0.4553 | P=0.1849 | R=0.6662 | F1=0.2895\n",
            "[epoch 07] train-loss=1.1669 | test-loss=1.1838 | train-acc=0.5508 | test-acc=0.4181 | P=0.1539 | R=0.5544 | F1=0.2409\n",
            "[epoch 08] train-loss=1.1670 | test-loss=1.1853 | train-acc=0.4257 | test-acc=0.3824 | P=0.1241 | R=0.4471 | F1=0.1943\n",
            "[epoch 09] train-loss=1.1673 | test-loss=1.1857 | train-acc=0.4656 | test-acc=0.4181 | P=0.1539 | R=0.5544 | F1=0.2409\n",
            "[epoch 10] train-loss=1.1700 | test-loss=1.1847 | train-acc=0.4508 | test-acc=0.5572 | P=0.1547 | R=0.3716 | F1=0.2185\n",
            "[epoch 11] train-loss=1.1634 | test-loss=1.1858 | train-acc=0.3804 | test-acc=0.4060 | P=0.1438 | R=0.5181 | F1=0.2251\n",
            "[epoch 12] train-loss=1.1653 | test-loss=1.1822 | train-acc=0.3626 | test-acc=0.4060 | P=0.1438 | R=0.5181 | F1=0.2251\n",
            "[epoch 13] train-loss=1.1694 | test-loss=1.1830 | train-acc=0.4753 | test-acc=0.6176 | P=0.2302 | R=0.5529 | F1=0.3250\n",
            "[epoch 14] train-loss=1.1632 | test-loss=1.1831 | train-acc=0.5515 | test-acc=0.3177 | P=0.1777 | R=0.8535 | F1=0.2941\n",
            "[epoch 15] train-loss=1.1620 | test-loss=1.1845 | train-acc=0.4270 | test-acc=0.5940 | P=0.2006 | R=0.4819 | F1=0.2833\n",
            "[epoch 16] train-loss=1.1666 | test-loss=1.1836 | train-acc=0.6908 | test-acc=0.5940 | P=0.2006 | R=0.4819 | F1=0.2833\n",
            "[epoch 17] train-loss=1.1643 | test-loss=1.1831 | train-acc=0.5297 | test-acc=0.5804 | P=0.1836 | R=0.4411 | F1=0.2593\n",
            "[epoch 18] train-loss=1.1634 | test-loss=1.1851 | train-acc=0.3639 | test-acc=0.6823 | P=0.1220 | R=0.1465 | F1=0.1332\n",
            "[epoch 19] train-loss=1.1644 | test-loss=1.1848 | train-acc=0.5763 | test-acc=0.5683 | P=0.1686 | R=0.4048 | F1=0.2380\n",
            "[epoch 20] train-loss=1.1643 | test-loss=1.1829 | train-acc=0.4807 | test-acc=0.4307 | P=0.1644 | R=0.5921 | F1=0.2573\n",
            "[epoch 21] train-loss=1.1654 | test-loss=1.1822 | train-acc=0.5190 | test-acc=0.4800 | P=0.2055 | R=0.7402 | F1=0.3216\n",
            "[epoch 22] train-loss=1.1630 | test-loss=1.1844 | train-acc=0.5545 | test-acc=0.4800 | P=0.2055 | R=0.7402 | F1=0.3216\n",
            "[epoch 23] train-loss=1.1656 | test-loss=1.1834 | train-acc=0.5096 | test-acc=0.4553 | P=0.1849 | R=0.6662 | F1=0.2895\n",
            "[epoch 24] train-loss=1.1675 | test-loss=1.1859 | train-acc=0.4351 | test-acc=0.4196 | P=0.1551 | R=0.5589 | F1=0.2429\n",
            "[epoch 25] train-loss=1.1653 | test-loss=1.1839 | train-acc=0.5086 | test-acc=0.5336 | P=0.1252 | R=0.3006 | F1=0.1767\n",
            "[epoch 26] train-loss=1.1648 | test-loss=1.1820 | train-acc=0.4163 | test-acc=0.4428 | P=0.1744 | R=0.6284 | F1=0.2731\n",
            "[epoch 27] train-loss=1.1659 | test-loss=1.1844 | train-acc=0.5126 | test-acc=0.4196 | P=0.1551 | R=0.5589 | F1=0.2429\n",
            "[epoch 28] train-loss=1.1675 | test-loss=1.1859 | train-acc=0.4012 | test-acc=0.4060 | P=0.1438 | R=0.5181 | F1=0.2251\n",
            "[epoch 29] train-loss=1.1683 | test-loss=1.1836 | train-acc=0.4015 | test-acc=0.4307 | P=0.1644 | R=0.5921 | F1=0.2573\n",
            "[epoch 30] train-loss=1.1654 | test-loss=1.1851 | train-acc=0.3844 | test-acc=0.3177 | P=0.1777 | R=0.8535 | F1=0.2941\n",
            "[epoch 31] train-loss=1.1632 | test-loss=1.1826 | train-acc=0.5549 | test-acc=0.5200 | P=0.1082 | R=0.2598 | F1=0.1528\n",
            "[epoch 32] train-loss=1.1647 | test-loss=1.1854 | train-acc=0.6173 | test-acc=0.5200 | P=0.1082 | R=0.2598 | F1=0.1528\n",
            "[epoch 33] train-loss=1.1635 | test-loss=1.1841 | train-acc=0.4287 | test-acc=0.5572 | P=0.1547 | R=0.3716 | F1=0.2185\n",
            "[epoch 34] train-loss=1.1675 | test-loss=1.1847 | train-acc=0.5622 | test-acc=0.5683 | P=0.1686 | R=0.4048 | F1=0.2380\n",
            "[epoch 35] train-loss=1.1633 | test-loss=1.1830 | train-acc=0.3263 | test-acc=0.4307 | P=0.1644 | R=0.5921 | F1=0.2573\n",
            "[epoch 36] train-loss=1.1660 | test-loss=1.1830 | train-acc=0.4216 | test-acc=0.5447 | P=0.1390 | R=0.3338 | F1=0.1963\n",
            "[epoch 37] train-loss=1.1675 | test-loss=1.1845 | train-acc=0.5421 | test-acc=0.6712 | P=0.0943 | R=0.1133 | F1=0.1030\n",
            "[epoch 38] train-loss=1.1661 | test-loss=1.1819 | train-acc=0.4904 | test-acc=0.2684 | P=0.1469 | R=0.7054 | F1=0.2431\n",
            "[epoch 39] train-loss=1.1667 | test-loss=1.1860 | train-acc=0.4458 | test-acc=0.5572 | P=0.1547 | R=0.3716 | F1=0.2185\n",
            "[epoch 40] train-loss=1.1637 | test-loss=1.1818 | train-acc=0.4458 | test-acc=0.5447 | P=0.1390 | R=0.3338 | F1=0.1963\n",
            "[epoch 41] train-loss=1.1660 | test-loss=1.1861 | train-acc=0.3334 | test-acc=0.5447 | P=0.1390 | R=0.3338 | F1=0.1963\n",
            "[epoch 42] train-loss=1.1658 | test-loss=1.1855 | train-acc=0.4320 | test-acc=0.4181 | P=0.1539 | R=0.5544 | F1=0.2409\n",
            "[epoch 43] train-loss=1.1658 | test-loss=1.1833 | train-acc=0.5450 | test-acc=0.5203 | P=0.1082 | R=0.2598 | F1=0.1528\n",
            "[epoch 44] train-loss=1.1646 | test-loss=1.1839 | train-acc=0.4364 | test-acc=0.3824 | P=0.1241 | R=0.4471 | F1=0.1943\n",
            "[epoch 45] train-loss=1.1632 | test-loss=1.1839 | train-acc=0.5015 | test-acc=0.5200 | P=0.1082 | R=0.2598 | F1=0.1528\n",
            "[epoch 46] train-loss=1.1642 | test-loss=1.1840 | train-acc=0.6495 | test-acc=0.4553 | P=0.1849 | R=0.6662 | F1=0.2895\n",
            "[epoch 47] train-loss=1.1653 | test-loss=1.1843 | train-acc=0.4831 | test-acc=0.7316 | P=0.2453 | R=0.2946 | F1=0.2677\n",
            "[epoch 48] train-loss=1.1641 | test-loss=1.1841 | train-acc=0.5146 | test-acc=0.5940 | P=0.2006 | R=0.4819 | F1=0.2833\n",
            "[epoch 49] train-loss=1.1633 | test-loss=1.1835 | train-acc=0.6059 | test-acc=0.5693 | P=0.1698 | R=0.4079 | F1=0.2398\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('multi-label_classifier.csv')"
      ],
      "metadata": {
        "id": "RiussApswGYU"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}