{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Bioinformatics Project 2025 - Motif CNN & GO Prediction\n",
        "\n",
        "**Course:** GRS34806 Deep Learning\n",
        "\n",
        "**Authors:** Berkay Helvaci & Maurits Naber\n",
        "\n",
        "**Date:**\n",
        "\n"
      ],
      "metadata": {
        "id": "HuP5FaUF6YLo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries"
      ],
      "metadata": {
        "id": "YbtfKAzz6ct5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clean the code last time! also the redundant libraries."
      ],
      "metadata": {
        "id": "amk7R7B2kcma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "! git clone https://git.wur.nl/bioinformatics/grs34806-deep-learning-project-data.git -q\n",
        "! git clone https://github.com/maussn/GRS34806-project.git -q\n",
        "os.chdir(Path('grs34806-deep-learning-project-data'))"
      ],
      "metadata": {
        "id": "Emlqnf_rAIWr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from datetime import datetime\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import random\n",
        "import seaborn as sns\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "3ctb6CE_bWXP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data I/O & Tokenisation"
      ],
      "metadata": {
        "id": "g96CRPCRCbdx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. read() --------------------------------------------------------------------\n",
        "def read(seqfile: str, posfile: str) -> tuple[list, list]:\n",
        "    \"\"\"Read file with sequences and file with positive cases into lists.\n",
        "\n",
        "    :param seqfile: file with sequences\n",
        "    :type seqfile: str\n",
        "    :param posfile: file with positive cases (annotated with function)\n",
        "    :type posfile: str\n",
        "    :return: two lists, first with sequences and second with boolean labels\n",
        "    :rtype: tuple[list, list]\n",
        "    \"\"\"\n",
        "    idlist = []\n",
        "    datalist = []\n",
        "    labellist = []\n",
        "    with open(seqfile, 'r') as f:\n",
        "        for line in f.readlines():\n",
        "            line = line.rstrip().split('\\t')\n",
        "            idlist.append(line[0])\n",
        "            datalist.append(line[1])\n",
        "            labellist.append(False)\n",
        "    with open(posfile, 'r') as f:\n",
        "        for line in f.readlines():\n",
        "            id = line.rstrip()\n",
        "            try:\n",
        "                i = idlist.index(id)\n",
        "                labellist[i] = True\n",
        "            except ValueError:\n",
        "                continue\n",
        "    return datalist, labellist\n",
        "\n",
        "\n",
        "# 2. split_labelled() ----------------------------------------------------------\n",
        "def split_labelled(datalist: list, labellist: list):\n",
        "    \"\"\"Return two separate sequence lists: positives & negatives.\"\"\"\n",
        "    pos_datalist = []\n",
        "    neg_datalist = []\n",
        "    for i, label in enumerate(labellist):\n",
        "        if label:\n",
        "            pos_datalist.append(datalist[i])\n",
        "        else:\n",
        "            neg_datalist.append(datalist[i])\n",
        "    return pos_datalist, neg_datalist\n",
        "\n",
        "\n",
        "# 3. remove_sequences() -----\n",
        "def remove_sequences(datalist: list, fraction=0.5):\n",
        "    \"\"\"Randomly keeps half of the list\"\"\"\n",
        "    random.shuffle(datalist)\n",
        "    keep = round(len(datalist) * fraction)\n",
        "    return datalist[:keep]\n",
        "\n",
        "\n",
        "# 4. fuse_sequence_lists() ------------\n",
        "def fuse_sequence_lists(pos_datalist: list, neg_datalist: list):\n",
        "    \"\"\"Merge postives and negetaves into one list + label\"\"\"\n",
        "    pos_labels = [True for _ in pos_datalist]\n",
        "    neg_labels = [False for _ in neg_datalist]\n",
        "    datalist = pos_datalist + neg_datalist\n",
        "    labellist = pos_labels + neg_labels\n",
        "    return datalist, labellist\n",
        "\n",
        "\n",
        "# 5. generate_train_test() --------\n",
        "def generate_train_test(datalist: list, labellist: list, fraction: float=0.8):\n",
        "    \"\"\"Split up dataset in training set and test set\n",
        "\n",
        "    :param datalist: list with sequences\n",
        "    :type datalist: list\n",
        "    :param labellist: list with labels\n",
        "    :type labellist: list\n",
        "    :param ratio: fraction to be added to the training set, remainder is added to the test set, defaults to 0.8\n",
        "    :type ratio: float, optional\n",
        "    :return: four lists, first two the training data and labels, second two the test data and labels\n",
        "    :rtype: tuple[list, list, list, list]\n",
        "    \"\"\"\n",
        "    c = list(zip(datalist, labellist))\n",
        "    random.shuffle(c)\n",
        "    datalist[:], labellist[:] = zip(*c)\n",
        "    i = round(len(datalist) * fraction)\n",
        "    traindatalist = datalist[:i]\n",
        "    trainlabellist = labellist[:i]\n",
        "    testdatalist = datalist[i:]\n",
        "    testlabellist = labellist[i:]\n",
        "    return traindatalist, trainlabellist,testdatalist,testlabellist\n",
        "\n",
        "\n",
        "# 6. Tokenisation & Padding --------\n",
        "def tokenize(data: list, map2num: dict, non_aa_num: int=20) -> list:\n",
        "    \"\"\"Tokenize all sequences in a list\n",
        "\n",
        "    :param data: list of sequences to tokenize\n",
        "    :type data: list\n",
        "    :param map2num: ammino acid -> integer token mapping\n",
        "    :type map2num: dict\n",
        "    :param non_aa_num: token for non amino acid characters, defaults to 20\n",
        "    :type non_aa_num: int, optional\n",
        "    :return: list of tokenized sequences\n",
        "    :rtype: list\n",
        "    \"\"\"\n",
        "    seq = []\n",
        "    for count, i in enumerate(data):\n",
        "        seq.append([map2num.get(j,non_aa_num) for j in list(i)])\n",
        "    return seq\n",
        "\n",
        "\n",
        "def truncate_pad(line: list, num_steps: int, padding_token: int) -> list:\n",
        "    \"\"\"Truncate or pad a tokenized sequence\n",
        "\n",
        "    :param line: tokenized sequence\n",
        "    :type line: list\n",
        "    :param num_steps: maximum sequence length\n",
        "    :type num_steps: int\n",
        "    :param padding_token: token to be used for padding\n",
        "    :type padding_token: int\n",
        "    :return: truncated/padded sequence\n",
        "    :rtype: list\n",
        "    \"\"\"\n",
        "    if len(line) > num_steps:\n",
        "        return line[:num_steps] # Truncate\n",
        "    return line + [padding_token] * (num_steps - len(line)) # Pad\n",
        "\n",
        "\n",
        "def build_seq_array(lines: list, num_steps: int, non_aa_num: int=20) -> torch.tensor:\n",
        "    \"\"\"Truncate or pad tokenized sequences and convert to tensor\n",
        "\n",
        "    :param lines: tokenized sequences\n",
        "    :type lines: list\n",
        "    :param num_steps: maximum sequence length\n",
        "    :type num_steps: int\n",
        "    :param non_aa_num: token for padding, defaults to 20\n",
        "    :type non_aa_num: int, optional\n",
        "    :return: tensor with truncated/padded tokenized sequences\n",
        "    :rtype: torch.tensor\n",
        "    \"\"\"\n",
        "    return torch.tensor([truncate_pad(l, num_steps, non_aa_num) for l in lines], dtype=torch.long)\n",
        "\n",
        "\n",
        "# 7. load_array() & load_data()\n",
        "def load_array(data_arrays: tuple[torch.tensor, torch.tensor], batch_size: int, is_train: bool=True) -> torch.utils.data.DataLoader:\n",
        "    \"\"\"Construct a PyTorch data iterator.\n",
        "\n",
        "    Taken from d2l package\"\"\"\n",
        "    dataset = torch.utils.data.TensorDataset(*data_arrays)\n",
        "    return torch.utils.data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
        "\n",
        "\n",
        "def load_data(batch_size: int, num_steps: int, dataset: tuple[list, list]) -> torch.utils.data.DataLoader:\n",
        "    \"\"\"Tokenize sequence/label dataset and load into dataloader.\n",
        "\n",
        "    :param batch_size: size of each batch\n",
        "    :type batch_size: int\n",
        "    :param num_steps: maximum sequence length\n",
        "    :type num_steps: int\n",
        "    :param dataset: first list contains sequences, second labels\n",
        "    :type dataset: tuple[list, list]\n",
        "    :return: torch dataloader which gives a tensor of sequences in a batch and a tensor with their labels\n",
        "    :rtype: torch.utils.data.DataLoader\n",
        "    \"\"\"\n",
        "    mapaa2num = {aa: i for (i, aa) in enumerate(list(\"ACDEFGHIKLMNPQRSTVWY\"))}\n",
        "    seq,lab = dataset\n",
        "    seq = tokenize(seq, mapaa2num)\n",
        "    seq_array = build_seq_array(seq, num_steps)\n",
        "    data_arrays = (seq_array, torch.tensor(lab, dtype=torch.long))\n",
        "    data_iter = load_array(data_arrays, batch_size)\n",
        "    return data_iter\n",
        "\n",
        "\n",
        "def read_fasta(fasta):\n",
        "    seqs = []\n",
        "    with open(fasta, 'r') as f:\n",
        "        for line in f.readlines():\n",
        "            if line[0] == '>':\n",
        "                continue\n",
        "            else:\n",
        "                seqs.append(line.rstrip('\\n'))\n",
        "    return seqs"
      ],
      "metadata": {
        "id": "eHpedW0hqx1V"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loader"
      ],
      "metadata": {
        "id": "_b5Kq5jEKZwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datalist, labellist = read(\"len200_500_n5000nr1.seq\", \"len200_500_n5000nr1.pos\")\n",
        "\n",
        "batch_size = 10\n",
        "num_steps = 1000 # max sequence length\n",
        "\n",
        "traindatalist, trainlabellist, testdatalist, testlabellist = generate_train_test(datalist, labellist, 0.8)\n",
        "traindataset = [traindatalist, trainlabellist]\n",
        "testdataset = [testdatalist, testlabellist]\n",
        "\n",
        "# Set batch_size and num_steps (maximum sequence length)\n",
        "train_iter = load_data(batch_size, num_steps, traindataset)\n",
        "test_iter = load_data(batch_size, num_steps, testdataset)\n",
        "\n",
        "print(\"batch shape  :\", next(iter(train_iter))[0].shape)\n",
        "print(\"labels shape :\", next(iter(train_iter))[1].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyRvhoyiwv_g",
        "outputId": "9a5774f7-b9ae-4857-fb9b-1cb5c0e282c6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch shape  : torch.Size([10, 1000])\n",
            "labels shape : torch.Size([10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training / Evaluation"
      ],
      "metadata": {
        "id": "BhuHUN6TRLun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(layer):\n",
        "    if type(layer) == nn.Linear or type(layer) == nn.Conv1d:\n",
        "        nn.init.xavier_uniform_(layer.weight)\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model, loss_fn, optimizer, device='cuda'):\n",
        "        self.model = model.to(device)\n",
        "        self.loss_fn = loss_fn\n",
        "        self.optimizer = optimizer\n",
        "        self.device = device\n",
        "        self.columns = [\n",
        "            'epoch',\n",
        "            'train_accuracy',\n",
        "            'train_precision',\n",
        "            'train_recall',\n",
        "            'train_fscore',\n",
        "            'train_loss',\n",
        "            'test_accuracy',\n",
        "            'test_precision',\n",
        "            'test_recall',\n",
        "            'test_fscore',\n",
        "            'test_loss'\n",
        "        ]\n",
        "        self.df = pd.DataFrame(columns=self.columns)\n",
        "\n",
        "\n",
        "    # One training epoch -------------------------------------------------------\n",
        "    def _train_one_epoch(self, train_iter):\n",
        "        result_loss = 0\n",
        "        tpos = fpos = tneg = fneg = 0\n",
        "        self.model.train(True)\n",
        "\n",
        "        for i, (inputs, labels) in enumerate(train_iter):\n",
        "            inputs = inputs.to(self.device)\n",
        "            labels = labels.to(self.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            outputs = self.model(inputs)\n",
        "            loss = self.loss_fn(outputs, labels)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            result_loss += loss.item()\n",
        "\n",
        "            # Confusion matrix calculation\n",
        "            for j, l in enumerate(labels):\n",
        "                o = outputs[j].tolist().index(max(outputs[j]))\n",
        "                l = l.item()\n",
        "                if o == 1 and l == 1:\n",
        "                    tpos += 1\n",
        "                elif o == 1 and l == 0:\n",
        "                    fpos += 1\n",
        "                elif o == 0 and l == 0:\n",
        "                    tneg += 1\n",
        "                elif o == 0 and l == 1:\n",
        "                    fneg += 1\n",
        "                else:\n",
        "                    raise ValueError(f'Output or label is not rounded to zero: {o = } {l = }')\n",
        "\n",
        "        accuracy = (tpos + tneg) / (tpos + fpos + tneg + fneg)\n",
        "        precision = tpos / (tpos + fpos) if (tpos + fpos) > 0 else 0\n",
        "        recall = tpos / (tpos + fneg) if (tpos + fneg) > 0 else 0\n",
        "        fscore = 2 * tpos / (2*tpos + fpos + fneg) if (2*tpos + fpos + fneg) > 0 else 0\n",
        "\n",
        "        return accuracy, precision, recall, fscore, result_loss / (i+1)\n",
        "\n",
        "    # Evaluation epoch ---------------------------------------------------------\n",
        "    def _test_one_epoch(self, test_iter):\n",
        "        result_loss = 0\n",
        "        tpos = fpos = tneg = fneg = 0\n",
        "        self.model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, (inputs, labels) in enumerate(test_iter, start=1):\n",
        "                inputs = inputs.to(self.device)\n",
        "                labels = labels.to(self.device)\n",
        "\n",
        "                outputs = self.model(inputs)\n",
        "                loss = self.loss_fn(input=outputs, target=labels)\n",
        "                result_loss += loss.item()\n",
        "\n",
        "                # Confusion matrix calculation\n",
        "                for j, l in enumerate(labels):\n",
        "                    o = outputs[j].tolist().index(max(outputs[j]))\n",
        "                    l = l.item()\n",
        "                    if o == 1 and l == 1:\n",
        "                        tpos += 1\n",
        "                    elif o == 1 and l == 0:\n",
        "                        fpos += 1\n",
        "                    elif o == 0 and l == 0:\n",
        "                        tneg += 1\n",
        "                    elif o == 0 and l == 1:\n",
        "                        fneg += 1\n",
        "                    else:\n",
        "                        raise ValueError(f'Output or label is not rounded to zero: {o = } {l = }')\n",
        "\n",
        "        accuracy = (tpos + tneg) / (tpos + fpos + tneg + fneg)\n",
        "        precision = tpos / (tpos + fpos) if (tpos + fpos) > 0 else 0\n",
        "        recall = tpos / (tpos + fneg) if (tpos + fneg) > 0 else 0\n",
        "        fscore = 2 * tpos / (2*tpos + fpos + fneg) if (2*tpos + fpos + fneg) > 0 else 0\n",
        "\n",
        "        return accuracy, precision, recall, fscore, result_loss / (i+1)\n",
        "\n",
        "\n",
        "    def _load_into_dict(self, epoch, train_stats, test_stats):\n",
        "        row = [epoch] + list(train_stats) + list(test_stats)\n",
        "        row = pd.DataFrame(row, index=self.columns).T\n",
        "        self.df = pd.concat([self.df, row], axis=0)\n",
        "\n",
        "\n",
        "    def train(self, epochs, train_iter, test_iter):\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            train_stats = self._train_one_epoch(train_iter)\n",
        "\n",
        "            test_stats = self._test_one_epoch(test_iter)\n",
        "            self._load_into_dict(epoch, train_stats, test_stats)\n",
        "            print(f\"[epoch {epoch:02d}] \"\n",
        "                  f\"train-loss={train_stats[-1]:.4f} | \"\n",
        "                  f\"test-loss={test_stats[-1]:.4f} | \"\n",
        "                  f\"train-acc={train_stats[0]:.4f} | \"\n",
        "                  f\"test-acc={test_stats[0]:.4f} | \"\n",
        "                  f\"P={test_stats[1]:.4f} | R={test_stats[2]:.4f} | F1={test_stats[3]:.4f}\")\n",
        "\n",
        "        return self.df"
      ],
      "metadata": {
        "id": "C0xPrmp3jHjD"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BerryCNN1D(nn.Module):\n",
        "    \"\"\"1D Convolutional Neural Network for protein function classification\"\"\"\n",
        "    def __init__(self, vocab_size: int = 21,\n",
        "                 dropout_rate = 0, conv_channels: int = 128,\n",
        "                 use_bias: bool = False):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "        # CNN model for binary classification\n",
        "        self.conv1 = nn.Sequential(\n",
        "            # conv block 1\n",
        "            nn.Conv1d(in_channels=vocab_size, out_channels=conv_channels, kernel_size=3, padding='same', bias=use_bias),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            # conv block 2\n",
        "            nn.Conv1d(in_channels=conv_channels, out_channels=conv_channels, kernel_size=3, padding='same', bias=use_bias),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveMaxPool1d(1)\n",
        "        )\n",
        "\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Sequential(\n",
        "            # flatten + classification head\n",
        "            nn.Flatten(1, -1),\n",
        "            nn.LazyLinear(out_features=2, bias=use_bias)  # binary classification\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.tensor, targets: torch.tensor = None):\n",
        "        \"\"\"Predict protein function class (0 or 1)\"\"\"\n",
        "        x = self.embedding(x).permute(0,2,1)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.dropout(x)\n",
        "        return self.fc(x)\n"
      ],
      "metadata": {
        "id": "2V5v63DoLaLr"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Model"
      ],
      "metadata": {
        "id": "UeAeB8Wo79Oj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "param_grid = {\n",
        "    'dropout_rate': [0, 0.3],\n",
        "    'lr': [0.01, 0.001],\n",
        "    'momentum': [0, 0.5],\n",
        "    'conv_channels': [64, 128]\n",
        "}\n",
        "\n",
        "grid = list(ParameterGrid(param_grid))\n",
        "\n",
        "best_acc = 0\n",
        "best_params = None\n",
        "\n",
        "df = None\n",
        "\n",
        "for params in grid:\n",
        "    print(\"Current hyper-parameters:\", params)\n",
        "    model = BerryCNN1D(\n",
        "        vocab_size=21,\n",
        "        dropout_rate=params['dropout_rate'],\n",
        "        conv_channels=params['conv_channels']\n",
        "    )\n",
        "    model.apply(init_weights)\n",
        "\n",
        "    optimizer = torch.optim.SGD(\n",
        "        model.parameters(),\n",
        "        lr=params['lr'],\n",
        "        momentum=params['momentum']\n",
        "    )\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    trainer = Trainer(model, loss_fn, optimizer, device)\n",
        "\n",
        "    out_df = trainer.train(epochs=10, train_iter=train_iter, test_iter=test_iter)\n",
        "\n",
        "    for p in params.keys():\n",
        "        out_df[p] = params[p]\n",
        "\n",
        "    acc = out_df['test_accuracy'].max()\n",
        "\n",
        "    if acc >= best_acc:\n",
        "        best_acc = []\n",
        "        best_acc = acc\n",
        "        best_params = params\n",
        "        print(f\"New best accuracy {best_acc:.4f} with {best_params} \\n\")\n",
        "\n",
        "    if type(df) != pd.DataFrame:\n",
        "        df = out_df\n",
        "    else:\n",
        "        df = pd.concat([df, out_df], axis=0)\n",
        "\n",
        "print(\"Best hyperâ€‘parameters found:\", best_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-HESI-RwU5o",
        "outputId": "4f82f392-8efa-42a8-ecaf-b523e8d3d7ba"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current hyper-parameters: {'conv_channels': 64, 'dropout_rate': 0, 'lr': 0.01, 'momentum': 0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-683adbe3301c>:107: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  self.df = pd.concat([self.df, row], axis=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 00] train-loss=0.4473 | test-loss=0.0441 | train-acc=0.7605 | test-acc=0.9990 | P=0.9981 | R=1.0000 | F1=0.9990\n",
            "[epoch 01] train-loss=0.0196 | test-loss=0.0093 | train-acc=0.9992 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 02] train-loss=0.0068 | test-loss=0.0045 | train-acc=0.9995 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 03] train-loss=0.0042 | test-loss=0.0030 | train-acc=0.9998 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 04] train-loss=0.0030 | test-loss=0.0021 | train-acc=0.9998 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 05] train-loss=0.0023 | test-loss=0.0017 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 06] train-loss=0.0018 | test-loss=0.0013 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 07] train-loss=0.0015 | test-loss=0.0013 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 08] train-loss=0.0013 | test-loss=0.0010 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 09] train-loss=0.0011 | test-loss=0.0009 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "New best accuracy 1.0000 with {'conv_channels': 64, 'dropout_rate': 0, 'lr': 0.01, 'momentum': 0} \n",
            "\n",
            "Current hyper-parameters: {'conv_channels': 64, 'dropout_rate': 0, 'lr': 0.01, 'momentum': 0.5}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-683adbe3301c>:107: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  self.df = pd.concat([self.df, row], axis=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 00] train-loss=0.1593 | test-loss=0.0078 | train-acc=0.9337 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 01] train-loss=0.0044 | test-loss=0.0031 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 02] train-loss=0.0021 | test-loss=0.0019 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 03] train-loss=0.0013 | test-loss=0.0014 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 04] train-loss=0.0010 | test-loss=0.0011 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 05] train-loss=0.0008 | test-loss=0.0009 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 06] train-loss=0.0006 | test-loss=0.0008 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 07] train-loss=0.0005 | test-loss=0.0007 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 08] train-loss=0.0004 | test-loss=0.0006 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 09] train-loss=0.0004 | test-loss=0.0006 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "New best accuracy 1.0000 with {'conv_channels': 64, 'dropout_rate': 0, 'lr': 0.01, 'momentum': 0.5} \n",
            "\n",
            "Current hyper-parameters: {'conv_channels': 64, 'dropout_rate': 0, 'lr': 0.001, 'momentum': 0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-683adbe3301c>:107: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  self.df = pd.concat([self.df, row], axis=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 00] train-loss=0.7035 | test-loss=0.6826 | train-acc=0.5045 | test-acc=0.5390 | P=0.5675 | R=0.4769 | F1=0.5183\n",
            "[epoch 01] train-loss=0.6851 | test-loss=0.6702 | train-acc=0.5605 | test-acc=0.5750 | P=0.5604 | R=0.8481 | F1=0.6748\n",
            "[epoch 02] train-loss=0.6657 | test-loss=0.6565 | train-acc=0.6310 | test-acc=0.6230 | P=0.8824 | R=0.3173 | F1=0.4668\n",
            "[epoch 03] train-loss=0.6267 | test-loss=0.5976 | train-acc=0.7220 | test-acc=0.7510 | P=0.8538 | R=0.6288 | F1=0.7243\n",
            "[epoch 04] train-loss=0.5370 | test-loss=0.4589 | train-acc=0.8570 | test-acc=0.9550 | P=0.9857 | R=0.9269 | F1=0.9554\n",
            "[epoch 05] train-loss=0.3603 | test-loss=0.2700 | train-acc=0.9895 | test-acc=0.9960 | P=0.9924 | R=1.0000 | F1=0.9962\n",
            "[epoch 06] train-loss=0.2055 | test-loss=0.1541 | train-acc=0.9990 | test-acc=0.9970 | P=0.9943 | R=1.0000 | F1=0.9971\n",
            "[epoch 07] train-loss=0.1196 | test-loss=0.0940 | train-acc=0.9995 | test-acc=0.9970 | P=0.9943 | R=1.0000 | F1=0.9971\n",
            "[epoch 08] train-loss=0.0766 | test-loss=0.0642 | train-acc=1.0000 | test-acc=0.9970 | P=0.9943 | R=1.0000 | F1=0.9971\n",
            "[epoch 09] train-loss=0.0538 | test-loss=0.0474 | train-acc=1.0000 | test-acc=0.9980 | P=0.9962 | R=1.0000 | F1=0.9981\n",
            "Current hyper-parameters: {'conv_channels': 64, 'dropout_rate': 0, 'lr': 0.001, 'momentum': 0.5}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-683adbe3301c>:107: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  self.df = pd.concat([self.df, row], axis=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 00] train-loss=0.6966 | test-loss=0.6864 | train-acc=0.5002 | test-acc=0.5140 | P=0.5500 | R=0.3596 | F1=0.4349\n",
            "[epoch 01] train-loss=0.6840 | test-loss=0.6629 | train-acc=0.5590 | test-acc=0.6290 | P=0.5995 | R=0.8635 | F1=0.7076\n",
            "[epoch 02] train-loss=0.5593 | test-loss=0.3560 | train-acc=0.8650 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 03] train-loss=0.1860 | test-loss=0.0907 | train-acc=0.9992 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 04] train-loss=0.0578 | test-loss=0.0404 | train-acc=0.9995 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 05] train-loss=0.0295 | test-loss=0.0244 | train-acc=0.9998 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 06] train-loss=0.0189 | test-loss=0.0172 | train-acc=0.9998 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 07] train-loss=0.0137 | test-loss=0.0131 | train-acc=0.9998 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 08] train-loss=0.0106 | test-loss=0.0105 | train-acc=0.9998 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 09] train-loss=0.0086 | test-loss=0.0088 | train-acc=0.9998 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "New best accuracy 1.0000 with {'conv_channels': 64, 'dropout_rate': 0, 'lr': 0.001, 'momentum': 0.5} \n",
            "\n",
            "Current hyper-parameters: {'conv_channels': 64, 'dropout_rate': 0.3, 'lr': 0.01, 'momentum': 0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-683adbe3301c>:107: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  self.df = pd.concat([self.df, row], axis=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 00] train-loss=0.6026 | test-loss=0.1952 | train-acc=0.6530 | test-acc=0.9950 | P=0.9905 | R=1.0000 | F1=0.9952\n",
            "[epoch 01] train-loss=0.1435 | test-loss=0.0372 | train-acc=0.9580 | test-acc=0.9950 | P=0.9905 | R=1.0000 | F1=0.9952\n",
            "[epoch 02] train-loss=0.0524 | test-loss=0.0110 | train-acc=0.9858 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 03] train-loss=0.0369 | test-loss=0.0081 | train-acc=0.9900 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 04] train-loss=0.0270 | test-loss=0.0052 | train-acc=0.9915 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 05] train-loss=0.0237 | test-loss=0.0077 | train-acc=0.9940 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 06] train-loss=0.0216 | test-loss=0.0068 | train-acc=0.9955 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 07] train-loss=0.0211 | test-loss=0.0033 | train-acc=0.9955 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 08] train-loss=0.0207 | test-loss=0.0044 | train-acc=0.9938 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 09] train-loss=0.0182 | test-loss=0.0046 | train-acc=0.9948 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "New best accuracy 1.0000 with {'conv_channels': 64, 'dropout_rate': 0.3, 'lr': 0.01, 'momentum': 0} \n",
            "\n",
            "Current hyper-parameters: {'conv_channels': 64, 'dropout_rate': 0.3, 'lr': 0.01, 'momentum': 0.5}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-683adbe3301c>:107: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  self.df = pd.concat([self.df, row], axis=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 00] train-loss=0.4366 | test-loss=0.0229 | train-acc=0.7542 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 01] train-loss=0.0356 | test-loss=0.0106 | train-acc=0.9912 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 02] train-loss=0.0244 | test-loss=0.0062 | train-acc=0.9948 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 03] train-loss=0.0188 | test-loss=0.0068 | train-acc=0.9955 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 04] train-loss=0.0096 | test-loss=0.0046 | train-acc=0.9982 | test-acc=0.9990 | P=0.9981 | R=1.0000 | F1=0.9990\n",
            "[epoch 05] train-loss=0.0073 | test-loss=0.0025 | train-acc=0.9988 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 06] train-loss=0.0065 | test-loss=0.0026 | train-acc=0.9982 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 07] train-loss=0.0044 | test-loss=0.0024 | train-acc=0.9995 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 08] train-loss=0.0050 | test-loss=0.0016 | train-acc=0.9988 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 09] train-loss=0.0053 | test-loss=0.0039 | train-acc=0.9988 | test-acc=0.9990 | P=0.9981 | R=1.0000 | F1=0.9990\n",
            "New best accuracy 1.0000 with {'conv_channels': 64, 'dropout_rate': 0.3, 'lr': 0.01, 'momentum': 0.5} \n",
            "\n",
            "Current hyper-parameters: {'conv_channels': 64, 'dropout_rate': 0.3, 'lr': 0.001, 'momentum': 0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-683adbe3301c>:107: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  self.df = pd.concat([self.df, row], axis=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 00] train-loss=0.7633 | test-loss=0.6884 | train-acc=0.4968 | test-acc=0.5100 | P=0.5852 | R=0.1981 | F1=0.2960\n",
            "[epoch 01] train-loss=0.7195 | test-loss=0.6728 | train-acc=0.5100 | test-acc=0.5670 | P=0.5589 | R=0.7942 | F1=0.6561\n",
            "[epoch 02] train-loss=0.6985 | test-loss=0.6517 | train-acc=0.5375 | test-acc=0.7220 | P=0.7597 | R=0.6808 | F1=0.7181\n",
            "[epoch 03] train-loss=0.6628 | test-loss=0.5927 | train-acc=0.6132 | test-acc=0.9160 | P=0.8759 | R=0.9769 | F1=0.9236\n",
            "[epoch 04] train-loss=0.5928 | test-loss=0.4925 | train-acc=0.7245 | test-acc=0.9870 | P=0.9756 | R=1.0000 | F1=0.9877\n",
            "[epoch 05] train-loss=0.4836 | test-loss=0.3565 | train-acc=0.8190 | test-acc=0.9990 | P=0.9981 | R=1.0000 | F1=0.9990\n",
            "[epoch 06] train-loss=0.3605 | test-loss=0.2285 | train-acc=0.8922 | test-acc=0.9990 | P=0.9981 | R=1.0000 | F1=0.9990\n",
            "[epoch 07] train-loss=0.2747 | test-loss=0.1483 | train-acc=0.9290 | test-acc=0.9970 | P=0.9943 | R=1.0000 | F1=0.9971\n",
            "[epoch 08] train-loss=0.2093 | test-loss=0.0953 | train-acc=0.9453 | test-acc=0.9980 | P=0.9962 | R=1.0000 | F1=0.9981\n",
            "[epoch 09] train-loss=0.1679 | test-loss=0.0756 | train-acc=0.9557 | test-acc=0.9970 | P=0.9943 | R=1.0000 | F1=0.9971\n",
            "Current hyper-parameters: {'conv_channels': 64, 'dropout_rate': 0.3, 'lr': 0.001, 'momentum': 0.5}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-683adbe3301c>:107: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  self.df = pd.concat([self.df, row], axis=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 00] train-loss=0.7220 | test-loss=0.6688 | train-acc=0.5072 | test-acc=0.5380 | P=0.5298 | R=0.9904 | F1=0.6903\n",
            "[epoch 01] train-loss=0.6366 | test-loss=0.4940 | train-acc=0.6378 | test-acc=0.9990 | P=0.9981 | R=1.0000 | F1=0.9990\n",
            "[epoch 02] train-loss=0.4142 | test-loss=0.2130 | train-acc=0.8690 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 03] train-loss=0.2519 | test-loss=0.0994 | train-acc=0.9377 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 04] train-loss=0.1649 | test-loss=0.0591 | train-acc=0.9583 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 05] train-loss=0.1345 | test-loss=0.0432 | train-acc=0.9627 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 06] train-loss=0.1070 | test-loss=0.0335 | train-acc=0.9710 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 07] train-loss=0.0968 | test-loss=0.0292 | train-acc=0.9745 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 08] train-loss=0.0782 | test-loss=0.0241 | train-acc=0.9805 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 09] train-loss=0.0666 | test-loss=0.0196 | train-acc=0.9845 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "New best accuracy 1.0000 with {'conv_channels': 64, 'dropout_rate': 0.3, 'lr': 0.001, 'momentum': 0.5} \n",
            "\n",
            "Current hyper-parameters: {'conv_channels': 128, 'dropout_rate': 0, 'lr': 0.01, 'momentum': 0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-683adbe3301c>:107: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  self.df = pd.concat([self.df, row], axis=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 00] train-loss=0.2003 | test-loss=0.0157 | train-acc=0.9205 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 01] train-loss=0.0089 | test-loss=0.0054 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 02] train-loss=0.0040 | test-loss=0.0031 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 03] train-loss=0.0025 | test-loss=0.0021 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 04] train-loss=0.0018 | test-loss=0.0016 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 05] train-loss=0.0014 | test-loss=0.0013 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 06] train-loss=0.0012 | test-loss=0.0011 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 07] train-loss=0.0010 | test-loss=0.0009 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 08] train-loss=0.0008 | test-loss=0.0008 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 09] train-loss=0.0007 | test-loss=0.0007 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "New best accuracy 1.0000 with {'conv_channels': 128, 'dropout_rate': 0, 'lr': 0.01, 'momentum': 0} \n",
            "\n",
            "Current hyper-parameters: {'conv_channels': 128, 'dropout_rate': 0, 'lr': 0.01, 'momentum': 0.5}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-683adbe3301c>:107: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  self.df = pd.concat([self.df, row], axis=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 00] train-loss=0.1454 | test-loss=0.0066 | train-acc=0.9337 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 01] train-loss=0.0038 | test-loss=0.0027 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 02] train-loss=0.0019 | test-loss=0.0016 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 03] train-loss=0.0012 | test-loss=0.0012 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 04] train-loss=0.0009 | test-loss=0.0009 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 05] train-loss=0.0007 | test-loss=0.0008 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 06] train-loss=0.0006 | test-loss=0.0006 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 07] train-loss=0.0005 | test-loss=0.0006 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 08] train-loss=0.0004 | test-loss=0.0005 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 09] train-loss=0.0004 | test-loss=0.0004 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "New best accuracy 1.0000 with {'conv_channels': 128, 'dropout_rate': 0, 'lr': 0.01, 'momentum': 0.5} \n",
            "\n",
            "Current hyper-parameters: {'conv_channels': 128, 'dropout_rate': 0, 'lr': 0.001, 'momentum': 0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-683adbe3301c>:107: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  self.df = pd.concat([self.df, row], axis=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 00] train-loss=0.6912 | test-loss=0.6771 | train-acc=0.5310 | test-acc=0.5470 | P=0.5356 | R=0.9692 | F1=0.6899\n",
            "[epoch 01] train-loss=0.6635 | test-loss=0.6234 | train-acc=0.6522 | test-acc=0.8210 | P=0.7549 | R=0.9712 | F1=0.8495\n",
            "[epoch 02] train-loss=0.5465 | test-loss=0.4256 | train-acc=0.9350 | test-acc=0.9950 | P=0.9905 | R=1.0000 | F1=0.9952\n",
            "[epoch 03] train-loss=0.3028 | test-loss=0.1943 | train-acc=0.9998 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 04] train-loss=0.1364 | test-loss=0.0934 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 05] train-loss=0.0717 | test-loss=0.0548 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 06] train-loss=0.0448 | test-loss=0.0367 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 07] train-loss=0.0313 | test-loss=0.0270 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 08] train-loss=0.0236 | test-loss=0.0211 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 09] train-loss=0.0187 | test-loss=0.0171 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "New best accuracy 1.0000 with {'conv_channels': 128, 'dropout_rate': 0, 'lr': 0.001, 'momentum': 0} \n",
            "\n",
            "Current hyper-parameters: {'conv_channels': 128, 'dropout_rate': 0, 'lr': 0.001, 'momentum': 0.5}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-683adbe3301c>:107: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  self.df = pd.concat([self.df, row], axis=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 00] train-loss=0.6947 | test-loss=0.6558 | train-acc=0.5195 | test-acc=0.5950 | P=0.5626 | R=0.9942 | F1=0.7186\n",
            "[epoch 01] train-loss=0.5590 | test-loss=0.3783 | train-acc=0.8702 | test-acc=0.9990 | P=0.9981 | R=1.0000 | F1=0.9990\n",
            "[epoch 02] train-loss=0.2137 | test-loss=0.1046 | train-acc=0.9998 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 03] train-loss=0.0662 | test-loss=0.0426 | train-acc=0.9998 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 04] train-loss=0.0316 | test-loss=0.0242 | train-acc=0.9998 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 05] train-loss=0.0195 | test-loss=0.0163 | train-acc=0.9998 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 06] train-loss=0.0137 | test-loss=0.0120 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 07] train-loss=0.0104 | test-loss=0.0094 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 08] train-loss=0.0083 | test-loss=0.0077 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 09] train-loss=0.0069 | test-loss=0.0065 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "New best accuracy 1.0000 with {'conv_channels': 128, 'dropout_rate': 0, 'lr': 0.001, 'momentum': 0.5} \n",
            "\n",
            "Current hyper-parameters: {'conv_channels': 128, 'dropout_rate': 0.3, 'lr': 0.01, 'momentum': 0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-683adbe3301c>:107: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  self.df = pd.concat([self.df, row], axis=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 00] train-loss=0.3869 | test-loss=0.0343 | train-acc=0.8073 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 01] train-loss=0.0526 | test-loss=0.0163 | train-acc=0.9875 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 02] train-loss=0.0263 | test-loss=0.0068 | train-acc=0.9940 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 03] train-loss=0.0151 | test-loss=0.0045 | train-acc=0.9960 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 04] train-loss=0.0124 | test-loss=0.0031 | train-acc=0.9968 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 05] train-loss=0.0105 | test-loss=0.0026 | train-acc=0.9972 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 06] train-loss=0.0105 | test-loss=0.0025 | train-acc=0.9978 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 07] train-loss=0.0052 | test-loss=0.0015 | train-acc=0.9992 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 08] train-loss=0.0059 | test-loss=0.0015 | train-acc=0.9982 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 09] train-loss=0.0056 | test-loss=0.0010 | train-acc=0.9988 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "New best accuracy 1.0000 with {'conv_channels': 128, 'dropout_rate': 0.3, 'lr': 0.01, 'momentum': 0} \n",
            "\n",
            "Current hyper-parameters: {'conv_channels': 128, 'dropout_rate': 0.3, 'lr': 0.01, 'momentum': 0.5}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-683adbe3301c>:107: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  self.df = pd.concat([self.df, row], axis=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 00] train-loss=0.3032 | test-loss=0.0183 | train-acc=0.8482 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 01] train-loss=0.0233 | test-loss=0.0034 | train-acc=0.9938 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 02] train-loss=0.0147 | test-loss=0.0027 | train-acc=0.9970 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 03] train-loss=0.0083 | test-loss=0.0017 | train-acc=0.9972 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 04] train-loss=0.0050 | test-loss=0.0021 | train-acc=0.9988 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 05] train-loss=0.0054 | test-loss=0.0026 | train-acc=0.9985 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 06] train-loss=0.0051 | test-loss=0.0019 | train-acc=0.9988 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 07] train-loss=0.0027 | test-loss=0.0010 | train-acc=0.9995 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 08] train-loss=0.0031 | test-loss=0.0007 | train-acc=0.9995 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 09] train-loss=0.0019 | test-loss=0.0018 | train-acc=0.9998 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "New best accuracy 1.0000 with {'conv_channels': 128, 'dropout_rate': 0.3, 'lr': 0.01, 'momentum': 0.5} \n",
            "\n",
            "Current hyper-parameters: {'conv_channels': 128, 'dropout_rate': 0.3, 'lr': 0.001, 'momentum': 0}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-683adbe3301c>:107: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  self.df = pd.concat([self.df, row], axis=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 00] train-loss=0.7327 | test-loss=0.6814 | train-acc=0.5052 | test-acc=0.5590 | P=0.6000 | R=0.4558 | F1=0.5180\n",
            "[epoch 01] train-loss=0.6989 | test-loss=0.6509 | train-acc=0.5407 | test-acc=0.7810 | P=0.7980 | R=0.7750 | F1=0.7863\n",
            "[epoch 02] train-loss=0.6485 | test-loss=0.5735 | train-acc=0.6282 | test-acc=0.9860 | P=1.0000 | R=0.9731 | F1=0.9864\n",
            "[epoch 03] train-loss=0.5637 | test-loss=0.4580 | train-acc=0.7628 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 04] train-loss=0.4722 | test-loss=0.3399 | train-acc=0.8355 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 05] train-loss=0.4071 | test-loss=0.2624 | train-acc=0.8462 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 06] train-loss=0.3606 | test-loss=0.2070 | train-acc=0.8512 | test-acc=0.9990 | P=0.9981 | R=1.0000 | F1=0.9990\n",
            "[epoch 07] train-loss=0.3143 | test-loss=0.1577 | train-acc=0.8690 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 08] train-loss=0.2755 | test-loss=0.1186 | train-acc=0.9050 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 09] train-loss=0.2393 | test-loss=0.0969 | train-acc=0.9335 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "New best accuracy 1.0000 with {'conv_channels': 128, 'dropout_rate': 0.3, 'lr': 0.001, 'momentum': 0} \n",
            "\n",
            "Current hyper-parameters: {'conv_channels': 128, 'dropout_rate': 0.3, 'lr': 0.001, 'momentum': 0.5}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-683adbe3301c>:107: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  self.df = pd.concat([self.df, row], axis=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 00] train-loss=0.7109 | test-loss=0.6522 | train-acc=0.5220 | test-acc=0.7120 | P=0.9056 | R=0.4981 | F1=0.6427\n",
            "[epoch 01] train-loss=0.5907 | test-loss=0.4010 | train-acc=0.6910 | test-acc=0.9990 | P=0.9981 | R=1.0000 | F1=0.9990\n",
            "[epoch 02] train-loss=0.3565 | test-loss=0.1776 | train-acc=0.8970 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 03] train-loss=0.2197 | test-loss=0.0876 | train-acc=0.9450 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 04] train-loss=0.1541 | test-loss=0.0525 | train-acc=0.9603 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 05] train-loss=0.1123 | test-loss=0.0364 | train-acc=0.9680 | test-acc=0.9990 | P=0.9981 | R=1.0000 | F1=0.9990\n",
            "[epoch 06] train-loss=0.0853 | test-loss=0.0259 | train-acc=0.9780 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 07] train-loss=0.0723 | test-loss=0.0249 | train-acc=0.9772 | test-acc=0.9990 | P=0.9981 | R=1.0000 | F1=0.9990\n",
            "[epoch 08] train-loss=0.0531 | test-loss=0.0208 | train-acc=0.9868 | test-acc=0.9980 | P=0.9962 | R=1.0000 | F1=0.9981\n",
            "[epoch 09] train-loss=0.0448 | test-loss=0.0174 | train-acc=0.9882 | test-acc=0.9980 | P=0.9962 | R=1.0000 | F1=0.9981\n",
            "New best accuracy 1.0000 with {'conv_channels': 128, 'dropout_rate': 0.3, 'lr': 0.001, 'momentum': 0.5} \n",
            "\n",
            "Best hyperâ€‘parameters found: {'conv_channels': 128, 'dropout_rate': 0.3, 'lr': 0.001, 'momentum': 0.5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training on reduced datasets"
      ],
      "metadata": {
        "id": "8ZdNGslz0cAU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Half of positives"
      ],
      "metadata": {
        "id": "301fqHMW1mGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datalist, labellist = read(\"len200_500_n5000nr1.seq\", \"len200_500_n5000nr1.pos\")\n",
        "pos_datalist, neg_datalist = split_labelled(datalist, labellist)\n",
        "pos_datalist = remove_sequences(pos_datalist, 0.5)\n",
        "datalist, labellist = fuse_sequence_lists(pos_datalist, neg_datalist)\n",
        "\n",
        "batch_size = 10\n",
        "num_steps = 1000 # max sequence length\n",
        "\n",
        "traindatalist, trainlabellist, testdatalist, testlabellist = generate_train_test(datalist, labellist, 0.8)\n",
        "traindataset = [traindatalist, trainlabellist]\n",
        "testdataset = [testdatalist, testlabellist]\n",
        "\n",
        "# Set batch_size and num_steps (maximum sequence length)\n",
        "train_iter = load_data(batch_size, num_steps, traindataset)\n",
        "test_iter = load_data(batch_size, num_steps, testdataset)\n",
        "\n",
        "\n",
        "model = BerryCNN1D(vocab_size=21,dropout_rate=0,conv_channels=64)\n",
        "model.apply(init_weights)\n",
        "optimizer = torch.optim.SGD(\n",
        "    model.parameters(),\n",
        "    lr=0.01,\n",
        "    momentum=0.5\n",
        ")\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "trainer = Trainer(model, loss_fn, optimizer, device)\n",
        "df = trainer.train(epochs=10, train_iter=train_iter, test_iter=test_iter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n58u1CAg0rbb",
        "outputId": "af011a41-be1e-4133-8498-84c10298b858"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-1cb05001b72f>:128: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  self.df = pd.concat([self.df, row], axis=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 00] train-loss=0.2455 | test-loss=0.0126 | train-acc=0.8971 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 01] train-loss=0.0077 | test-loss=0.0040 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 02] train-loss=0.0034 | test-loss=0.0021 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 03] train-loss=0.0021 | test-loss=0.0014 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 04] train-loss=0.0016 | test-loss=0.0011 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 05] train-loss=0.0012 | test-loss=0.0009 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 06] train-loss=0.0010 | test-loss=0.0007 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 07] train-loss=0.0008 | test-loss=0.0006 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 08] train-loss=0.0007 | test-loss=0.0005 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 09] train-loss=0.0006 | test-loss=0.0005 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('half_pos_len200_500_n5000nr1.csv')"
      ],
      "metadata": {
        "id": "SjVt5LX-1S80"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Half of negatives"
      ],
      "metadata": {
        "id": "qaxou8V21paQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datalist, labellist = read(\"len200_500_n5000nr1.seq\", \"len200_500_n5000nr1.pos\")\n",
        "pos_datalist, neg_datalist = split_labelled(datalist, labellist)\n",
        "neg_datalist = remove_sequences(neg_datalist, 0.5)\n",
        "datalist, labellist = fuse_sequence_lists(pos_datalist, neg_datalist)\n",
        "\n",
        "batch_size = 10\n",
        "num_steps = 1000 # max sequence length\n",
        "\n",
        "traindatalist, trainlabellist, testdatalist, testlabellist = generate_train_test(datalist, labellist, 0.8)\n",
        "traindataset = [traindatalist, trainlabellist]\n",
        "testdataset = [testdatalist, testlabellist]\n",
        "\n",
        "# Set batch_size and num_steps (maximum sequence length)\n",
        "train_iter = load_data(batch_size, num_steps, traindataset)\n",
        "test_iter = load_data(batch_size, num_steps, testdataset)\n",
        "\n",
        "print(\"batch shape  :\", next(iter(train_iter))[0].shape)\n",
        "print(\"labels shape :\", next(iter(train_iter))[1].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfBF1M-W1h9I",
        "outputId": "315707a3-2da9-4416-ae91-92e6248f6408"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch shape  : torch.Size([10, 1000])\n",
            "labels shape : torch.Size([10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = BerryCNN1D(vocab_size=21,dropout_rate=0,conv_channels=64)\n",
        "model.apply(init_weights)\n",
        "optimizer = torch.optim.SGD(\n",
        "    model.parameters(),\n",
        "    lr=0.01,\n",
        "    momentum=0.5\n",
        ")\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "trainer = Trainer(model, loss_fn, optimizer, device)\n",
        "df = trainer.train(epochs=10, train_iter=train_iter, test_iter=test_iter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KcLovPuh1r3s",
        "outputId": "d3a9da20-f064-499e-8011-86da3d7a4f66"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-32-1cb05001b72f>:128: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  self.df = pd.concat([self.df, row], axis=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 00] train-loss=0.5165 | test-loss=0.0688 | train-acc=0.7465 | test-acc=0.9960 | P=0.9941 | R=1.0000 | F1=0.9970\n",
            "[epoch 01] train-loss=0.0241 | test-loss=0.0087 | train-acc=0.9983 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 02] train-loss=0.0067 | test-loss=0.0043 | train-acc=0.9993 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 03] train-loss=0.0038 | test-loss=0.0029 | train-acc=0.9997 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 04] train-loss=0.0025 | test-loss=0.0022 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 05] train-loss=0.0019 | test-loss=0.0018 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 06] train-loss=0.0015 | test-loss=0.0016 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 07] train-loss=0.0012 | test-loss=0.0014 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 08] train-loss=0.0010 | test-loss=0.0012 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n",
            "[epoch 09] train-loss=0.0009 | test-loss=0.0011 | train-acc=1.0000 | test-acc=1.0000 | P=1.0000 | R=1.0000 | F1=1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('half_neg_len200_500_n5000nr1.csv')"
      ],
      "metadata": {
        "id": "B2dtThLk1t0q"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training on GO annotations"
      ],
      "metadata": {
        "id": "L6-AIB-L0ToX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Full set"
      ],
      "metadata": {
        "id": "1MI_br9w7v9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datalist, labellist = read(\"expr5Tseq_filtGO_100-1000.lis\", \"GO_3A0005576.annotprot\")\n",
        "\n",
        "batch_size = 10\n",
        "num_steps = 1000 # max sequence length\n",
        "\n",
        "traindatalist, trainlabellist, testdatalist, testlabellist = generate_train_test(datalist, labellist, 0.8)\n",
        "traindataset = [traindatalist, trainlabellist]\n",
        "testdataset = [testdatalist, testlabellist]\n",
        "\n",
        "# Set batch_size and num_steps (maximum sequence length)\n",
        "train_iter = load_data(batch_size, num_steps, traindataset)\n",
        "test_iter = load_data(batch_size, num_steps, testdataset)\n",
        "\n",
        "model = BerryCNN1D(vocab_size=21,dropout_rate=0.2,conv_channels=128)\n",
        "model.apply(init_weights)\n",
        "optimizer = torch.optim.SGD(\n",
        "    model.parameters(),\n",
        "    lr=0.01,\n",
        "    momentum=0.5\n",
        ")\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "trainer = Trainer(model, loss_fn, optimizer, device)\n",
        "df = trainer.train(epochs=50, train_iter=train_iter, test_iter=test_iter)\n",
        "\n",
        "df.to_csv(\"GO_3A0005576_full_set.csv\")\n"
      ],
      "metadata": {
        "id": "yDEFWAwbK_s3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3d299a2-8393-493d-cd7f-9a170ae988fd"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-1b0abac1b1a5>:108: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  self.df = pd.concat([self.df, row], axis=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 00] train-loss=0.1911 | test-loss=0.1870 | train-acc=0.9536 | test-acc=0.9528 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 01] train-loss=0.1806 | test-loss=0.1850 | train-acc=0.9552 | test-acc=0.9528 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 02] train-loss=0.1770 | test-loss=0.1879 | train-acc=0.9552 | test-acc=0.9528 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 03] train-loss=0.1747 | test-loss=0.1819 | train-acc=0.9552 | test-acc=0.9528 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 04] train-loss=0.1687 | test-loss=0.1829 | train-acc=0.9552 | test-acc=0.9528 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 05] train-loss=0.1582 | test-loss=0.1812 | train-acc=0.9552 | test-acc=0.9528 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 06] train-loss=0.1497 | test-loss=0.1750 | train-acc=0.9554 | test-acc=0.9528 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 07] train-loss=0.1390 | test-loss=0.1764 | train-acc=0.9569 | test-acc=0.9528 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 08] train-loss=0.1271 | test-loss=0.1671 | train-acc=0.9584 | test-acc=0.9528 | P=0.5000 | R=0.0156 | F1=0.0303\n",
            "[epoch 09] train-loss=0.1137 | test-loss=0.1725 | train-acc=0.9611 | test-acc=0.9543 | P=0.6250 | R=0.0781 | F1=0.1389\n",
            "[epoch 10] train-loss=0.1032 | test-loss=0.1866 | train-acc=0.9646 | test-acc=0.9528 | P=0.5000 | R=0.0156 | F1=0.0303\n",
            "[epoch 11] train-loss=0.0880 | test-loss=0.1878 | train-acc=0.9672 | test-acc=0.9565 | P=0.6087 | R=0.2188 | F1=0.3218\n",
            "[epoch 12] train-loss=0.0790 | test-loss=0.2131 | train-acc=0.9727 | test-acc=0.9528 | P=0.5000 | R=0.0156 | F1=0.0303\n",
            "[epoch 13] train-loss=0.0686 | test-loss=0.1638 | train-acc=0.9749 | test-acc=0.9536 | P=0.5556 | R=0.0781 | F1=0.1370\n",
            "[epoch 14] train-loss=0.0596 | test-loss=0.2037 | train-acc=0.9807 | test-acc=0.9536 | P=0.6667 | R=0.0312 | F1=0.0597\n",
            "[epoch 15] train-loss=0.0445 | test-loss=0.2344 | train-acc=0.9849 | test-acc=0.9528 | P=0.5000 | R=0.0156 | F1=0.0303\n",
            "[epoch 16] train-loss=0.0425 | test-loss=0.1733 | train-acc=0.9856 | test-acc=0.9536 | P=0.5333 | R=0.1250 | F1=0.2025\n",
            "[epoch 17] train-loss=0.0354 | test-loss=0.1775 | train-acc=0.9891 | test-acc=0.9521 | P=0.4545 | R=0.0781 | F1=0.1333\n",
            "[epoch 18] train-loss=0.0322 | test-loss=0.2177 | train-acc=0.9897 | test-acc=0.9543 | P=0.7500 | R=0.0469 | F1=0.0882\n",
            "[epoch 19] train-loss=0.0229 | test-loss=0.1852 | train-acc=0.9934 | test-acc=0.9521 | P=0.4667 | R=0.1094 | F1=0.1772\n",
            "[epoch 20] train-loss=0.0208 | test-loss=0.2130 | train-acc=0.9958 | test-acc=0.9550 | P=0.8000 | R=0.0625 | F1=0.1159\n",
            "[epoch 21] train-loss=0.0178 | test-loss=0.1901 | train-acc=0.9956 | test-acc=0.9425 | P=0.3478 | R=0.2500 | F1=0.2909\n",
            "[epoch 22] train-loss=0.0177 | test-loss=0.2282 | train-acc=0.9961 | test-acc=0.9543 | P=0.6667 | R=0.0625 | F1=0.1143\n",
            "[epoch 23] train-loss=0.0136 | test-loss=0.2024 | train-acc=0.9969 | test-acc=0.9543 | P=0.6250 | R=0.0781 | F1=0.1389\n",
            "[epoch 24] train-loss=0.0133 | test-loss=0.2225 | train-acc=0.9978 | test-acc=0.9536 | P=0.5714 | R=0.0625 | F1=0.1127\n",
            "[epoch 25] train-loss=0.0105 | test-loss=0.2264 | train-acc=0.9983 | test-acc=0.9543 | P=0.6667 | R=0.0625 | F1=0.1143\n",
            "[epoch 26] train-loss=0.0114 | test-loss=0.2298 | train-acc=0.9978 | test-acc=0.9536 | P=0.5714 | R=0.0625 | F1=0.1127\n",
            "[epoch 27] train-loss=0.0104 | test-loss=0.2645 | train-acc=0.9987 | test-acc=0.9543 | P=0.7500 | R=0.0469 | F1=0.0882\n",
            "[epoch 28] train-loss=0.0100 | test-loss=0.2403 | train-acc=0.9980 | test-acc=0.9536 | P=0.5714 | R=0.0625 | F1=0.1127\n",
            "[epoch 29] train-loss=0.0105 | test-loss=0.2674 | train-acc=0.9976 | test-acc=0.9543 | P=0.6667 | R=0.0625 | F1=0.1143\n",
            "[epoch 30] train-loss=0.0057 | test-loss=0.2812 | train-acc=0.9994 | test-acc=0.9550 | P=0.8000 | R=0.0625 | F1=0.1159\n",
            "[epoch 31] train-loss=0.0050 | test-loss=0.2695 | train-acc=0.9998 | test-acc=0.9543 | P=0.6667 | R=0.0625 | F1=0.1143\n",
            "[epoch 32] train-loss=0.0050 | test-loss=0.2366 | train-acc=0.9994 | test-acc=0.9528 | P=0.5000 | R=0.0938 | F1=0.1579\n",
            "[epoch 33] train-loss=0.0053 | test-loss=0.2573 | train-acc=0.9993 | test-acc=0.9558 | P=0.7500 | R=0.0938 | F1=0.1667\n",
            "[epoch 34] train-loss=0.0049 | test-loss=0.2940 | train-acc=0.9993 | test-acc=0.9543 | P=0.7500 | R=0.0469 | F1=0.0882\n",
            "[epoch 35] train-loss=0.0052 | test-loss=0.2770 | train-acc=0.9993 | test-acc=0.9543 | P=0.6667 | R=0.0625 | F1=0.1143\n",
            "[epoch 36] train-loss=0.0044 | test-loss=0.2946 | train-acc=0.9996 | test-acc=0.9543 | P=0.6667 | R=0.0625 | F1=0.1143\n",
            "[epoch 37] train-loss=0.0059 | test-loss=0.2966 | train-acc=0.9993 | test-acc=0.9543 | P=0.6667 | R=0.0625 | F1=0.1143\n",
            "[epoch 38] train-loss=0.0056 | test-loss=0.2778 | train-acc=0.9987 | test-acc=0.9543 | P=0.6667 | R=0.0625 | F1=0.1143\n",
            "[epoch 39] train-loss=0.0061 | test-loss=0.2830 | train-acc=0.9991 | test-acc=0.9543 | P=0.7500 | R=0.0469 | F1=0.0882\n",
            "[epoch 40] train-loss=0.0040 | test-loss=0.3175 | train-acc=0.9996 | test-acc=0.9536 | P=0.6000 | R=0.0469 | F1=0.0870\n",
            "[epoch 41] train-loss=0.0044 | test-loss=0.3014 | train-acc=0.9993 | test-acc=0.9543 | P=0.6667 | R=0.0625 | F1=0.1143\n",
            "[epoch 42] train-loss=0.0033 | test-loss=0.2958 | train-acc=0.9996 | test-acc=0.9543 | P=0.6667 | R=0.0625 | F1=0.1143\n",
            "[epoch 43] train-loss=0.0066 | test-loss=0.2775 | train-acc=0.9980 | test-acc=0.9550 | P=0.7143 | R=0.0781 | F1=0.1408\n",
            "[epoch 44] train-loss=0.0048 | test-loss=0.4802 | train-acc=0.9991 | test-acc=0.9536 | P=0.6667 | R=0.0312 | F1=0.0597\n",
            "[epoch 45] train-loss=0.0036 | test-loss=0.3312 | train-acc=0.9994 | test-acc=0.9543 | P=0.7500 | R=0.0469 | F1=0.0882\n",
            "[epoch 46] train-loss=0.0028 | test-loss=0.2624 | train-acc=0.9998 | test-acc=0.9543 | P=0.6000 | R=0.0938 | F1=0.1622\n",
            "[epoch 47] train-loss=0.0037 | test-loss=0.3206 | train-acc=0.9991 | test-acc=0.9543 | P=0.6667 | R=0.0625 | F1=0.1143\n",
            "[epoch 48] train-loss=0.0034 | test-loss=0.2755 | train-acc=0.9996 | test-acc=0.9536 | P=0.5556 | R=0.0781 | F1=0.1370\n",
            "[epoch 49] train-loss=0.0021 | test-loss=0.3178 | train-acc=0.9998 | test-acc=0.9550 | P=0.8000 | R=0.0625 | F1=0.1159\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Half positives"
      ],
      "metadata": {
        "id": "0keR8voU7yTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datalist, labellist = read(\"expr5Tseq_filtGO_100-1000.lis\", \"GO_3A0005576.annotprot\")\n",
        "pos_datalist, neg_datalist = split_labelled(datalist, labellist)\n",
        "pos_datalist = remove_sequences(pos_datalist, 0.5)\n",
        "datalist, labellist = fuse_sequence_lists(pos_datalist, neg_datalist)\n",
        "\n",
        "batch_size = 10\n",
        "num_steps = 1000 # max sequence length\n",
        "\n",
        "traindatalist, trainlabellist, testdatalist, testlabellist = generate_train_test(datalist, labellist, 0.8)\n",
        "traindataset = [traindatalist, trainlabellist]\n",
        "testdataset = [testdatalist, testlabellist]\n",
        "\n",
        "# Set batch_size and num_steps (maximum sequence length)\n",
        "train_iter = load_data(batch_size, num_steps, traindataset)\n",
        "test_iter = load_data(batch_size, num_steps, testdataset)\n",
        "\n",
        "model = BerryCNN1D(vocab_size=21,dropout_rate=0.2,conv_channels=128)\n",
        "model.apply(init_weights)\n",
        "optimizer = torch.optim.SGD(\n",
        "    model.parameters(),\n",
        "    lr=0.01,\n",
        "    momentum=0.5\n",
        ")\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "trainer = Trainer(model, loss_fn, optimizer, device)\n",
        "df = trainer.train(epochs=50, train_iter=train_iter, test_iter=test_iter)\n",
        "\n",
        "df.to_csv(\"GO_3A0005576_half_pos.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ITkHkZ67k0N",
        "outputId": "7e63b7d1-877f-4181-cbe0-f0bdf0e9c425"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-1b0abac1b1a5>:108: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  self.df = pd.concat([self.df, row], axis=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 00] train-loss=0.1183 | test-loss=0.1066 | train-acc=0.9746 | test-acc=0.9781 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 01] train-loss=0.1124 | test-loss=0.1091 | train-acc=0.9764 | test-acc=0.9781 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 02] train-loss=0.1104 | test-loss=0.1083 | train-acc=0.9764 | test-acc=0.9781 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 03] train-loss=0.1086 | test-loss=0.1074 | train-acc=0.9764 | test-acc=0.9781 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 04] train-loss=0.1065 | test-loss=0.1018 | train-acc=0.9764 | test-acc=0.9781 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 05] train-loss=0.1013 | test-loss=0.1037 | train-acc=0.9764 | test-acc=0.9781 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 06] train-loss=0.0985 | test-loss=0.1006 | train-acc=0.9764 | test-acc=0.9781 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 07] train-loss=0.0928 | test-loss=0.0990 | train-acc=0.9764 | test-acc=0.9781 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 08] train-loss=0.0905 | test-loss=0.1088 | train-acc=0.9766 | test-acc=0.9781 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 09] train-loss=0.0841 | test-loss=0.0984 | train-acc=0.9766 | test-acc=0.9781 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 10] train-loss=0.0798 | test-loss=0.0964 | train-acc=0.9768 | test-acc=0.9781 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 11] train-loss=0.0707 | test-loss=0.0981 | train-acc=0.9776 | test-acc=0.9781 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 12] train-loss=0.0690 | test-loss=0.1009 | train-acc=0.9783 | test-acc=0.9774 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 13] train-loss=0.0573 | test-loss=0.0964 | train-acc=0.9808 | test-acc=0.9781 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 14] train-loss=0.0535 | test-loss=0.0966 | train-acc=0.9815 | test-acc=0.9781 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 15] train-loss=0.0458 | test-loss=0.1175 | train-acc=0.9845 | test-acc=0.9781 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 16] train-loss=0.0374 | test-loss=0.1055 | train-acc=0.9868 | test-acc=0.9781 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 17] train-loss=0.0315 | test-loss=0.1021 | train-acc=0.9900 | test-acc=0.9774 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 18] train-loss=0.0302 | test-loss=0.1431 | train-acc=0.9902 | test-acc=0.9781 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 19] train-loss=0.0234 | test-loss=0.1006 | train-acc=0.9928 | test-acc=0.9774 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 20] train-loss=0.0194 | test-loss=0.1054 | train-acc=0.9942 | test-acc=0.9774 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 21] train-loss=0.0157 | test-loss=0.1465 | train-acc=0.9959 | test-acc=0.9781 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 22] train-loss=0.0138 | test-loss=0.1205 | train-acc=0.9959 | test-acc=0.9781 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 23] train-loss=0.0122 | test-loss=0.1079 | train-acc=0.9972 | test-acc=0.9774 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 24] train-loss=0.0112 | test-loss=0.1504 | train-acc=0.9981 | test-acc=0.9781 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 25] train-loss=0.0094 | test-loss=0.1285 | train-acc=0.9983 | test-acc=0.9781 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 26] train-loss=0.0088 | test-loss=0.1126 | train-acc=0.9981 | test-acc=0.9766 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 27] train-loss=0.0066 | test-loss=0.1507 | train-acc=0.9992 | test-acc=0.9781 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 28] train-loss=0.0072 | test-loss=0.1198 | train-acc=0.9981 | test-acc=0.9774 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 29] train-loss=0.0065 | test-loss=0.1244 | train-acc=0.9992 | test-acc=0.9781 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 30] train-loss=0.0044 | test-loss=0.1359 | train-acc=0.9996 | test-acc=0.9781 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 31] train-loss=0.0044 | test-loss=0.1209 | train-acc=0.9994 | test-acc=0.9774 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 32] train-loss=0.0033 | test-loss=0.1336 | train-acc=1.0000 | test-acc=0.9774 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 33] train-loss=0.0042 | test-loss=0.1461 | train-acc=0.9996 | test-acc=0.9781 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 34] train-loss=0.0032 | test-loss=0.1669 | train-acc=0.9998 | test-acc=0.9781 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 35] train-loss=0.0032 | test-loss=0.1522 | train-acc=1.0000 | test-acc=0.9781 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 36] train-loss=0.0023 | test-loss=0.1718 | train-acc=1.0000 | test-acc=0.9781 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 37] train-loss=0.0042 | test-loss=0.1614 | train-acc=0.9992 | test-acc=0.9781 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 38] train-loss=0.0033 | test-loss=0.1440 | train-acc=0.9998 | test-acc=0.9781 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 39] train-loss=0.0022 | test-loss=0.1604 | train-acc=0.9998 | test-acc=0.9781 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 40] train-loss=0.0024 | test-loss=0.1477 | train-acc=0.9998 | test-acc=0.9774 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 41] train-loss=0.0022 | test-loss=0.1417 | train-acc=1.0000 | test-acc=0.9766 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 42] train-loss=0.0022 | test-loss=0.1296 | train-acc=0.9998 | test-acc=0.9766 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 43] train-loss=0.0027 | test-loss=0.1626 | train-acc=0.9996 | test-acc=0.9781 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 44] train-loss=0.0023 | test-loss=0.1648 | train-acc=0.9996 | test-acc=0.9781 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 45] train-loss=0.0020 | test-loss=0.1711 | train-acc=0.9998 | test-acc=0.9781 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 46] train-loss=0.0017 | test-loss=0.1573 | train-acc=0.9998 | test-acc=0.9781 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 47] train-loss=0.0016 | test-loss=0.1598 | train-acc=1.0000 | test-acc=0.9781 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 48] train-loss=0.0013 | test-loss=0.1624 | train-acc=1.0000 | test-acc=0.9781 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 49] train-loss=0.0018 | test-loss=0.1599 | train-acc=0.9998 | test-acc=0.9781 | P=0.0000 | R=0.0000 | F1=0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Half negatives"
      ],
      "metadata": {
        "id": "WBv-Uvbj72-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datalist, labellist = read(\"expr5Tseq_filtGO_100-1000.lis\", \"GO_3A0005576.annotprot\")\n",
        "pos_datalist, neg_datalist = split_labelled(datalist, labellist)\n",
        "neg_datalist = remove_sequences(neg_datalist, 0.5)\n",
        "datalist, labellist = fuse_sequence_lists(pos_datalist, neg_datalist)\n",
        "\n",
        "batch_size = 10\n",
        "num_steps = 1000 # max sequence length\n",
        "\n",
        "traindatalist, trainlabellist, testdatalist, testlabellist = generate_train_test(datalist, labellist, 0.8)\n",
        "traindataset = [traindatalist, trainlabellist]\n",
        "testdataset = [testdatalist, testlabellist]\n",
        "\n",
        "# Set batch_size and num_steps (maximum sequence length)\n",
        "train_iter = load_data(batch_size, num_steps, traindataset)\n",
        "test_iter = load_data(batch_size, num_steps, testdataset)\n",
        "\n",
        "model = BerryCNN1D(vocab_size=21,dropout_rate=0.2,conv_channels=128)\n",
        "model.apply(init_weights)\n",
        "optimizer = torch.optim.SGD(\n",
        "    model.parameters(),\n",
        "    lr=0.01,\n",
        "    momentum=0.5\n",
        ")\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "trainer = Trainer(model, loss_fn, optimizer, device)\n",
        "df = trainer.train(epochs=50, train_iter=train_iter, test_iter=test_iter)\n",
        "\n",
        "df.to_csv(\"GO_3A0005576_half_neg.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6ki3olN73Ut",
        "outputId": "bec6d4fc-f8f1-461a-da77-9096c6c6a0b5"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-1b0abac1b1a5>:108: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  self.df = pd.concat([self.df, row], axis=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 00] train-loss=0.3024 | test-loss=0.2880 | train-acc=0.9140 | test-acc=0.9111 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 01] train-loss=0.2884 | test-loss=0.2869 | train-acc=0.9140 | test-acc=0.9111 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 02] train-loss=0.2862 | test-loss=0.2843 | train-acc=0.9140 | test-acc=0.9111 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 03] train-loss=0.2692 | test-loss=0.2850 | train-acc=0.9140 | test-acc=0.9111 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 04] train-loss=0.2663 | test-loss=0.2747 | train-acc=0.9143 | test-acc=0.9111 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 05] train-loss=0.2487 | test-loss=0.3015 | train-acc=0.9140 | test-acc=0.9111 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 06] train-loss=0.2306 | test-loss=0.2782 | train-acc=0.9171 | test-acc=0.9111 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 07] train-loss=0.2155 | test-loss=0.2530 | train-acc=0.9224 | test-acc=0.9126 | P=1.0000 | R=0.0159 | F1=0.0312\n",
            "[epoch 08] train-loss=0.1931 | test-loss=0.2868 | train-acc=0.9305 | test-acc=0.9126 | P=1.0000 | R=0.0159 | F1=0.0312\n",
            "[epoch 09] train-loss=0.1840 | test-loss=0.2583 | train-acc=0.9341 | test-acc=0.9126 | P=1.0000 | R=0.0159 | F1=0.0312\n",
            "[epoch 10] train-loss=0.1605 | test-loss=0.2463 | train-acc=0.9425 | test-acc=0.9168 | P=0.8333 | R=0.0794 | F1=0.1449\n",
            "[epoch 11] train-loss=0.1464 | test-loss=0.2519 | train-acc=0.9422 | test-acc=0.9126 | P=0.5172 | R=0.2381 | F1=0.3261\n",
            "[epoch 12] train-loss=0.1318 | test-loss=0.2841 | train-acc=0.9520 | test-acc=0.9126 | P=1.0000 | R=0.0159 | F1=0.0312\n",
            "[epoch 13] train-loss=0.1097 | test-loss=0.2402 | train-acc=0.9637 | test-acc=0.9097 | P=0.4828 | R=0.2222 | F1=0.3043\n",
            "[epoch 14] train-loss=0.0916 | test-loss=0.2496 | train-acc=0.9700 | test-acc=0.9126 | P=0.5116 | R=0.3492 | F1=0.4151\n",
            "[epoch 15] train-loss=0.0811 | test-loss=0.2610 | train-acc=0.9718 | test-acc=0.9182 | P=0.7778 | R=0.1111 | F1=0.1944\n",
            "[epoch 16] train-loss=0.0692 | test-loss=0.2951 | train-acc=0.9785 | test-acc=0.9182 | P=1.0000 | R=0.0794 | F1=0.1471\n",
            "[epoch 17] train-loss=0.0598 | test-loss=0.3274 | train-acc=0.9810 | test-acc=0.9168 | P=1.0000 | R=0.0635 | F1=0.1194\n",
            "[epoch 18] train-loss=0.0407 | test-loss=0.2884 | train-acc=0.9926 | test-acc=0.9168 | P=0.6667 | R=0.1270 | F1=0.2133\n",
            "[epoch 19] train-loss=0.0418 | test-loss=0.2755 | train-acc=0.9894 | test-acc=0.9154 | P=0.6154 | R=0.1270 | F1=0.2105\n",
            "[epoch 20] train-loss=0.0337 | test-loss=0.2974 | train-acc=0.9944 | test-acc=0.9196 | P=0.8000 | R=0.1270 | F1=0.2192\n",
            "[epoch 21] train-loss=0.0301 | test-loss=0.3564 | train-acc=0.9954 | test-acc=0.9196 | P=0.8750 | R=0.1111 | F1=0.1972\n",
            "[epoch 22] train-loss=0.0257 | test-loss=0.4911 | train-acc=0.9965 | test-acc=0.9140 | P=1.0000 | R=0.0317 | F1=0.0615\n",
            "[epoch 23] train-loss=0.0239 | test-loss=0.2653 | train-acc=0.9968 | test-acc=0.9097 | P=0.4762 | R=0.1587 | F1=0.2381\n",
            "[epoch 24] train-loss=0.0205 | test-loss=0.3178 | train-acc=0.9968 | test-acc=0.9168 | P=0.7000 | R=0.1111 | F1=0.1918\n",
            "[epoch 25] train-loss=0.0176 | test-loss=0.2835 | train-acc=0.9975 | test-acc=0.9126 | P=0.5263 | R=0.1587 | F1=0.2439\n",
            "[epoch 26] train-loss=0.0156 | test-loss=0.2971 | train-acc=0.9968 | test-acc=0.9126 | P=0.5263 | R=0.1587 | F1=0.2439\n",
            "[epoch 27] train-loss=0.0174 | test-loss=0.3014 | train-acc=0.9972 | test-acc=0.9154 | P=0.6000 | R=0.1429 | F1=0.2308\n",
            "[epoch 28] train-loss=0.0142 | test-loss=0.3658 | train-acc=0.9979 | test-acc=0.9210 | P=1.0000 | R=0.1111 | F1=0.2000\n",
            "[epoch 29] train-loss=0.0115 | test-loss=0.3085 | train-acc=0.9993 | test-acc=0.9168 | P=0.6429 | R=0.1429 | F1=0.2338\n",
            "[epoch 30] train-loss=0.0089 | test-loss=0.2563 | train-acc=0.9993 | test-acc=0.9168 | P=0.5556 | R=0.3175 | F1=0.4040\n",
            "[epoch 31] train-loss=0.0101 | test-loss=0.3030 | train-acc=0.9993 | test-acc=0.9111 | P=0.5000 | R=0.1429 | F1=0.2222\n",
            "[epoch 32] train-loss=0.0085 | test-loss=0.3804 | train-acc=0.9993 | test-acc=0.9196 | P=0.8000 | R=0.1270 | F1=0.2192\n",
            "[epoch 33] train-loss=0.0070 | test-loss=0.3813 | train-acc=1.0000 | test-acc=0.9196 | P=0.8750 | R=0.1111 | F1=0.1972\n",
            "[epoch 34] train-loss=0.0079 | test-loss=0.4327 | train-acc=0.9993 | test-acc=0.9196 | P=1.0000 | R=0.0952 | F1=0.1739\n",
            "[epoch 35] train-loss=0.0082 | test-loss=0.3674 | train-acc=0.9989 | test-acc=0.9210 | P=1.0000 | R=0.1111 | F1=0.2000\n",
            "[epoch 36] train-loss=0.0072 | test-loss=0.4367 | train-acc=0.9989 | test-acc=0.9196 | P=1.0000 | R=0.0952 | F1=0.1739\n",
            "[epoch 37] train-loss=0.0055 | test-loss=0.4053 | train-acc=0.9996 | test-acc=0.9210 | P=1.0000 | R=0.1111 | F1=0.2000\n",
            "[epoch 38] train-loss=0.0071 | test-loss=0.4245 | train-acc=0.9989 | test-acc=0.9196 | P=1.0000 | R=0.0952 | F1=0.1739\n",
            "[epoch 39] train-loss=0.0056 | test-loss=0.4166 | train-acc=0.9996 | test-acc=0.9196 | P=0.8750 | R=0.1111 | F1=0.1972\n",
            "[epoch 40] train-loss=0.0051 | test-loss=0.3282 | train-acc=0.9996 | test-acc=0.9154 | P=0.6000 | R=0.1429 | F1=0.2308\n",
            "[epoch 41] train-loss=0.0048 | test-loss=0.4170 | train-acc=0.9996 | test-acc=0.9182 | P=0.7778 | R=0.1111 | F1=0.1944\n",
            "[epoch 42] train-loss=0.0051 | test-loss=0.4704 | train-acc=0.9996 | test-acc=0.9196 | P=1.0000 | R=0.0952 | F1=0.1739\n",
            "[epoch 43] train-loss=0.0054 | test-loss=0.2729 | train-acc=0.9996 | test-acc=0.9154 | P=0.5455 | R=0.2857 | F1=0.3750\n",
            "[epoch 44] train-loss=0.0075 | test-loss=0.3314 | train-acc=0.9993 | test-acc=0.9126 | P=0.5294 | R=0.1429 | F1=0.2250\n",
            "[epoch 45] train-loss=0.0061 | test-loss=0.3778 | train-acc=0.9989 | test-acc=0.9182 | P=0.7273 | R=0.1270 | F1=0.2162\n",
            "[epoch 46] train-loss=0.0043 | test-loss=0.3730 | train-acc=1.0000 | test-acc=0.9182 | P=0.6923 | R=0.1429 | F1=0.2368\n",
            "[epoch 47] train-loss=0.0038 | test-loss=0.4359 | train-acc=1.0000 | test-acc=0.9210 | P=1.0000 | R=0.1111 | F1=0.2000\n",
            "[epoch 48] train-loss=0.0037 | test-loss=0.3615 | train-acc=0.9996 | test-acc=0.9182 | P=0.6923 | R=0.1429 | F1=0.2368\n",
            "[epoch 49] train-loss=0.0025 | test-loss=0.3820 | train-acc=1.0000 | test-acc=0.9196 | P=0.7500 | R=0.1429 | F1=0.2400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### All five sets and large test set"
      ],
      "metadata": {
        "id": "HZGtsxFM8DUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_set = read_fasta('test_set_filt.f')\n",
        "\n",
        "len_list = []\n",
        "for s in test_set:\n",
        "    len_list.append(len(s))\n",
        "sns.histplot(len_list)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "Z_1i0nRKzdYo",
        "outputId": "392b3ff8-ea6b-40b2-8c2d-059815997b77"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: ylabel='Count'>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "num_steps = 1000 # max sequence length\n",
        "\n",
        "go_annotations = [\"GO_3A0005576\", \"GO_3A0005739\", \"GO_3A0007165\", \"GO_3A0043066\", \"GO_3A0055085\"]\n",
        "\n",
        "datalist_large_test = read_fasta('test_set_filt.f')\n",
        "labellist_large_test = [False for _ in range(len(datalist_large_test))]\n",
        "large_test = (datalist_large_test, labellist_large_test)\n",
        "\n",
        "large_test_result = []\n",
        "\n",
        "for go in go_annotations:\n",
        "    datalist, labellist = read(\"expr5Tseq_filtGO_100-1000.lis\", f\"{go}.annotprot\")\n",
        "    pos_datalist, neg_datalist = split_labelled(datalist, labellist)\n",
        "    neg_datalist = remove_sequences(neg_datalist, 0.2)\n",
        "    datalist, labellist = fuse_sequence_lists(pos_datalist, neg_datalist)\n",
        "\n",
        "    traindatalist, trainlabellist, testdatalist, testlabellist = generate_train_test(datalist, labellist, 0.8)\n",
        "    traindataset = [traindatalist, trainlabellist]\n",
        "    testdataset = [testdatalist, testlabellist]\n",
        "\n",
        "    # Set batch_size and num_steps (maximum sequence length)\n",
        "    train_iter = load_data(batch_size, num_steps, traindataset)\n",
        "    test_iter = load_data(batch_size, num_steps, testdataset)\n",
        "\n",
        "    # Train model\n",
        "    model = BerryCNN1D(vocab_size=21,dropout_rate=0,conv_channels=64)\n",
        "    model.apply(init_weights)\n",
        "    optimizer = torch.optim.SGD(\n",
        "        model.parameters(),\n",
        "        lr=0.01,\n",
        "        momentum=0.5\n",
        "    )\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    trainer = Trainer(model, loss_fn, optimizer, device)\n",
        "    df = trainer.train(epochs=50, train_iter=train_iter, test_iter=test_iter)\n",
        "\n",
        "    df.to_csv(f\"{go}.csv\")\n",
        "\n",
        "    # Run on large set\n",
        "    large_set_iter = load_data(batch_size, num_steps, large_test)\n",
        "\n",
        "    pos = 0\n",
        "    neg = 0\n",
        "    model.eval()\n",
        "\n",
        "    for seq, _ in large_set_iter:\n",
        "        outputs = model(seq.to(device))\n",
        "        for output in outputs:\n",
        "            o = output.tolist().index(max(output))\n",
        "            if o == 0:\n",
        "                neg += 1\n",
        "            elif o == 1:\n",
        "                pos += 1\n",
        "            else:\n",
        "                raise ValueError(f'{o=}')\n",
        "    print(f'{go}:\\t{pos = }\\t{neg = }')\n",
        "    large_test_result.append([go, pos])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ix_1KPPF5gJI",
        "outputId": "33729ea3-8ccd-456d-ec21-dd5faf782790"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-1b0abac1b1a5>:108: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  self.df = pd.concat([self.df, row], axis=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 00] train-loss=0.5105 | test-loss=0.6011 | train-acc=0.8081 | test-acc=0.7750 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 01] train-loss=0.4712 | test-loss=0.5881 | train-acc=0.8167 | test-acc=0.7750 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 02] train-loss=0.4517 | test-loss=0.5384 | train-acc=0.8167 | test-acc=0.7781 | P=1.0000 | R=0.0139 | F1=0.0274\n",
            "[epoch 03] train-loss=0.4283 | test-loss=0.5018 | train-acc=0.8229 | test-acc=0.7750 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 04] train-loss=0.4047 | test-loss=0.5872 | train-acc=0.8284 | test-acc=0.7125 | P=0.3864 | R=0.4722 | F1=0.4250\n",
            "[epoch 05] train-loss=0.3722 | test-loss=0.5152 | train-acc=0.8401 | test-acc=0.7438 | P=0.3214 | R=0.1250 | F1=0.1800\n",
            "[epoch 06] train-loss=0.3521 | test-loss=0.5542 | train-acc=0.8440 | test-acc=0.7188 | P=0.3977 | R=0.4861 | F1=0.4375\n",
            "[epoch 07] train-loss=0.3153 | test-loss=0.5328 | train-acc=0.8619 | test-acc=0.7500 | P=0.4444 | R=0.4444 | F1=0.4444\n",
            "[epoch 08] train-loss=0.2707 | test-loss=0.5496 | train-acc=0.8892 | test-acc=0.7219 | P=0.4045 | R=0.5000 | F1=0.4472\n",
            "[epoch 09] train-loss=0.2462 | test-loss=0.4923 | train-acc=0.9064 | test-acc=0.7750 | P=0.5000 | R=0.1528 | F1=0.2340\n",
            "[epoch 10] train-loss=0.1954 | test-loss=0.5124 | train-acc=0.9282 | test-acc=0.7469 | P=0.3953 | R=0.2361 | F1=0.2957\n",
            "[epoch 11] train-loss=0.1763 | test-loss=0.6201 | train-acc=0.9415 | test-acc=0.7719 | P=0.4000 | R=0.0278 | F1=0.0519\n",
            "[epoch 12] train-loss=0.1370 | test-loss=0.7018 | train-acc=0.9626 | test-acc=0.7656 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 13] train-loss=0.1166 | test-loss=0.8090 | train-acc=0.9711 | test-acc=0.7719 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 14] train-loss=0.0907 | test-loss=0.5937 | train-acc=0.9860 | test-acc=0.7844 | P=0.5714 | R=0.1667 | F1=0.2581\n",
            "[epoch 15] train-loss=0.0739 | test-loss=0.7859 | train-acc=0.9945 | test-acc=0.7719 | P=0.4000 | R=0.0278 | F1=0.0519\n",
            "[epoch 16] train-loss=0.0622 | test-loss=0.8413 | train-acc=0.9938 | test-acc=0.7719 | P=0.3333 | R=0.0139 | F1=0.0267\n",
            "[epoch 17] train-loss=0.0440 | test-loss=0.6800 | train-acc=1.0000 | test-acc=0.7844 | P=0.5789 | R=0.1528 | F1=0.2418\n",
            "[epoch 18] train-loss=0.0343 | test-loss=0.6041 | train-acc=1.0000 | test-acc=0.7656 | P=0.4595 | R=0.2361 | F1=0.3119\n",
            "[epoch 19] train-loss=0.0276 | test-loss=0.6123 | train-acc=1.0000 | test-acc=0.7594 | P=0.4359 | R=0.2361 | F1=0.3063\n",
            "[epoch 20] train-loss=0.0232 | test-loss=0.7421 | train-acc=1.0000 | test-acc=0.7875 | P=0.6250 | R=0.1389 | F1=0.2273\n",
            "[epoch 21] train-loss=0.0218 | test-loss=0.7215 | train-acc=1.0000 | test-acc=0.7781 | P=0.5238 | R=0.1528 | F1=0.2366\n",
            "[epoch 22] train-loss=0.0175 | test-loss=0.6643 | train-acc=1.0000 | test-acc=0.7781 | P=0.5152 | R=0.2361 | F1=0.3238\n",
            "[epoch 23] train-loss=0.0151 | test-loss=0.6888 | train-acc=1.0000 | test-acc=0.7719 | P=0.4839 | R=0.2083 | F1=0.2913\n",
            "[epoch 24] train-loss=0.0136 | test-loss=0.7293 | train-acc=1.0000 | test-acc=0.7750 | P=0.5000 | R=0.1667 | F1=0.2500\n",
            "[epoch 25] train-loss=0.0120 | test-loss=0.7707 | train-acc=1.0000 | test-acc=0.7812 | P=0.5500 | R=0.1528 | F1=0.2391\n",
            "[epoch 26] train-loss=0.0110 | test-loss=0.7923 | train-acc=1.0000 | test-acc=0.7812 | P=0.5500 | R=0.1528 | F1=0.2391\n",
            "[epoch 27] train-loss=0.0101 | test-loss=0.8153 | train-acc=1.0000 | test-acc=0.7812 | P=0.5556 | R=0.1389 | F1=0.2222\n",
            "[epoch 28] train-loss=0.0093 | test-loss=0.7690 | train-acc=1.0000 | test-acc=0.7750 | P=0.5000 | R=0.1667 | F1=0.2500\n",
            "[epoch 29] train-loss=0.0084 | test-loss=0.7639 | train-acc=1.0000 | test-acc=0.7750 | P=0.5000 | R=0.1806 | F1=0.2653\n",
            "[epoch 30] train-loss=0.0079 | test-loss=0.7974 | train-acc=1.0000 | test-acc=0.7750 | P=0.5000 | R=0.1528 | F1=0.2340\n",
            "[epoch 31] train-loss=0.0073 | test-loss=0.7428 | train-acc=1.0000 | test-acc=0.7781 | P=0.5161 | R=0.2222 | F1=0.3107\n",
            "[epoch 32] train-loss=0.0069 | test-loss=0.8273 | train-acc=1.0000 | test-acc=0.7781 | P=0.5238 | R=0.1528 | F1=0.2366\n",
            "[epoch 33] train-loss=0.0066 | test-loss=0.7946 | train-acc=1.0000 | test-acc=0.7750 | P=0.5000 | R=0.1806 | F1=0.2653\n",
            "[epoch 34] train-loss=0.0062 | test-loss=0.8109 | train-acc=1.0000 | test-acc=0.7750 | P=0.5000 | R=0.1667 | F1=0.2500\n",
            "[epoch 35] train-loss=0.0058 | test-loss=0.8046 | train-acc=1.0000 | test-acc=0.7750 | P=0.5000 | R=0.1806 | F1=0.2653\n",
            "[epoch 36] train-loss=0.0054 | test-loss=0.7508 | train-acc=1.0000 | test-acc=0.7719 | P=0.4848 | R=0.2222 | F1=0.3048\n",
            "[epoch 37] train-loss=0.0052 | test-loss=0.8754 | train-acc=1.0000 | test-acc=0.7781 | P=0.5263 | R=0.1389 | F1=0.2198\n",
            "[epoch 38] train-loss=0.0049 | test-loss=0.8443 | train-acc=1.0000 | test-acc=0.7719 | P=0.4783 | R=0.1528 | F1=0.2316\n",
            "[epoch 39] train-loss=0.0047 | test-loss=0.8268 | train-acc=1.0000 | test-acc=0.7781 | P=0.5185 | R=0.1944 | F1=0.2828\n",
            "[epoch 40] train-loss=0.0045 | test-loss=0.8265 | train-acc=1.0000 | test-acc=0.7781 | P=0.5185 | R=0.1944 | F1=0.2828\n",
            "[epoch 41] train-loss=0.0043 | test-loss=0.8259 | train-acc=1.0000 | test-acc=0.7781 | P=0.5172 | R=0.2083 | F1=0.2970\n",
            "[epoch 42] train-loss=0.0041 | test-loss=0.8780 | train-acc=1.0000 | test-acc=0.7750 | P=0.5000 | R=0.1528 | F1=0.2340\n",
            "[epoch 43] train-loss=0.0039 | test-loss=0.8488 | train-acc=1.0000 | test-acc=0.7750 | P=0.5000 | R=0.1806 | F1=0.2653\n",
            "[epoch 44] train-loss=0.0038 | test-loss=0.7975 | train-acc=1.0000 | test-acc=0.7750 | P=0.5000 | R=0.2222 | F1=0.3077\n",
            "[epoch 45] train-loss=0.0037 | test-loss=0.8512 | train-acc=1.0000 | test-acc=0.7781 | P=0.5185 | R=0.1944 | F1=0.2828\n",
            "[epoch 46] train-loss=0.0035 | test-loss=0.8391 | train-acc=1.0000 | test-acc=0.7781 | P=0.5172 | R=0.2083 | F1=0.2970\n",
            "[epoch 47] train-loss=0.0034 | test-loss=0.8228 | train-acc=1.0000 | test-acc=0.7781 | P=0.5161 | R=0.2222 | F1=0.3107\n",
            "[epoch 48] train-loss=0.0033 | test-loss=0.8884 | train-acc=1.0000 | test-acc=0.7781 | P=0.5217 | R=0.1667 | F1=0.2526\n",
            "[epoch 49] train-loss=0.0032 | test-loss=0.8710 | train-acc=1.0000 | test-acc=0.7750 | P=0.5000 | R=0.1806 | F1=0.2653\n",
            "GO_3A0005576:\tpos = 1408\tneg = 13357\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-1b0abac1b1a5>:108: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  self.df = pd.concat([self.df, row], axis=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 00] train-loss=0.6116 | test-loss=0.5879 | train-acc=0.7008 | test-acc=0.7205 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 01] train-loss=0.5821 | test-loss=0.5735 | train-acc=0.7318 | test-acc=0.7205 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 02] train-loss=0.5712 | test-loss=0.5721 | train-acc=0.7296 | test-acc=0.7176 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 03] train-loss=0.5430 | test-loss=0.5695 | train-acc=0.7340 | test-acc=0.7205 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 04] train-loss=0.5192 | test-loss=0.5727 | train-acc=0.7426 | test-acc=0.7205 | P=0.5000 | R=0.1443 | F1=0.2240\n",
            "[epoch 05] train-loss=0.4777 | test-loss=0.6312 | train-acc=0.7722 | test-acc=0.7205 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 06] train-loss=0.4533 | test-loss=0.6521 | train-acc=0.7916 | test-acc=0.7205 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 07] train-loss=0.4072 | test-loss=0.6202 | train-acc=0.8291 | test-acc=0.6311 | P=0.3817 | R=0.5155 | F1=0.4386\n",
            "[epoch 08] train-loss=0.3676 | test-loss=0.8558 | train-acc=0.8407 | test-acc=0.7205 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 09] train-loss=0.3311 | test-loss=0.5644 | train-acc=0.8681 | test-acc=0.7118 | P=0.4754 | R=0.2990 | F1=0.3671\n",
            "[epoch 10] train-loss=0.3023 | test-loss=0.5672 | train-acc=0.8681 | test-acc=0.7147 | P=0.4848 | R=0.3299 | F1=0.3926\n",
            "[epoch 11] train-loss=0.2429 | test-loss=0.5967 | train-acc=0.9257 | test-acc=0.7349 | P=0.6000 | R=0.1546 | F1=0.2459\n",
            "[epoch 12] train-loss=0.2004 | test-loss=0.6192 | train-acc=0.9387 | test-acc=0.7262 | P=0.5357 | R=0.1546 | F1=0.2400\n",
            "[epoch 13] train-loss=0.1872 | test-loss=0.5920 | train-acc=0.9488 | test-acc=0.7205 | P=0.5000 | R=0.2371 | F1=0.3217\n",
            "[epoch 14] train-loss=0.1401 | test-loss=0.7750 | train-acc=0.9676 | test-acc=0.7262 | P=0.6000 | R=0.0619 | F1=0.1121\n",
            "[epoch 15] train-loss=0.1093 | test-loss=0.7181 | train-acc=0.9820 | test-acc=0.6110 | P=0.3797 | R=0.6186 | F1=0.4706\n",
            "[epoch 16] train-loss=0.0809 | test-loss=0.6320 | train-acc=0.9921 | test-acc=0.7118 | P=0.4805 | R=0.3814 | F1=0.4253\n",
            "[epoch 17] train-loss=0.0570 | test-loss=0.6899 | train-acc=0.9978 | test-acc=0.6628 | P=0.4153 | R=0.5052 | F1=0.4558\n",
            "[epoch 18] train-loss=0.0470 | test-loss=0.7665 | train-acc=0.9986 | test-acc=0.7291 | P=0.5600 | R=0.1443 | F1=0.2295\n",
            "[epoch 19] train-loss=0.0432 | test-loss=0.6737 | train-acc=0.9971 | test-acc=0.7291 | P=0.5211 | R=0.3814 | F1=0.4405\n",
            "[epoch 20] train-loss=0.0317 | test-loss=0.6754 | train-acc=0.9986 | test-acc=0.7032 | P=0.4667 | R=0.4330 | F1=0.4492\n",
            "[epoch 21] train-loss=0.0265 | test-loss=0.6957 | train-acc=0.9993 | test-acc=0.7118 | P=0.4810 | R=0.3918 | F1=0.4318\n",
            "[epoch 22] train-loss=0.0268 | test-loss=0.7757 | train-acc=0.9978 | test-acc=0.7291 | P=0.5405 | R=0.2062 | F1=0.2985\n",
            "[epoch 23] train-loss=0.0249 | test-loss=0.7458 | train-acc=0.9978 | test-acc=0.7262 | P=0.5208 | R=0.2577 | F1=0.3448\n",
            "[epoch 24] train-loss=0.0254 | test-loss=0.7106 | train-acc=0.9964 | test-acc=0.7061 | P=0.4719 | R=0.4330 | F1=0.4516\n",
            "[epoch 25] train-loss=0.0213 | test-loss=0.7525 | train-acc=0.9986 | test-acc=0.7291 | P=0.5254 | R=0.3196 | F1=0.3974\n",
            "[epoch 26] train-loss=0.0201 | test-loss=0.7512 | train-acc=0.9971 | test-acc=0.7176 | P=0.4921 | R=0.3196 | F1=0.3875\n",
            "[epoch 27] train-loss=0.0182 | test-loss=0.7848 | train-acc=0.9978 | test-acc=0.7176 | P=0.4894 | R=0.2371 | F1=0.3194\n",
            "[epoch 28] train-loss=0.0131 | test-loss=0.7565 | train-acc=0.9993 | test-acc=0.7291 | P=0.5224 | R=0.3608 | F1=0.4268\n",
            "[epoch 29] train-loss=0.0163 | test-loss=0.8267 | train-acc=0.9986 | test-acc=0.7262 | P=0.5238 | R=0.2268 | F1=0.3165\n",
            "[epoch 30] train-loss=0.0150 | test-loss=0.7720 | train-acc=0.9986 | test-acc=0.7320 | P=0.5294 | R=0.3711 | F1=0.4364\n",
            "[epoch 31] train-loss=0.0163 | test-loss=0.7928 | train-acc=0.9971 | test-acc=0.7349 | P=0.5424 | R=0.3299 | F1=0.4103\n",
            "[epoch 32] train-loss=0.0112 | test-loss=0.8154 | train-acc=0.9993 | test-acc=0.7205 | P=0.5000 | R=0.2784 | F1=0.3576\n",
            "[epoch 33] train-loss=0.0126 | test-loss=0.8092 | train-acc=0.9993 | test-acc=0.7320 | P=0.5345 | R=0.3196 | F1=0.4000\n",
            "[epoch 34] train-loss=0.0153 | test-loss=0.7811 | train-acc=0.9986 | test-acc=0.7262 | P=0.5132 | R=0.4021 | F1=0.4509\n",
            "[epoch 35] train-loss=0.0109 | test-loss=0.7974 | train-acc=0.9993 | test-acc=0.7147 | P=0.4868 | R=0.3814 | F1=0.4277\n",
            "[epoch 36] train-loss=0.0144 | test-loss=0.8311 | train-acc=0.9986 | test-acc=0.7291 | P=0.5263 | R=0.3093 | F1=0.3896\n",
            "[epoch 37] train-loss=0.0131 | test-loss=0.8038 | train-acc=0.9986 | test-acc=0.7003 | P=0.4598 | R=0.4124 | F1=0.4348\n",
            "[epoch 38] train-loss=0.0092 | test-loss=0.8824 | train-acc=0.9993 | test-acc=0.7320 | P=0.5476 | R=0.2371 | F1=0.3309\n",
            "[epoch 39] train-loss=0.0123 | test-loss=0.8137 | train-acc=0.9986 | test-acc=0.7118 | P=0.4815 | R=0.4021 | F1=0.4382\n",
            "[epoch 40] train-loss=0.0091 | test-loss=0.8081 | train-acc=0.9986 | test-acc=0.7176 | P=0.4938 | R=0.4124 | F1=0.4494\n",
            "[epoch 41] train-loss=0.0088 | test-loss=0.8580 | train-acc=0.9986 | test-acc=0.7291 | P=0.5254 | R=0.3196 | F1=0.3974\n",
            "[epoch 42] train-loss=0.0126 | test-loss=0.8305 | train-acc=0.9978 | test-acc=0.7205 | P=0.5000 | R=0.3814 | F1=0.4327\n",
            "[epoch 43] train-loss=0.0088 | test-loss=0.8538 | train-acc=0.9993 | test-acc=0.7233 | P=0.5077 | R=0.3402 | F1=0.4074\n",
            "[epoch 44] train-loss=0.0077 | test-loss=0.8555 | train-acc=0.9993 | test-acc=0.7205 | P=0.5000 | R=0.3402 | F1=0.4049\n",
            "[epoch 45] train-loss=0.0125 | test-loss=0.8549 | train-acc=0.9978 | test-acc=0.7118 | P=0.4783 | R=0.3402 | F1=0.3976\n",
            "[epoch 46] train-loss=0.0125 | test-loss=0.8771 | train-acc=0.9978 | test-acc=0.7320 | P=0.5345 | R=0.3196 | F1=0.4000\n",
            "[epoch 47] train-loss=0.0084 | test-loss=0.8702 | train-acc=0.9993 | test-acc=0.7205 | P=0.5000 | R=0.3505 | F1=0.4121\n",
            "[epoch 48] train-loss=0.0079 | test-loss=0.8552 | train-acc=0.9993 | test-acc=0.7176 | P=0.4933 | R=0.3814 | F1=0.4302\n",
            "[epoch 49] train-loss=0.0073 | test-loss=0.8827 | train-acc=0.9993 | test-acc=0.7262 | P=0.5152 | R=0.3505 | F1=0.4172\n",
            "GO_3A0005739:\tpos = 2328\tneg = 12437\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-1b0abac1b1a5>:108: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  self.df = pd.concat([self.df, row], axis=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 00] train-loss=0.6075 | test-loss=0.5233 | train-acc=0.7378 | test-acc=0.7729 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 01] train-loss=0.5747 | test-loss=0.5472 | train-acc=0.7400 | test-acc=0.7729 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 02] train-loss=0.5573 | test-loss=0.5671 | train-acc=0.7400 | test-acc=0.7729 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 03] train-loss=0.5364 | test-loss=0.5186 | train-acc=0.7482 | test-acc=0.7729 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 04] train-loss=0.5055 | test-loss=0.5373 | train-acc=0.7496 | test-acc=0.7729 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 05] train-loss=0.4843 | test-loss=0.5139 | train-acc=0.7718 | test-acc=0.7729 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 06] train-loss=0.4536 | test-loss=0.5121 | train-acc=0.7814 | test-acc=0.7729 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 07] train-loss=0.4240 | test-loss=0.7062 | train-acc=0.8072 | test-acc=0.5044 | P=0.2667 | R=0.6753 | F1=0.3824\n",
            "[epoch 08] train-loss=0.3859 | test-loss=0.5473 | train-acc=0.8471 | test-acc=0.7729 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 09] train-loss=0.3467 | test-loss=0.6010 | train-acc=0.8582 | test-acc=0.6785 | P=0.3140 | R=0.3506 | F1=0.3313\n",
            "[epoch 10] train-loss=0.3077 | test-loss=0.6019 | train-acc=0.8804 | test-acc=0.7729 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 11] train-loss=0.2791 | test-loss=1.0128 | train-acc=0.8922 | test-acc=0.7729 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 12] train-loss=0.2492 | test-loss=0.7966 | train-acc=0.9269 | test-acc=0.7729 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 13] train-loss=0.2025 | test-loss=0.5748 | train-acc=0.9298 | test-acc=0.7522 | P=0.3333 | R=0.0909 | F1=0.1429\n",
            "[epoch 14] train-loss=0.1507 | test-loss=0.6305 | train-acc=0.9675 | test-acc=0.7788 | P=0.6667 | R=0.0519 | F1=0.0964\n",
            "[epoch 15] train-loss=0.1165 | test-loss=1.2856 | train-acc=0.9860 | test-acc=0.7729 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 16] train-loss=0.0940 | test-loss=0.6369 | train-acc=0.9911 | test-acc=0.7493 | P=0.3182 | R=0.0909 | F1=0.1414\n",
            "[epoch 17] train-loss=0.0639 | test-loss=1.3404 | train-acc=0.9978 | test-acc=0.3717 | P=0.2500 | R=0.8831 | F1=0.3897\n",
            "[epoch 18] train-loss=0.0574 | test-loss=0.8870 | train-acc=0.9970 | test-acc=0.5398 | P=0.2663 | R=0.5844 | F1=0.3659\n",
            "[epoch 19] train-loss=0.0505 | test-loss=0.6449 | train-acc=0.9970 | test-acc=0.7493 | P=0.3947 | R=0.1948 | F1=0.2609\n",
            "[epoch 20] train-loss=0.0318 | test-loss=0.6598 | train-acc=1.0000 | test-acc=0.7493 | P=0.3947 | R=0.1948 | F1=0.2609\n",
            "[epoch 21] train-loss=0.0266 | test-loss=0.6792 | train-acc=1.0000 | test-acc=0.7493 | P=0.3889 | R=0.1818 | F1=0.2478\n",
            "[epoch 22] train-loss=0.0225 | test-loss=0.6753 | train-acc=1.0000 | test-acc=0.7375 | P=0.3571 | R=0.1948 | F1=0.2521\n",
            "[epoch 23] train-loss=0.0190 | test-loss=0.7414 | train-acc=1.0000 | test-acc=0.7670 | P=0.4583 | R=0.1429 | F1=0.2178\n",
            "[epoch 24] train-loss=0.0169 | test-loss=0.6861 | train-acc=1.0000 | test-acc=0.7257 | P=0.3400 | R=0.2208 | F1=0.2677\n",
            "[epoch 25] train-loss=0.0147 | test-loss=0.7129 | train-acc=1.0000 | test-acc=0.7463 | P=0.3846 | R=0.1948 | F1=0.2586\n",
            "[epoch 26] train-loss=0.0133 | test-loss=0.6989 | train-acc=1.0000 | test-acc=0.7139 | P=0.3276 | R=0.2468 | F1=0.2815\n",
            "[epoch 27] train-loss=0.0123 | test-loss=0.7471 | train-acc=1.0000 | test-acc=0.7581 | P=0.4242 | R=0.1818 | F1=0.2545\n",
            "[epoch 28] train-loss=0.0111 | test-loss=0.8139 | train-acc=1.0000 | test-acc=0.7699 | P=0.4783 | R=0.1429 | F1=0.2200\n",
            "[epoch 29] train-loss=0.0103 | test-loss=0.7593 | train-acc=1.0000 | test-acc=0.7552 | P=0.4118 | R=0.1818 | F1=0.2523\n",
            "[epoch 30] train-loss=0.0094 | test-loss=0.7875 | train-acc=1.0000 | test-acc=0.7640 | P=0.4516 | R=0.1818 | F1=0.2593\n",
            "[epoch 31] train-loss=0.0088 | test-loss=0.7452 | train-acc=1.0000 | test-acc=0.7434 | P=0.3864 | R=0.2208 | F1=0.2810\n",
            "[epoch 32] train-loss=0.0083 | test-loss=0.7399 | train-acc=1.0000 | test-acc=0.7286 | P=0.3529 | R=0.2338 | F1=0.2812\n",
            "[epoch 33] train-loss=0.0077 | test-loss=0.7700 | train-acc=1.0000 | test-acc=0.7463 | P=0.3846 | R=0.1948 | F1=0.2586\n",
            "[epoch 34] train-loss=0.0073 | test-loss=0.7943 | train-acc=1.0000 | test-acc=0.7522 | P=0.4000 | R=0.1818 | F1=0.2500\n",
            "[epoch 35] train-loss=0.0068 | test-loss=0.7566 | train-acc=1.0000 | test-acc=0.7316 | P=0.3542 | R=0.2208 | F1=0.2720\n",
            "[epoch 36] train-loss=0.0064 | test-loss=0.7768 | train-acc=1.0000 | test-acc=0.7463 | P=0.3953 | R=0.2208 | F1=0.2833\n",
            "[epoch 37] train-loss=0.0060 | test-loss=0.7823 | train-acc=1.0000 | test-acc=0.7463 | P=0.3953 | R=0.2208 | F1=0.2833\n",
            "[epoch 38] train-loss=0.0058 | test-loss=0.7868 | train-acc=1.0000 | test-acc=0.7434 | P=0.3810 | R=0.2078 | F1=0.2689\n",
            "[epoch 39] train-loss=0.0055 | test-loss=0.8073 | train-acc=1.0000 | test-acc=0.7493 | P=0.3947 | R=0.1948 | F1=0.2609\n",
            "[epoch 40] train-loss=0.0052 | test-loss=0.7917 | train-acc=1.0000 | test-acc=0.7434 | P=0.3864 | R=0.2208 | F1=0.2810\n",
            "[epoch 41] train-loss=0.0050 | test-loss=0.8022 | train-acc=1.0000 | test-acc=0.7493 | P=0.4048 | R=0.2208 | F1=0.2857\n",
            "[epoch 42] train-loss=0.0048 | test-loss=0.7907 | train-acc=1.0000 | test-acc=0.7345 | P=0.3617 | R=0.2208 | F1=0.2742\n",
            "[epoch 43] train-loss=0.0046 | test-loss=0.8023 | train-acc=1.0000 | test-acc=0.7434 | P=0.3864 | R=0.2208 | F1=0.2810\n",
            "[epoch 44] train-loss=0.0044 | test-loss=0.8541 | train-acc=1.0000 | test-acc=0.7670 | P=0.4667 | R=0.1818 | F1=0.2617\n",
            "[epoch 45] train-loss=0.0042 | test-loss=0.8157 | train-acc=1.0000 | test-acc=0.7463 | P=0.3902 | R=0.2078 | F1=0.2712\n",
            "[epoch 46] train-loss=0.0040 | test-loss=0.8375 | train-acc=1.0000 | test-acc=0.7522 | P=0.4054 | R=0.1948 | F1=0.2632\n",
            "[epoch 47] train-loss=0.0039 | test-loss=0.8125 | train-acc=1.0000 | test-acc=0.7345 | P=0.3617 | R=0.2208 | F1=0.2742\n",
            "[epoch 48] train-loss=0.0038 | test-loss=0.8174 | train-acc=1.0000 | test-acc=0.7404 | P=0.3778 | R=0.2208 | F1=0.2787\n",
            "[epoch 49] train-loss=0.0036 | test-loss=0.8301 | train-acc=1.0000 | test-acc=0.7463 | P=0.3953 | R=0.2208 | F1=0.2833\n",
            "GO_3A0007165:\tpos = 977\tneg = 13788\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-1b0abac1b1a5>:108: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  self.df = pd.concat([self.df, row], axis=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 00] train-loss=0.4009 | test-loss=0.4022 | train-acc=0.8757 | test-acc=0.8675 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 01] train-loss=0.3816 | test-loss=0.3729 | train-acc=0.8757 | test-acc=0.8675 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 02] train-loss=0.3684 | test-loss=0.4000 | train-acc=0.8757 | test-acc=0.8675 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 03] train-loss=0.3568 | test-loss=0.3998 | train-acc=0.8757 | test-acc=0.8675 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 04] train-loss=0.3487 | test-loss=0.3912 | train-acc=0.8757 | test-acc=0.8675 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 05] train-loss=0.3372 | test-loss=0.3730 | train-acc=0.8757 | test-acc=0.8675 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 06] train-loss=0.3172 | test-loss=0.3731 | train-acc=0.8774 | test-acc=0.8675 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 07] train-loss=0.2988 | test-loss=0.3990 | train-acc=0.8774 | test-acc=0.8709 | P=1.0000 | R=0.0250 | F1=0.0488\n",
            "[epoch 08] train-loss=0.2775 | test-loss=0.4008 | train-acc=0.8782 | test-acc=0.8709 | P=1.0000 | R=0.0250 | F1=0.0488\n",
            "[epoch 09] train-loss=0.2552 | test-loss=0.4106 | train-acc=0.8890 | test-acc=0.8609 | P=0.2500 | R=0.0250 | F1=0.0455\n",
            "[epoch 10] train-loss=0.2247 | test-loss=0.3783 | train-acc=0.8940 | test-acc=0.8642 | P=0.3333 | R=0.0250 | F1=0.0465\n",
            "[epoch 11] train-loss=0.1935 | test-loss=0.4401 | train-acc=0.9221 | test-acc=0.8709 | P=1.0000 | R=0.0250 | F1=0.0488\n",
            "[epoch 12] train-loss=0.1670 | test-loss=0.3832 | train-acc=0.9321 | test-acc=0.8675 | P=0.5000 | R=0.0250 | F1=0.0476\n",
            "[epoch 13] train-loss=0.1381 | test-loss=0.4452 | train-acc=0.9544 | test-acc=0.8709 | P=1.0000 | R=0.0250 | F1=0.0488\n",
            "[epoch 14] train-loss=0.1162 | test-loss=0.4005 | train-acc=0.9685 | test-acc=0.8675 | P=0.5000 | R=0.0250 | F1=0.0476\n",
            "[epoch 15] train-loss=0.0906 | test-loss=0.4732 | train-acc=0.9851 | test-acc=0.8709 | P=1.0000 | R=0.0250 | F1=0.0488\n",
            "[epoch 16] train-loss=0.0788 | test-loss=0.4401 | train-acc=0.9867 | test-acc=0.8709 | P=1.0000 | R=0.0250 | F1=0.0488\n",
            "[epoch 17] train-loss=0.0633 | test-loss=0.5153 | train-acc=0.9959 | test-acc=0.8675 | P=0.5000 | R=0.0250 | F1=0.0476\n",
            "[epoch 18] train-loss=0.0488 | test-loss=0.5162 | train-acc=0.9975 | test-acc=0.8675 | P=0.5000 | R=0.0250 | F1=0.0476\n",
            "[epoch 19] train-loss=0.0419 | test-loss=0.4501 | train-acc=0.9983 | test-acc=0.8675 | P=0.5000 | R=0.0250 | F1=0.0476\n",
            "[epoch 20] train-loss=0.0340 | test-loss=0.6068 | train-acc=1.0000 | test-acc=0.8709 | P=1.0000 | R=0.0250 | F1=0.0488\n",
            "[epoch 21] train-loss=0.0288 | test-loss=0.4600 | train-acc=1.0000 | test-acc=0.8609 | P=0.2500 | R=0.0250 | F1=0.0455\n",
            "[epoch 22] train-loss=0.0245 | test-loss=0.4570 | train-acc=1.0000 | test-acc=0.8576 | P=0.2000 | R=0.0250 | F1=0.0444\n",
            "[epoch 23] train-loss=0.0214 | test-loss=0.5061 | train-acc=1.0000 | test-acc=0.8675 | P=0.5000 | R=0.0250 | F1=0.0476\n",
            "[epoch 24] train-loss=0.0188 | test-loss=0.5560 | train-acc=1.0000 | test-acc=0.8709 | P=1.0000 | R=0.0250 | F1=0.0488\n",
            "[epoch 25] train-loss=0.0166 | test-loss=0.4955 | train-acc=1.0000 | test-acc=0.8609 | P=0.2500 | R=0.0250 | F1=0.0455\n",
            "[epoch 26] train-loss=0.0149 | test-loss=0.5226 | train-acc=1.0000 | test-acc=0.8642 | P=0.3333 | R=0.0250 | F1=0.0465\n",
            "[epoch 27] train-loss=0.0135 | test-loss=0.5714 | train-acc=1.0000 | test-acc=0.8675 | P=0.5000 | R=0.0250 | F1=0.0476\n",
            "[epoch 28] train-loss=0.0122 | test-loss=0.5323 | train-acc=1.0000 | test-acc=0.8642 | P=0.3333 | R=0.0250 | F1=0.0465\n",
            "[epoch 29] train-loss=0.0113 | test-loss=0.5569 | train-acc=1.0000 | test-acc=0.8642 | P=0.3333 | R=0.0250 | F1=0.0465\n",
            "[epoch 30] train-loss=0.0101 | test-loss=0.5853 | train-acc=1.0000 | test-acc=0.8675 | P=0.5000 | R=0.0250 | F1=0.0476\n",
            "[epoch 31] train-loss=0.0093 | test-loss=0.5539 | train-acc=1.0000 | test-acc=0.8642 | P=0.3333 | R=0.0250 | F1=0.0465\n",
            "[epoch 32] train-loss=0.0087 | test-loss=0.5579 | train-acc=1.0000 | test-acc=0.8642 | P=0.3333 | R=0.0250 | F1=0.0465\n",
            "[epoch 33] train-loss=0.0082 | test-loss=0.5804 | train-acc=1.0000 | test-acc=0.8675 | P=0.5000 | R=0.0250 | F1=0.0476\n",
            "[epoch 34] train-loss=0.0076 | test-loss=0.5940 | train-acc=1.0000 | test-acc=0.8675 | P=0.5000 | R=0.0250 | F1=0.0476\n",
            "[epoch 35] train-loss=0.0072 | test-loss=0.5955 | train-acc=1.0000 | test-acc=0.8675 | P=0.5000 | R=0.0250 | F1=0.0476\n",
            "[epoch 36] train-loss=0.0068 | test-loss=0.5840 | train-acc=1.0000 | test-acc=0.8642 | P=0.3333 | R=0.0250 | F1=0.0465\n",
            "[epoch 37] train-loss=0.0065 | test-loss=0.5701 | train-acc=1.0000 | test-acc=0.8576 | P=0.2000 | R=0.0250 | F1=0.0444\n",
            "[epoch 38] train-loss=0.0061 | test-loss=0.6159 | train-acc=1.0000 | test-acc=0.8675 | P=0.5000 | R=0.0250 | F1=0.0476\n",
            "[epoch 39] train-loss=0.0057 | test-loss=0.5802 | train-acc=1.0000 | test-acc=0.8609 | P=0.2500 | R=0.0250 | F1=0.0455\n",
            "[epoch 40] train-loss=0.0055 | test-loss=0.5739 | train-acc=1.0000 | test-acc=0.8576 | P=0.2000 | R=0.0250 | F1=0.0444\n",
            "[epoch 41] train-loss=0.0051 | test-loss=0.6526 | train-acc=1.0000 | test-acc=0.8642 | P=0.3333 | R=0.0250 | F1=0.0465\n",
            "[epoch 42] train-loss=0.0049 | test-loss=0.6018 | train-acc=1.0000 | test-acc=0.8642 | P=0.3333 | R=0.0250 | F1=0.0465\n",
            "[epoch 43] train-loss=0.0048 | test-loss=0.7356 | train-acc=1.0000 | test-acc=0.8642 | P=0.3333 | R=0.0250 | F1=0.0465\n",
            "[epoch 44] train-loss=0.0046 | test-loss=0.7165 | train-acc=1.0000 | test-acc=0.8642 | P=0.3333 | R=0.0250 | F1=0.0465\n",
            "[epoch 45] train-loss=0.0044 | test-loss=0.6871 | train-acc=1.0000 | test-acc=0.8642 | P=0.3333 | R=0.0250 | F1=0.0465\n",
            "[epoch 46] train-loss=0.0042 | test-loss=0.6136 | train-acc=1.0000 | test-acc=0.8642 | P=0.3333 | R=0.0250 | F1=0.0465\n",
            "[epoch 47] train-loss=0.0040 | test-loss=0.6037 | train-acc=1.0000 | test-acc=0.8576 | P=0.2000 | R=0.0250 | F1=0.0444\n",
            "[epoch 48] train-loss=0.0038 | test-loss=0.5939 | train-acc=1.0000 | test-acc=0.8477 | P=0.1250 | R=0.0250 | F1=0.0417\n",
            "[epoch 49] train-loss=0.0037 | test-loss=0.6375 | train-acc=1.0000 | test-acc=0.8642 | P=0.3333 | R=0.0250 | F1=0.0465\n",
            "GO_3A0043066:\tpos = 65\tneg = 14700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-1b0abac1b1a5>:108: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  self.df = pd.concat([self.df, row], axis=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 00] train-loss=0.4570 | test-loss=0.4163 | train-acc=0.8488 | test-acc=0.8442 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 01] train-loss=0.4222 | test-loss=0.4132 | train-acc=0.8553 | test-acc=0.8442 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 02] train-loss=0.4005 | test-loss=0.3976 | train-acc=0.8585 | test-acc=0.8442 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 03] train-loss=0.3777 | test-loss=0.3932 | train-acc=0.8553 | test-acc=0.8442 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 04] train-loss=0.3512 | test-loss=0.4107 | train-acc=0.8569 | test-acc=0.8539 | P=0.7143 | R=0.1042 | F1=0.1818\n",
            "[epoch 05] train-loss=0.3196 | test-loss=0.3858 | train-acc=0.8707 | test-acc=0.8571 | P=0.7000 | R=0.1458 | F1=0.2414\n",
            "[epoch 06] train-loss=0.2937 | test-loss=0.4028 | train-acc=0.8870 | test-acc=0.8442 | P=0.0000 | R=0.0000 | F1=0.0000\n",
            "[epoch 07] train-loss=0.2636 | test-loss=0.3829 | train-acc=0.8959 | test-acc=0.8474 | P=1.0000 | R=0.0208 | F1=0.0408\n",
            "[epoch 08] train-loss=0.2238 | test-loss=0.3384 | train-acc=0.9122 | test-acc=0.8799 | P=0.6897 | R=0.4167 | F1=0.5195\n",
            "[epoch 09] train-loss=0.1899 | test-loss=0.3616 | train-acc=0.9390 | test-acc=0.8539 | P=1.0000 | R=0.0625 | F1=0.1176\n",
            "[epoch 10] train-loss=0.1840 | test-loss=0.3513 | train-acc=0.9317 | test-acc=0.8636 | P=1.0000 | R=0.1250 | F1=0.2222\n",
            "[epoch 11] train-loss=0.1483 | test-loss=0.4384 | train-acc=0.9520 | test-acc=0.8506 | P=1.0000 | R=0.0417 | F1=0.0800\n",
            "[epoch 12] train-loss=0.1215 | test-loss=0.3297 | train-acc=0.9626 | test-acc=0.8701 | P=0.7500 | R=0.2500 | F1=0.3750\n",
            "[epoch 13] train-loss=0.0985 | test-loss=0.3811 | train-acc=0.9724 | test-acc=0.8571 | P=0.8333 | R=0.1042 | F1=0.1852\n",
            "[epoch 14] train-loss=0.0813 | test-loss=0.3238 | train-acc=0.9837 | test-acc=0.8766 | P=0.6923 | R=0.3750 | F1=0.4865\n",
            "[epoch 15] train-loss=0.0623 | test-loss=0.3403 | train-acc=0.9927 | test-acc=0.8701 | P=0.7000 | R=0.2917 | F1=0.4118\n",
            "[epoch 16] train-loss=0.0482 | test-loss=0.3465 | train-acc=0.9976 | test-acc=0.8799 | P=0.7391 | R=0.3542 | F1=0.4789\n",
            "[epoch 17] train-loss=0.0400 | test-loss=0.3532 | train-acc=1.0000 | test-acc=0.8799 | P=0.7391 | R=0.3542 | F1=0.4789\n",
            "[epoch 18] train-loss=0.0315 | test-loss=0.3791 | train-acc=1.0000 | test-acc=0.8701 | P=0.7222 | R=0.2708 | F1=0.3939\n",
            "[epoch 19] train-loss=0.0253 | test-loss=0.3616 | train-acc=1.0000 | test-acc=0.8831 | P=0.7500 | R=0.3750 | F1=0.5000\n",
            "[epoch 20] train-loss=0.0232 | test-loss=0.3487 | train-acc=1.0000 | test-acc=0.8766 | P=0.6250 | R=0.5208 | F1=0.5682\n",
            "[epoch 21] train-loss=0.0198 | test-loss=0.4946 | train-acc=1.0000 | test-acc=0.8539 | P=0.8000 | R=0.0833 | F1=0.1509\n",
            "[epoch 22] train-loss=0.0170 | test-loss=0.3745 | train-acc=1.0000 | test-acc=0.8799 | P=0.7391 | R=0.3542 | F1=0.4789\n",
            "[epoch 23] train-loss=0.0155 | test-loss=0.3757 | train-acc=1.0000 | test-acc=0.8799 | P=0.7391 | R=0.3542 | F1=0.4789\n",
            "[epoch 24] train-loss=0.0140 | test-loss=0.4161 | train-acc=1.0000 | test-acc=0.8701 | P=0.7222 | R=0.2708 | F1=0.3939\n",
            "[epoch 25] train-loss=0.0121 | test-loss=0.4187 | train-acc=1.0000 | test-acc=0.8734 | P=0.7368 | R=0.2917 | F1=0.4179\n",
            "[epoch 26] train-loss=0.0112 | test-loss=0.4179 | train-acc=1.0000 | test-acc=0.8831 | P=0.7727 | R=0.3542 | F1=0.4857\n",
            "[epoch 27] train-loss=0.0099 | test-loss=0.3928 | train-acc=1.0000 | test-acc=0.8831 | P=0.7500 | R=0.3750 | F1=0.5000\n",
            "[epoch 28] train-loss=0.0092 | test-loss=0.4407 | train-acc=1.0000 | test-acc=0.8734 | P=0.7647 | R=0.2708 | F1=0.4000\n",
            "[epoch 29] train-loss=0.0087 | test-loss=0.4143 | train-acc=1.0000 | test-acc=0.8831 | P=0.7727 | R=0.3542 | F1=0.4857\n",
            "[epoch 30] train-loss=0.0081 | test-loss=0.4080 | train-acc=1.0000 | test-acc=0.8799 | P=0.7391 | R=0.3542 | F1=0.4789\n",
            "[epoch 31] train-loss=0.0074 | test-loss=0.4206 | train-acc=1.0000 | test-acc=0.8831 | P=0.7727 | R=0.3542 | F1=0.4857\n",
            "[epoch 32] train-loss=0.0070 | test-loss=0.4188 | train-acc=1.0000 | test-acc=0.8799 | P=0.7391 | R=0.3542 | F1=0.4789\n",
            "[epoch 33] train-loss=0.0065 | test-loss=0.3971 | train-acc=1.0000 | test-acc=0.8961 | P=0.7857 | R=0.4583 | F1=0.5789\n",
            "[epoch 34] train-loss=0.0063 | test-loss=0.4173 | train-acc=1.0000 | test-acc=0.8799 | P=0.7391 | R=0.3542 | F1=0.4789\n",
            "[epoch 35] train-loss=0.0058 | test-loss=0.4540 | train-acc=1.0000 | test-acc=0.8734 | P=0.7368 | R=0.2917 | F1=0.4179\n",
            "[epoch 36] train-loss=0.0055 | test-loss=0.4287 | train-acc=1.0000 | test-acc=0.8831 | P=0.7727 | R=0.3542 | F1=0.4857\n",
            "[epoch 37] train-loss=0.0052 | test-loss=0.4166 | train-acc=1.0000 | test-acc=0.8831 | P=0.7500 | R=0.3750 | F1=0.5000\n",
            "[epoch 38] train-loss=0.0049 | test-loss=0.4655 | train-acc=1.0000 | test-acc=0.8734 | P=0.7368 | R=0.2917 | F1=0.4179\n",
            "[epoch 39] train-loss=0.0047 | test-loss=0.4423 | train-acc=1.0000 | test-acc=0.8831 | P=0.7727 | R=0.3542 | F1=0.4857\n",
            "[epoch 40] train-loss=0.0045 | test-loss=0.4340 | train-acc=1.0000 | test-acc=0.8799 | P=0.7391 | R=0.3542 | F1=0.4789\n",
            "[epoch 41] train-loss=0.0043 | test-loss=0.4434 | train-acc=1.0000 | test-acc=0.8831 | P=0.7727 | R=0.3542 | F1=0.4857\n",
            "[epoch 42] train-loss=0.0041 | test-loss=0.4540 | train-acc=1.0000 | test-acc=0.8766 | P=0.7500 | R=0.3125 | F1=0.4412\n",
            "[epoch 43] train-loss=0.0040 | test-loss=0.4743 | train-acc=1.0000 | test-acc=0.8734 | P=0.7368 | R=0.2917 | F1=0.4179\n",
            "[epoch 44] train-loss=0.0038 | test-loss=0.4693 | train-acc=1.0000 | test-acc=0.8734 | P=0.7368 | R=0.2917 | F1=0.4179\n",
            "[epoch 45] train-loss=0.0037 | test-loss=0.4551 | train-acc=1.0000 | test-acc=0.8766 | P=0.7500 | R=0.3125 | F1=0.4412\n",
            "[epoch 46] train-loss=0.0035 | test-loss=0.4621 | train-acc=1.0000 | test-acc=0.8766 | P=0.7500 | R=0.3125 | F1=0.4412\n",
            "[epoch 47] train-loss=0.0034 | test-loss=0.4563 | train-acc=1.0000 | test-acc=0.8799 | P=0.7619 | R=0.3333 | F1=0.4638\n",
            "[epoch 48] train-loss=0.0033 | test-loss=0.4591 | train-acc=1.0000 | test-acc=0.8799 | P=0.7619 | R=0.3333 | F1=0.4638\n",
            "[epoch 49] train-loss=0.0032 | test-loss=0.4682 | train-acc=1.0000 | test-acc=0.8766 | P=0.7500 | R=0.3125 | F1=0.4412\n",
            "GO_3A0055085:\tpos = 250\tneg = 14515\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(\n",
        "    data={\n",
        "        'Annotation': ['GO_3A0005576', 'GO_3A0005739', 'GO_3A0007165', 'GO_3A0043066', 'GO_3A0055085'],\n",
        "        'Positive': [1408, 2328, 977, 65, 250],\n",
        "        'Negative': [13357, 12437, 13788, 14700, 14515]\n",
        "    }\n",
        ")\n",
        "\n",
        "df.to_csv('large_test_set.csv')"
      ],
      "metadata": {
        "id": "yyZs7v7kFXoY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-label prediction"
      ],
      "metadata": {
        "id": "1w0ZU-T_u2yd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from datetime import datetime\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import random\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "from functools import total_ordering\n",
        "def init_weights(layer):\n",
        "    if type(layer) == nn.Linear or type(layer) == nn.Conv1d:\n",
        "        nn.init.xavier_uniform_(layer.weight)"
      ],
      "metadata": {
        "id": "NpXTnCJ63_6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data multiple labels"
      ],
      "metadata": {
        "id": "i1r1FMrdxNj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_multiple_pos(seqfile: str, posfiles: list[str]) -> tuple[list, list]:\n",
        "    \"\"\"Read file with sequences and file with positive cases into lists.\n",
        "\n",
        "    :param seqfile: file with sequences\n",
        "    :type seqfile: str\n",
        "    :param posfiles: files with positive cases (annotated with function)\n",
        "    :type posfile: str\n",
        "    :return: two lists, first with sequences and second with integer labels\n",
        "    :rtype: tuple[list, list]\n",
        "    \"\"\"\n",
        "    idlist = []\n",
        "    datalist = []\n",
        "    labellist = []\n",
        "    with open(seqfile, 'r') as f:\n",
        "        for line in f.readlines():\n",
        "            line = line.rstrip().split('\\t')\n",
        "            idlist.append(line[0])\n",
        "            datalist.append(line[1])\n",
        "            labellist.append([0 for _ in posfiles])\n",
        "    for l, posfile in enumerate(posfiles):\n",
        "        with open(posfile, 'r') as f:\n",
        "            for line in f.readlines():\n",
        "                id = line.rstrip()\n",
        "                try:\n",
        "                    i = idlist.index(id)\n",
        "                    # if labellist[i] != 0:\n",
        "                    #     raise Exception(f'Sequence with multiple labels. {id = }, {labellist[i] = }, {l = }')\n",
        "                    labellist[i][l] = 1\n",
        "                except ValueError:\n",
        "                    continue\n",
        "    return datalist, labellist\n",
        "\n",
        "\n",
        "def split_labelled_multiple_pos(datalist: list, labellist: list):\n",
        "    pos_datalist = []\n",
        "    pos_labellist = []\n",
        "    neg_datalist = []\n",
        "    neg_labellist = []\n",
        "    for i, labels in enumerate(labellist):\n",
        "        is_pos = False\n",
        "        for label in labels:\n",
        "            if label:\n",
        "                is_pos = True\n",
        "        if is_pos:\n",
        "            pos_datalist.append(datalist[i])\n",
        "            pos_labellist.append(labels)\n",
        "        else:\n",
        "            neg_datalist.append(datalist[i])\n",
        "            neg_labellist.append(labels)\n",
        "    return pos_datalist, pos_labellist, neg_datalist, neg_labellist\n",
        "\n",
        "\n",
        "def zip_n_shuffle(list1: list, list2: list) -> tuple[list, list]:\n",
        "    assert len(list1) == len(list2)\n",
        "    combined = list(zip(list1, list2))\n",
        "    random.shuffle(combined)\n",
        "    list1, list2 = zip(*combined)\n",
        "    return list(list1), list(list2)\n",
        "\n",
        "\n",
        "def remove_sequences_multiple_pos(datalist: list, labellist, fraction=0.5):\n",
        "    datalist, labellist = zip_n_shuffle(datalist, labellist)\n",
        "    i = round(len(datalist) * fraction)\n",
        "    return datalist[:i], labellist[:i]\n",
        "\n",
        "\n",
        "def remove_sequences_equal_multiple_pos(reduced_datalist: list, reduced_labellist: list, compared_datalist: list):\n",
        "    reduced_datalist, reduced_labellist = zip_n_shuffle(reduced_datalist, reduced_labellist)\n",
        "    reduced_datalist = reduced_datalist[:len(compared_datalist)]\n",
        "    reduced_labellist = reduced_labellist[:len(compared_datalist)]\n",
        "    if len(compared_datalist) != len(reduced_datalist) or len(compared_datalist) != len(reduced_labellist):\n",
        "        raise ValueError\n",
        "    return reduced_datalist, reduced_labellist\n",
        "\n",
        "\n",
        "def fuse_sequence_lists_multiple_pos(pos_datalist: list, pos_labellist:list, neg_datalist: list, neg_labellist):\n",
        "    datalist = pos_datalist + neg_datalist\n",
        "    labellist = pos_labellist + neg_labellist\n",
        "    return datalist, labellist\n",
        "\n",
        "\n",
        "def calculate_pos_weights(labellist: list):\n",
        "    total_samples = len(labellist)\n",
        "    label_counts = torch.Tensor(labellist).sum(0)\n",
        "    pos_weights = (total_samples - label_counts) / (label_counts + 1e-5)\n",
        "    return pos_weights"
      ],
      "metadata": {
        "id": "g9TF3MfTr1o6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trainer multiple labels"
      ],
      "metadata": {
        "id": "o0TJEbhQxRLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "def dataset_stats(dl, ll):\n",
        "    len_list = []\n",
        "    for s in dl:\n",
        "        len_list.append(len(s))\n",
        "    sns.histplot(len_list)\n",
        "\n",
        "    p = 0\n",
        "    n = 0\n",
        "    for labels in ll:\n",
        "        found_pos = False\n",
        "        for l in labels:\n",
        "            if l:\n",
        "                p += 1\n",
        "                found_pos = True\n",
        "                break\n",
        "        if not found_pos:\n",
        "            n+=1\n",
        "    print(f'{p = }\\n{n = }')\n",
        "\n",
        "\n",
        "class TrainerMultipleClasses:\n",
        "    def __init__(self, model, loss_fn, optimizer, device):\n",
        "        self.model = model.to(device)\n",
        "        self.loss_fn = loss_fn\n",
        "        self.optimizer = optimizer\n",
        "        self.device = device\n",
        "        self.columns = [\n",
        "            'epoch',\n",
        "            'train_accuracy',\n",
        "            'train_precision',\n",
        "            'train_recall',\n",
        "            'train_fscore',\n",
        "            'train_loss',\n",
        "            'test_accuracy',\n",
        "            'test_precision',\n",
        "            'test_recall',\n",
        "            'test_fscore',\n",
        "            'test_loss'\n",
        "        ]\n",
        "        self.df = pd.DataFrame(columns=self.columns)\n",
        "\n",
        "\n",
        "    def _train_one_epoch(self, train_iter):\n",
        "        result_loss = 0\n",
        "        tpos = 0\n",
        "        fpos = 0\n",
        "        tneg = 0\n",
        "        fneg = 0\n",
        "        self.model.train(True)\n",
        "        for i, (inputs, labels) in enumerate(train_iter):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            outputs = self.model(inputs).to('cpu')\n",
        "            labels = labels.type(torch.float32)\n",
        "\n",
        "            loss = loss_fn(input=outputs, target=labels)\n",
        "            result_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            self.optimizer.step()\n",
        "            for b, lab in enumerate(labels):\n",
        "                out = torch.round(torch.sigmoid(outputs[b]))\n",
        "\n",
        "                for j, o in enumerate(out):\n",
        "                    # print(f'{o=}\\t{l=}')\n",
        "                    l = lab[j]\n",
        "                    if o == 1 and l == 1:\n",
        "                        tpos += 1\n",
        "                    elif o == 1 and l == 0:\n",
        "                        fpos += 1\n",
        "                    elif o == 0 and l == 0:\n",
        "                        tneg += 1\n",
        "                    elif o == 0 and l == 1:\n",
        "                        fneg += 1\n",
        "                    else:\n",
        "                        raise ValueError(f'Output or label is not rounded to zero: {o = } {l = }')\n",
        "                    # print(f'{tpos=}\\t{fpos=}\\t{tneg=}\\t{fneg=}')\n",
        "        accuracy = (tpos + tneg) / (tpos + fpos + tneg + fneg)\n",
        "        precision = tpos / (tpos + fpos) if (tpos + fpos) > 0 else 0\n",
        "        recall = tpos / (tpos + fneg) if (tpos + fneg) > 0 else 0\n",
        "        fscore = 2 * tpos / (2*tpos + fpos + fneg) if (2*tpos + fpos + fneg) > 0 else 0\n",
        "        return accuracy, precision, recall, fscore, result_loss / (i+1)\n",
        "\n",
        "\n",
        "    def _test_one_epoch(self, test_iter):\n",
        "        result_loss = 0\n",
        "        tpos = 0\n",
        "        fpos = 0\n",
        "        tneg = 0\n",
        "        fneg = 0\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, (inputs, labels) in enumerate(test_iter):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels\n",
        "                outputs = self.model(inputs).to('cpu')\n",
        "                labels = labels.type(torch.float32)\n",
        "                loss = loss_fn(input=outputs, target=labels)\n",
        "                result_loss += loss.item()\n",
        "                for b, lab in enumerate(labels):\n",
        "                    out = torch.round(torch.sigmoid(outputs[b]))\n",
        "                    for j, o in enumerate(out):\n",
        "                        l = lab[j]\n",
        "                        if o == 1 and l == 1:\n",
        "                            tpos += 1\n",
        "                        elif o == 1 and l == 0:\n",
        "                            fpos += 1\n",
        "                        elif o == 0 and l == 0:\n",
        "                            tneg += 1\n",
        "                        elif o == 0 and l == 1:\n",
        "                            fneg += 1\n",
        "                        else:\n",
        "                            raise ValueError(f'Output or label is not rounded to zero: {o = } {l = }')\n",
        "        # print(f'{tpos=}\\t{fpos=}\\t{tneg=}\\t{fneg=}')\n",
        "        accuracy = (tpos + tneg) / (tpos + fpos + tneg + fneg)\n",
        "        precision = tpos / (tpos + fpos) if (tpos + fpos) > 0 else 0\n",
        "        recall = tpos / (tpos + fneg) if (tpos + fneg) > 0 else 0\n",
        "        fscore = 2 * tpos / (2*tpos + fpos + fneg) if (2*tpos + fpos + fneg) > 0 else 0\n",
        "        return accuracy, precision, recall, fscore, result_loss / (i+1)\n",
        "\n",
        "\n",
        "    def _load_into_dict(self, epoch, train_stats, test_stats):\n",
        "        row = [epoch] + list(train_stats) + list(test_stats)\n",
        "        row = pd.DataFrame(row, index=self.columns).T\n",
        "        self.df = pd.concat([self.df, row], axis=0)\n",
        "\n",
        "\n",
        "    def train(self, epochs, train_iter, test_iter):\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            train_stats = self._train_one_epoch(train_iter)\n",
        "\n",
        "            test_stats = self._test_one_epoch(test_iter)\n",
        "            self._load_into_dict(epoch, train_stats, test_stats)\n",
        "            print(f\"[epoch {epoch:02d}] \"\n",
        "                  f\"train-loss={train_stats[-1]:.4f} | \"\n",
        "                  f\"test-loss={test_stats[-1]:.4f} | \"\n",
        "                  f\"train-acc={train_stats[0]:.4f} | \"\n",
        "                  f\"test-acc={test_stats[0]:.4f} | \"\n",
        "                  f\"P={test_stats[1]:.4f} | R={test_stats[2]:.4f} | F1={test_stats[3]:.4f}\")\n",
        "\n",
        "        return self.df"
      ],
      "metadata": {
        "id": "7xssSozK0uO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model multiple labels"
      ],
      "metadata": {
        "id": "xsJ8Xb1zxVYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiClassifierCNN1D(nn.Module):\n",
        "    \"\"\"1D Convolutional Neural Network for protein function classification\"\"\"\n",
        "    def __init__(self, num_classes: int, vocab_size: int = 21,  conv_channels: int = 128, use_bias: bool = False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "        # CNN model for binary classification\n",
        "        self.conv1 = nn.Sequential(\n",
        "            # conv block 1\n",
        "            nn.Conv1d(in_channels=vocab_size, out_channels=conv_channels, kernel_size=3, padding='same', bias=use_bias),\n",
        "            nn.BatchNorm1d(conv_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
        "            nn.Dropout(0)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            # conv block 2\n",
        "            nn.Conv1d(in_channels=conv_channels, out_channels=conv_channels, kernel_size=3, padding='same', bias=use_bias),\n",
        "            nn.BatchNorm1d(conv_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(1),\n",
        "            nn.Dropout(0)\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            # conv block 3\n",
        "            nn.Conv1d(in_channels=conv_channels, out_channels=conv_channels, kernel_size=3, padding='same', bias=use_bias),\n",
        "            nn.BatchNorm1d(conv_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveMaxPool1d(1)\n",
        "        )\n",
        "        self.fc1 = nn.Sequential(\n",
        "            # flatten + classification head\n",
        "            nn.Flatten(1, -1),\n",
        "            nn.LazyLinear(out_features=64, bias=use_bias),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0)\n",
        "        )\n",
        "        self.fc2 = nn.Sequential(\n",
        "            # flatten + classification head\n",
        "            nn.LazyLinear(out_features=num_classes, bias=use_bias)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.tensor, targets: torch.tensor = None):\n",
        "        \"\"\"Predict protein function class (0 or 1)\"\"\"\n",
        "        x = self.embedding(x).permute(0,2,1)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.fc1(x)\n",
        "        return self.fc2(x)\n"
      ],
      "metadata": {
        "id": "DvXk1P5yzsRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load GO data multiple labels"
      ],
      "metadata": {
        "id": "5H-PP8bUxpCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "annot_files = [\n",
        "    \"GO_3A0005576.annotprot\",\n",
        "    \"GO_3A0005739.annotprot\",\n",
        "    \"GO_3A0007165.annotprot\",\n",
        "    \"GO_3A0043066.annotprot\",\n",
        "    \"GO_3A0055085.annotprot\"\n",
        "]\n",
        "\n",
        "batch_size = 10\n",
        "num_steps = 1000\n",
        "\n",
        "dl, ll = read_multiple_pos(\"expr5Tseq_filtGO_100-1000.lis\", annot_files)\n",
        "\n",
        "p_dl, p_ll, n_dl, n_ll = split_labelled_multiple_pos(dl, ll)\n",
        "n_dl, n_ll = remove_sequences_multiple_pos(n_dl, n_ll, 0.1)\n",
        "dl, ll = fuse_sequence_lists_multiple_pos(p_dl, p_ll, n_dl, n_ll)\n",
        "\n",
        "train_dl, train_ll, test_dl, test_ll = generate_train_test(dl, ll, 0.6)\n",
        "train_ds = [train_dl, train_ll]\n",
        "test_ds = [test_dl, test_ll]\n",
        "\n",
        "traindataloader = load_data(batch_size, num_steps, train_ds)\n",
        "testdataloader = load_data(batch_size, num_steps, test_ds)\n",
        "\n",
        "pos_weights = calculate_pos_weights(train_ll)\n",
        "\n",
        "print(next(iter(traindataloader)))"
      ],
      "metadata": {
        "id": "CId5XjpFtW82",
        "outputId": "8e80adb5-3f9b-4b9e-9199-8d4f9c2a9be6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[10,  0, 14,  ..., 20, 20, 20],\n",
            "        [10, 19, 14,  ..., 20, 20, 20],\n",
            "        [10,  0,  6,  ..., 20, 20, 20],\n",
            "        ...,\n",
            "        [10, 17,  6,  ..., 20, 20, 20],\n",
            "        [10, 17,  6,  ..., 20, 20, 20],\n",
            "        [10,  3,  8,  ..., 20, 20, 20]]), tensor([[1, 0, 0, 0, 0],\n",
            "        [0, 1, 0, 0, 0],\n",
            "        [0, 1, 0, 0, 0],\n",
            "        [1, 0, 1, 0, 0],\n",
            "        [0, 0, 1, 0, 0],\n",
            "        [0, 0, 1, 1, 0],\n",
            "        [0, 0, 0, 0, 0],\n",
            "        [1, 0, 0, 0, 0],\n",
            "        [0, 1, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len_list = []\n",
        "for s in dl:\n",
        "    len_list.append(len(s))\n",
        "sns.histplot(len_list)\n",
        "\n",
        "p = 0\n",
        "n = 0\n",
        "for labels in ll:\n",
        "    found_pos = False\n",
        "    for l in labels:\n",
        "        if l:\n",
        "            p += 1\n",
        "            found_pos = True\n",
        "            break\n",
        "    if not found_pos:\n",
        "        n+=1\n",
        "print(f'{p = }\\n{n = }')"
      ],
      "metadata": {
        "id": "JrwN-K7Syzod",
        "outputId": "45c03b8a-9efd-4b65-a98b-6355f74bac2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p = 1454\n",
            "n = 533\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train model multiple labels"
      ],
      "metadata": {
        "id": "bcZoEI7-xuop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultiClassifierCNN1D(num_classes=5, conv_channels=256, use_bias=True)\n",
        "model.apply(init_weights)\n",
        "loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9)\n",
        "model"
      ],
      "metadata": {
        "id": "towK5DH_147q",
        "outputId": "522fb760-6dc9-401e-8bb6-f15194d6fe1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultiClassifierCNN1D(\n",
              "  (embedding): Embedding(21, 21)\n",
              "  (conv1): Sequential(\n",
              "    (0): Conv1d(21, 256, kernel_size=(3,), stride=(1,), padding=same)\n",
              "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (4): Dropout(p=0, inplace=False)\n",
              "  )\n",
              "  (conv2): Sequential(\n",
              "    (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=same)\n",
              "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
              "    (4): Dropout(p=0, inplace=False)\n",
              "  )\n",
              "  (conv3): Sequential(\n",
              "    (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=same)\n",
              "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): AdaptiveMaxPool1d(output_size=1)\n",
              "  )\n",
              "  (fc1): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): LazyLinear(in_features=0, out_features=64, bias=True)\n",
              "    (2): ReLU()\n",
              "    (3): Dropout(p=0, inplace=False)\n",
              "  )\n",
              "  (fc2): Sequential(\n",
              "    (0): LazyLinear(in_features=0, out_features=5, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = TrainerMultipleClasses(model, loss_fn, optimizer, device)\n",
        "df = trainer.train(epochs=50, train_iter=traindataloader, test_iter=testdataloader)"
      ],
      "metadata": {
        "id": "xv-k3v3e2Afo",
        "outputId": "04d5e80a-4bb9-490c-8796-3a3b0b878db6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py:823: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:180.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "<ipython-input-6-1a27aa51fb67>:112: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  self.df = pd.concat([self.df, row], axis=0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 00] train-loss=1.1920 | test-loss=1.1838 | train-acc=0.4854 | test-acc=0.4060 | P=0.1438 | R=0.5181 | F1=0.2251\n",
            "[epoch 01] train-loss=1.1665 | test-loss=1.1852 | train-acc=0.4055 | test-acc=0.4060 | P=0.1438 | R=0.5181 | F1=0.2251\n",
            "[epoch 02] train-loss=1.1685 | test-loss=1.1838 | train-acc=0.4878 | test-acc=0.4060 | P=0.1438 | R=0.5181 | F1=0.2251\n",
            "[epoch 03] train-loss=1.1643 | test-loss=1.1831 | train-acc=0.4173 | test-acc=0.5572 | P=0.1547 | R=0.3716 | F1=0.2185\n",
            "[epoch 04] train-loss=1.1660 | test-loss=1.1844 | train-acc=0.5163 | test-acc=0.4196 | P=0.1551 | R=0.5589 | F1=0.2429\n",
            "[epoch 05] train-loss=1.1647 | test-loss=1.1842 | train-acc=0.4210 | test-acc=0.5572 | P=0.1547 | R=0.3716 | F1=0.2185\n",
            "[epoch 06] train-loss=1.1691 | test-loss=1.1840 | train-acc=0.4653 | test-acc=0.4553 | P=0.1849 | R=0.6662 | F1=0.2895\n",
            "[epoch 07] train-loss=1.1669 | test-loss=1.1838 | train-acc=0.5508 | test-acc=0.4181 | P=0.1539 | R=0.5544 | F1=0.2409\n",
            "[epoch 08] train-loss=1.1670 | test-loss=1.1853 | train-acc=0.4257 | test-acc=0.3824 | P=0.1241 | R=0.4471 | F1=0.1943\n",
            "[epoch 09] train-loss=1.1673 | test-loss=1.1857 | train-acc=0.4656 | test-acc=0.4181 | P=0.1539 | R=0.5544 | F1=0.2409\n",
            "[epoch 10] train-loss=1.1700 | test-loss=1.1847 | train-acc=0.4508 | test-acc=0.5572 | P=0.1547 | R=0.3716 | F1=0.2185\n",
            "[epoch 11] train-loss=1.1634 | test-loss=1.1858 | train-acc=0.3804 | test-acc=0.4060 | P=0.1438 | R=0.5181 | F1=0.2251\n",
            "[epoch 12] train-loss=1.1653 | test-loss=1.1822 | train-acc=0.3626 | test-acc=0.4060 | P=0.1438 | R=0.5181 | F1=0.2251\n",
            "[epoch 13] train-loss=1.1694 | test-loss=1.1830 | train-acc=0.4753 | test-acc=0.6176 | P=0.2302 | R=0.5529 | F1=0.3250\n",
            "[epoch 14] train-loss=1.1632 | test-loss=1.1831 | train-acc=0.5515 | test-acc=0.3177 | P=0.1777 | R=0.8535 | F1=0.2941\n",
            "[epoch 15] train-loss=1.1620 | test-loss=1.1845 | train-acc=0.4270 | test-acc=0.5940 | P=0.2006 | R=0.4819 | F1=0.2833\n",
            "[epoch 16] train-loss=1.1666 | test-loss=1.1836 | train-acc=0.6908 | test-acc=0.5940 | P=0.2006 | R=0.4819 | F1=0.2833\n",
            "[epoch 17] train-loss=1.1643 | test-loss=1.1831 | train-acc=0.5297 | test-acc=0.5804 | P=0.1836 | R=0.4411 | F1=0.2593\n",
            "[epoch 18] train-loss=1.1634 | test-loss=1.1851 | train-acc=0.3639 | test-acc=0.6823 | P=0.1220 | R=0.1465 | F1=0.1332\n",
            "[epoch 19] train-loss=1.1644 | test-loss=1.1848 | train-acc=0.5763 | test-acc=0.5683 | P=0.1686 | R=0.4048 | F1=0.2380\n",
            "[epoch 20] train-loss=1.1643 | test-loss=1.1829 | train-acc=0.4807 | test-acc=0.4307 | P=0.1644 | R=0.5921 | F1=0.2573\n",
            "[epoch 21] train-loss=1.1654 | test-loss=1.1822 | train-acc=0.5190 | test-acc=0.4800 | P=0.2055 | R=0.7402 | F1=0.3216\n",
            "[epoch 22] train-loss=1.1630 | test-loss=1.1844 | train-acc=0.5545 | test-acc=0.4800 | P=0.2055 | R=0.7402 | F1=0.3216\n",
            "[epoch 23] train-loss=1.1656 | test-loss=1.1834 | train-acc=0.5096 | test-acc=0.4553 | P=0.1849 | R=0.6662 | F1=0.2895\n",
            "[epoch 24] train-loss=1.1675 | test-loss=1.1859 | train-acc=0.4351 | test-acc=0.4196 | P=0.1551 | R=0.5589 | F1=0.2429\n",
            "[epoch 25] train-loss=1.1653 | test-loss=1.1839 | train-acc=0.5086 | test-acc=0.5336 | P=0.1252 | R=0.3006 | F1=0.1767\n",
            "[epoch 26] train-loss=1.1648 | test-loss=1.1820 | train-acc=0.4163 | test-acc=0.4428 | P=0.1744 | R=0.6284 | F1=0.2731\n",
            "[epoch 27] train-loss=1.1659 | test-loss=1.1844 | train-acc=0.5126 | test-acc=0.4196 | P=0.1551 | R=0.5589 | F1=0.2429\n",
            "[epoch 28] train-loss=1.1675 | test-loss=1.1859 | train-acc=0.4012 | test-acc=0.4060 | P=0.1438 | R=0.5181 | F1=0.2251\n",
            "[epoch 29] train-loss=1.1683 | test-loss=1.1836 | train-acc=0.4015 | test-acc=0.4307 | P=0.1644 | R=0.5921 | F1=0.2573\n",
            "[epoch 30] train-loss=1.1654 | test-loss=1.1851 | train-acc=0.3844 | test-acc=0.3177 | P=0.1777 | R=0.8535 | F1=0.2941\n",
            "[epoch 31] train-loss=1.1632 | test-loss=1.1826 | train-acc=0.5549 | test-acc=0.5200 | P=0.1082 | R=0.2598 | F1=0.1528\n",
            "[epoch 32] train-loss=1.1647 | test-loss=1.1854 | train-acc=0.6173 | test-acc=0.5200 | P=0.1082 | R=0.2598 | F1=0.1528\n",
            "[epoch 33] train-loss=1.1635 | test-loss=1.1841 | train-acc=0.4287 | test-acc=0.5572 | P=0.1547 | R=0.3716 | F1=0.2185\n",
            "[epoch 34] train-loss=1.1675 | test-loss=1.1847 | train-acc=0.5622 | test-acc=0.5683 | P=0.1686 | R=0.4048 | F1=0.2380\n",
            "[epoch 35] train-loss=1.1633 | test-loss=1.1830 | train-acc=0.3263 | test-acc=0.4307 | P=0.1644 | R=0.5921 | F1=0.2573\n",
            "[epoch 36] train-loss=1.1660 | test-loss=1.1830 | train-acc=0.4216 | test-acc=0.5447 | P=0.1390 | R=0.3338 | F1=0.1963\n",
            "[epoch 37] train-loss=1.1675 | test-loss=1.1845 | train-acc=0.5421 | test-acc=0.6712 | P=0.0943 | R=0.1133 | F1=0.1030\n",
            "[epoch 38] train-loss=1.1661 | test-loss=1.1819 | train-acc=0.4904 | test-acc=0.2684 | P=0.1469 | R=0.7054 | F1=0.2431\n",
            "[epoch 39] train-loss=1.1667 | test-loss=1.1860 | train-acc=0.4458 | test-acc=0.5572 | P=0.1547 | R=0.3716 | F1=0.2185\n",
            "[epoch 40] train-loss=1.1637 | test-loss=1.1818 | train-acc=0.4458 | test-acc=0.5447 | P=0.1390 | R=0.3338 | F1=0.1963\n",
            "[epoch 41] train-loss=1.1660 | test-loss=1.1861 | train-acc=0.3334 | test-acc=0.5447 | P=0.1390 | R=0.3338 | F1=0.1963\n",
            "[epoch 42] train-loss=1.1658 | test-loss=1.1855 | train-acc=0.4320 | test-acc=0.4181 | P=0.1539 | R=0.5544 | F1=0.2409\n",
            "[epoch 43] train-loss=1.1658 | test-loss=1.1833 | train-acc=0.5450 | test-acc=0.5203 | P=0.1082 | R=0.2598 | F1=0.1528\n",
            "[epoch 44] train-loss=1.1646 | test-loss=1.1839 | train-acc=0.4364 | test-acc=0.3824 | P=0.1241 | R=0.4471 | F1=0.1943\n",
            "[epoch 45] train-loss=1.1632 | test-loss=1.1839 | train-acc=0.5015 | test-acc=0.5200 | P=0.1082 | R=0.2598 | F1=0.1528\n",
            "[epoch 46] train-loss=1.1642 | test-loss=1.1840 | train-acc=0.6495 | test-acc=0.4553 | P=0.1849 | R=0.6662 | F1=0.2895\n",
            "[epoch 47] train-loss=1.1653 | test-loss=1.1843 | train-acc=0.4831 | test-acc=0.7316 | P=0.2453 | R=0.2946 | F1=0.2677\n",
            "[epoch 48] train-loss=1.1641 | test-loss=1.1841 | train-acc=0.5146 | test-acc=0.5940 | P=0.2006 | R=0.4819 | F1=0.2833\n",
            "[epoch 49] train-loss=1.1633 | test-loss=1.1835 | train-acc=0.6059 | test-acc=0.5693 | P=0.1698 | R=0.4079 | F1=0.2398\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('multi-label_classifier.csv')"
      ],
      "metadata": {
        "id": "RiussApswGYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Figures"
      ],
      "metadata": {
        "id": "uOilbtffBAiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def reformat_df(df):\n",
        "    df = df.drop(\n",
        "        columns=['train_accuracy','train_precision','train_recall','train_fscore']\n",
        "    )\n",
        "    df = pd.melt(\n",
        "        df,\n",
        "        id_vars=['epoch'],\n",
        "        value_vars=['train_loss','test_accuracy','test_precision','test_recall','test_fscore','test_loss']\n",
        "    )\n",
        "    df.rename(columns={'epoch': 'Epoch', 'variable': 'Variable', 'value': 'Value'}, inplace=True)\n",
        "    # df = df.replace('train_accuracy', 'Train Accuracy')\n",
        "    # df = df.replace('train_precision', 'Train Precision')\n",
        "    # df = df.replace('train_recall', 'Train Recall')\n",
        "    # df = df.replace('train_fscore', 'Train F-score')\n",
        "    df = df.replace('train_loss', 'Train Loss')\n",
        "    df = df.replace('test_accuracy', 'Test Accuracy')\n",
        "    df = df.replace('test_precision', 'Test Precision')\n",
        "    df = df.replace('test_recall', 'Test Recall')\n",
        "    df = df.replace('test_fscore', 'Test F-score')\n",
        "    df = df.replace('test_loss', 'Test Loss')\n",
        "    return df\n",
        "\n",
        "\n",
        "def reformat_df_hyper(df):\n",
        "    df = df.drop(\n",
        "        columns=['train_accuracy','train_precision','train_recall','train_fscore']\n",
        "    )\n",
        "    df = pd.melt(\n",
        "        df,\n",
        "        id_vars=['epoch', 'conv_channels', 'dropout_rate', 'lr', 'momentum'],\n",
        "        value_vars=['train_loss','test_accuracy','test_precision','test_recall','test_fscore','test_loss']\n",
        "    )\n",
        "    df.rename(columns={'epoch': 'Epoch', 'variable': 'Variable', 'value': 'Value'}, inplace=True)\n",
        "    # df = df.replace('train_accuracy', 'Train Accuracy')\n",
        "    # df = df.replace('train_precision', 'Train Precision')\n",
        "    # df = df.replace('train_recall', 'Train Recall')\n",
        "    # df = df.replace('train_fscore', 'Train F-score')\n",
        "    df = df.replace('train_loss', 'Train Loss')\n",
        "    df = df.replace('test_accuracy', 'Test Accuracy')\n",
        "    df = df.replace('test_precision', 'Test Precision')\n",
        "    df = df.replace('test_recall', 'Test Recall')\n",
        "    df = df.replace('test_fscore', 'Test F-score')\n",
        "    df = df.replace('test_loss', 'Test Loss')\n",
        "    return df\n",
        "\n",
        "\n",
        "def plot_single_model(df, **kwargs):\n",
        "    ax = sns.lineplot(df, x='Epoch', y='Value', hue='Variable')\n",
        "    sns.move_legend(\n",
        "        obj=ax,\n",
        "        loc='upper left',\n",
        "        bbox_to_anchor=(1, 1)\n",
        "    )\n",
        "    if 'filepath' in kwargs.keys():\n",
        "        plt.savefig(kwargs['filepath'], format='svg', bbox_inches='tight')\n",
        "        plt.clf()\n",
        "    else:\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "GqAqH6ylA_w-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('hyperparameter_grid_len200_500_n5000nr1.csv')\n",
        "df = reformat_df_hyper(df)\n",
        "\n",
        "groups = df.groupby(['conv_channels', 'dropout_rate', 'lr', 'momentum'])\n",
        "\n",
        "split_dfs = {name: group for name, group in groups}\n",
        "\n",
        "for key in split_dfs.keys():\n",
        "    sub_df = split_dfs[key]\n",
        "    path = f'hyper_conv_channels_{key[0]}_dropout_{key[1]}_lr_{key[2]}_momentum_{key[3]}'.replace('.', '_') + '.svg'\n",
        "    plot_single_model(sub_df, filepath=path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DaYwNE_JJ8F9",
        "outputId": "8b2923e7-b8b0-4bfc-82f6-c04c9eaaf4ad"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sim_csv = [\n",
        "    'half_neg_len200_500_n5000nr1',\n",
        "    'half_pos_len200_500_n5000nr1'\n",
        "]\n",
        "\n",
        "for sim in sim_csv:\n",
        "    df = pd.read_csv(f'{sim}.csv')\n",
        "    df = reformat_df(df)\n",
        "    plot_single_model(df, filepath=f'{sim}.svg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0sro9icJHXQt",
        "outputId": "ebcba2a3-3e6d-49c3-b35f-04669a29033d"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "go_csv = [\n",
        "    \"GO_3A0005576\",\n",
        "    \"GO_3A0005739\",\n",
        "    \"GO_3A0007165\",\n",
        "    \"GO_3A0043066\",\n",
        "    \"GO_3A0055085\"\n",
        "]\n",
        "\n",
        "\n",
        "for go in go_csv:\n",
        "    df = pd.read_csv(f'{go}.csv')\n",
        "    df = reformat_df(df)\n",
        "    plot_single_model(df, filepath=f'{go}.svg')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lf0fZG0CF_gQ",
        "outputId": "45bbdfbc-9024-4b37-ae15-a3f4eaded477"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "other_go_csv = [\n",
        "    \"GO_3A0005576_full_set\",\n",
        "    \"GO_3A0005576_half_pos\",\n",
        "    \"GO_3A0005576_half_neg\"\n",
        "]\n",
        "\n",
        "for go in other_go_csv:\n",
        "    df = pd.read_csv(f'{go}.csv')\n",
        "    df = reformat_df(df)\n",
        "    plot_single_model(df, filepath=f'{go}.svg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xfA-gz2b-2OV",
        "outputId": "502ad817-72b8-4a37-c794-a5b975e45169"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('GO_3A0005576_full_set.csv')\n",
        "df = reformat_df(df)\n",
        "plot_single_model(df, filepath='GO_3A0005576_full_set.svg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7F5zlDwEJxeb",
        "outputId": "8ef584d1-2701-4381-93f5-474957462018"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_large_test_set(df, **kwargs):\n",
        "    df = df.melt(id_vars=['Annotation'], value_vars=['Positive', 'Negative'], var_name='Label', value_name='Amount')\n",
        "    ax = sns.barplot(data=df, x='Annotation', y='Amount', hue='Label')\n",
        "    plt.xticks(rotation=70)\n",
        "    sns.move_legend(\n",
        "        obj=ax,\n",
        "        loc='upper left',\n",
        "        bbox_to_anchor=(1, 1)\n",
        "    )\n",
        "    if 'filepath' in kwargs.keys():\n",
        "        plt.savefig(kwargs['filepath'], format='svg', bbox_inches='tight')\n",
        "        plt.clf()\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "df = pd.read_csv('large_test_set.csv')\n",
        "plot_large_test_set(df, filepath='large_test_set_barplot.svg')\n"
      ],
      "metadata": {
        "id": "OyzgUltJHsxO",
        "outputId": "150c981e-12f5-456a-9727-ed3d57217cf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}