{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QrywNfRlazm-"
      },
      "outputs": [],
      "source": [
        "! git clone https://git.wur.nl/bioinformatics/grs34806-deep-learning-project-data.git -q\n",
        "! git clone https://github.com/maussn/GRS34806-project.git -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from datetime import datetime\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import random\n",
        "import seaborn as sns\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "from functools import total_ordering\n",
        "def init_weights(layer):\n",
        "    if type(layer) == nn.Linear or type(layer) == nn.Conv1d:\n",
        "        nn.init.xavier_uniform_(layer.weight)\n"
      ],
      "metadata": {
        "id": "3ctb6CE_bWXP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(Path('grs34806-deep-learning-project-data'))"
      ],
      "metadata": {
        "id": "SEpH6j4dbJuO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data loading"
      ],
      "metadata": {
        "id": "DzPLAugMwzCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read(seqfile: str, posfile: str) -> tuple[list, list]:\n",
        "    \"\"\"Read file with sequences and file with positive cases into lists.\n",
        "\n",
        "    :param seqfile: file with sequences\n",
        "    :type seqfile: str\n",
        "    :param posfile: file with positive cases (annotated with function)\n",
        "    :type posfile: str\n",
        "    :return: two lists, first with sequences and second with boolean labels\n",
        "    :rtype: tuple[list, list]\n",
        "    \"\"\"\n",
        "    idlist = []\n",
        "    datalist = []\n",
        "    labellist = []\n",
        "    with open(seqfile, 'r') as f:\n",
        "        for line in f.readlines():\n",
        "            line = line.rstrip().split('\\t')\n",
        "            idlist.append(line[0])\n",
        "            datalist.append(line[1])\n",
        "            labellist.append(False)\n",
        "    with open(posfile, 'r') as f:\n",
        "        for line in f.readlines():\n",
        "            id = line.rstrip()\n",
        "            try:\n",
        "                i = idlist.index(id)\n",
        "                labellist[i] = True\n",
        "            except ValueError:\n",
        "                continue\n",
        "    return datalist, labellist\n",
        "\n",
        "\n",
        "def split_labelled(datalist: list, labellist: list):\n",
        "    pos_datalist = []\n",
        "    neg_datalist = []\n",
        "    for i, label in enumerate(labellist):\n",
        "        if label:\n",
        "            pos_datalist.append(datalist[i])\n",
        "        else:\n",
        "            neg_datalist.append(datalist[i])\n",
        "    return pos_datalist, neg_datalist\n",
        "\n",
        "\n",
        "def remove_sequences(datalist: list, fraction=0.5):\n",
        "    random.shuffle(datalist)\n",
        "    i = round(len(datalist) * fraction)\n",
        "    return datalist[:i]\n",
        "\n",
        "\n",
        "def remove_sequences_equal(reduced_datalist: list, compared_datalist: list):\n",
        "    random.shuffle(reduced_datalist)\n",
        "    random.shuffle(compared_datalist)\n",
        "    reduced_datalist = reduced_datalist[:len(compared_datalist)]\n",
        "    if len(compared_datalist) != len(reduced_datalist):\n",
        "        raise ValueError\n",
        "    return reduced_datalist\n",
        "\n",
        "\n",
        "def fuse_sequence_lists(pos_datalist: list, neg_datalist: list):\n",
        "    pos_labels = [True for _ in pos_datalist]\n",
        "    neg_labels = [False for _ in neg_datalist]\n",
        "    datalist = pos_datalist + neg_datalist\n",
        "    labellist = pos_labels + neg_labels\n",
        "    return datalist, labellist\n",
        "\n",
        "\n",
        "def generate_train_test(datalist: list, labellist: list, fraction: float=0.8):\n",
        "    \"\"\"Split up dataset in training set and test set\n",
        "\n",
        "    :param datalist: list with sequences\n",
        "    :type datalist: list\n",
        "    :param labellist: list with labels\n",
        "    :type labellist: list\n",
        "    :param ratio: fraction to be added to the training set, remainder is added to the test set, defaults to 0.8\n",
        "    :type ratio: float, optional\n",
        "    :return: four lists, first two the training data and labels, second two the test data and labels\n",
        "    :rtype: tuple[list, list, list, list]\n",
        "    \"\"\"\n",
        "    c = list(zip(datalist, labellist))\n",
        "    random.shuffle(c)\n",
        "    datalist[:], labellist[:] = zip(*c)\n",
        "    i = round(len(datalist) * fraction)\n",
        "    traindatalist = datalist[:i]\n",
        "    trainlabellist = labellist[:i]\n",
        "    testdatalist = datalist[i:]\n",
        "    testlabellist = labellist[i:]\n",
        "    return traindatalist, trainlabellist,testdatalist,testlabellist\n",
        "\n",
        "\n",
        "def tokenize(data: list, map2num: dict, non_aa_num: int=20) -> list:\n",
        "    \"\"\"Tokenize all sequences in a list\n",
        "\n",
        "    :param data: list of sequences to tokenize\n",
        "    :type data: list\n",
        "    :param map2num: ammino acid -> integer token mapping\n",
        "    :type map2num: dict\n",
        "    :param non_aa_num: token for non amino acid characters, defaults to 20\n",
        "    :type non_aa_num: int, optional\n",
        "    :return: list of tokenized sequences\n",
        "    :rtype: list\n",
        "    \"\"\"\n",
        "    seq = []\n",
        "    for count, i in enumerate(data):\n",
        "        seq.append([map2num.get(j,non_aa_num) for j in list(i)])\n",
        "    return seq\n",
        "\n",
        "\n",
        "def truncate_pad(line: list, num_steps: int, padding_token: int) -> list:\n",
        "    \"\"\"Truncate or pad a tokenized sequence\n",
        "\n",
        "    :param line: tokenized sequence\n",
        "    :type line: list\n",
        "    :param num_steps: maximum sequence length\n",
        "    :type num_steps: int\n",
        "    :param padding_token: token to be used for padding\n",
        "    :type padding_token: int\n",
        "    :return: truncated/padded sequence\n",
        "    :rtype: list\n",
        "    \"\"\"\n",
        "    if len(line) > num_steps:\n",
        "        return line[:num_steps] # Truncate\n",
        "    return line + [padding_token] * (num_steps - len(line)) # Pad\n",
        "\n",
        "\n",
        "def build_seq_array(lines: list, num_steps: int, non_aa_num: int=20) -> torch.tensor:\n",
        "    \"\"\"Truncate or pad tokenized sequences and convert to tensor\n",
        "\n",
        "    :param lines: tokenized sequences\n",
        "    :type lines: list\n",
        "    :param num_steps: maximum sequence length\n",
        "    :type num_steps: int\n",
        "    :param non_aa_num: token for padding, defaults to 20\n",
        "    :type non_aa_num: int, optional\n",
        "    :return: tensor with truncated/padded tokenized sequences\n",
        "    :rtype: torch.tensor\n",
        "    \"\"\"\n",
        "    return torch.tensor([truncate_pad(l, num_steps, non_aa_num) for l in lines], dtype=torch.long)\n",
        "\n",
        "\n",
        "def load_array(data_arrays: tuple[torch.tensor, torch.tensor], batch_size: int, is_train: bool=True) -> torch.utils.data.DataLoader:\n",
        "    \"\"\"Construct a PyTorch data iterator.\n",
        "\n",
        "    Taken from d2l package\"\"\"\n",
        "    dataset = torch.utils.data.TensorDataset(*data_arrays)\n",
        "    return torch.utils.data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
        "\n",
        "\n",
        "def load_data(batch_size: int, num_steps: int, dataset: tuple[list, list]) -> torch.utils.data.DataLoader:\n",
        "    \"\"\"Tokenize sequence/label dataset and load into dataloader.\n",
        "\n",
        "    :param batch_size: size of each batch\n",
        "    :type batch_size: int\n",
        "    :param num_steps: maximum sequence length\n",
        "    :type num_steps: int\n",
        "    :param dataset: first list contains sequences, second labels\n",
        "    :type dataset: tuple[list, list]\n",
        "    :return: torch dataloader which gives a tensor of sequences in a batch and a tensor with their labels\n",
        "    :rtype: torch.utils.data.DataLoader\n",
        "    \"\"\"\n",
        "    mapaa2num = {aa: i for (i, aa) in enumerate(list(\"ACDEFGHIKLMNPQRSTVWY\"))}\n",
        "    seq,lab = dataset\n",
        "    seq = tokenize(seq, mapaa2num)\n",
        "    seq_array = build_seq_array(seq, num_steps)\n",
        "    data_arrays = (seq_array, torch.tensor(lab, dtype=torch.long))\n",
        "    data_iter = load_array(data_arrays, batch_size)\n",
        "    return data_iter"
      ],
      "metadata": {
        "id": "eHpedW0hqx1V"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load paired data"
      ],
      "metadata": {
        "id": "iFc0lo8vw291"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "num_steps = 1000\n",
        "\n",
        "# Example for one of the simulated datasets\n",
        "datalist, labellist = read(\"expr5Tseq_filtGO_100-1000.lis\", \"GO_3A0055085.annotprot\")\n",
        "# datalist, labellist = read(\"len200_500_n5000nr4.seq\", \"len200_500_n5000nr4.pos\")\n",
        "\n",
        "# Remove negatives\n",
        "pos_datalist, neg_datalist = split_labelled(datalist, labellist)\n",
        "neg_datalist = remove_sequences(neg_datalist, 0.1)\n",
        "# neg_datalist = remove_sequences_equal(neg_datalist, pos_datalist)\n",
        "datalist, labellist = fuse_sequence_lists(pos_datalist, neg_datalist)\n",
        "\n",
        "traindatalist, trainlabellist, testdatalist, testlabellist = generate_train_test(datalist, labellist, 0.6)\n",
        "traindataset = [traindatalist, trainlabellist]\n",
        "testdataset = [testdatalist, testlabellist]\n",
        "\n",
        "# Set batch_size and num_steps (maximum sequence length)\n",
        "train_iter = load_data(batch_size, num_steps, traindataset)\n",
        "test_iter = load_data(batch_size, num_steps, testdataset)\n",
        "\n",
        "print(next(iter(train_iter)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyRvhoyiwv_g",
        "outputId": "08e74a12-8090-4e5f-ad9c-ee243796bb58"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[10, 17,  9,  ..., 20, 20, 20],\n",
            "        [10,  0,  0,  ..., 20, 20, 20],\n",
            "        [10,  3,  7,  ..., 20, 20, 20],\n",
            "        ...,\n",
            "        [10,  9,  5,  ..., 20, 20, 20],\n",
            "        [10, 15, 17,  ..., 20, 20, 20],\n",
            "        [10, 15, 14,  ..., 20, 20, 20]]), tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len_list = []\n",
        "for s in datalist:\n",
        "    len_list.append(len(s))\n",
        "sns.histplot(len_list)\n",
        "\n",
        "p = 0\n",
        "n = 0\n",
        "for l in labellist:\n",
        "    if l:\n",
        "        p += 1\n",
        "    else:\n",
        "        n+=1\n",
        "print(f'{p = }\\n{n = }')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "26FSv9_pGhWY",
        "outputId": "993e8055-f846-48b7-ecce-3e48d64cb365"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p = 226\n",
            "n = 656\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train functions binary classifier"
      ],
      "metadata": {
        "id": "reETSGvLw6-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model, loss_fn, optimizer, device):\n",
        "        self.model = model.to(device)\n",
        "        self.loss_fn = loss_fn\n",
        "        self.optimizer = optimizer\n",
        "        self.device = device\n",
        "\n",
        "\n",
        "    def _train_one_epoch(self, train_iter):\n",
        "        result_loss = 0\n",
        "        tpos = 0\n",
        "        fpos = 0\n",
        "        tneg = 0\n",
        "        fneg = 0\n",
        "        self.model.train(True)\n",
        "        for i, (inputs, labels) in enumerate(train_iter):\n",
        "\n",
        "            inputs = inputs.to(self.device)\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            outputs = self.model(inputs).to('cpu')\n",
        "\n",
        "            loss = self.loss_fn(outputs, labels)\n",
        "            result_loss += loss.item()\n",
        "            loss.backward()\n",
        "\n",
        "            self.optimizer.step()\n",
        "\n",
        "            for j, l in enumerate(labels):\n",
        "                o = outputs[j].tolist().index(max(outputs[j]))\n",
        "                l = l.item()\n",
        "                if o == 1 and l == 1:\n",
        "                    tpos += 1\n",
        "                elif o == 1 and l == 0:\n",
        "                    fpos += 1\n",
        "                elif o == 0 and l == 0:\n",
        "                    tneg += 1\n",
        "                elif o == 0 and l == 1:\n",
        "                    fneg += 1\n",
        "                else:\n",
        "                    raise ValueError(f'Output or label is not rounded to zero: {o = } {l = }')\n",
        "        accuracy = (tpos + tneg) / (tpos + fpos + tneg + fneg)\n",
        "        precision = tpos / (tpos + fpos) if (tpos + fpos) > 0 else 0\n",
        "        recall = tpos / (tpos + fneg) if (tpos + fneg) > 0 else 0\n",
        "        fscore = 2 * tpos / (2*tpos + fpos + fneg) if (2*tpos + fpos + fneg) > 0 else 0\n",
        "        return accuracy, precision, recall, fscore, result_loss / (i+1)\n",
        "\n",
        "\n",
        "    def _test_one_epoch(self, test_iter):\n",
        "        result_loss = 0\n",
        "        tpos = 0\n",
        "        fpos = 0\n",
        "        tneg = 0\n",
        "        fneg = 0\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, (inputs, labels) in enumerate(test_iter):\n",
        "                inputs = inputs.to(device)\n",
        "\n",
        "                outputs = self.model(inputs).to('cpu')\n",
        "\n",
        "                loss = self.loss_fn(input=outputs, target=labels)\n",
        "                result_loss += loss.item()\n",
        "                for j, l in enumerate(labels):\n",
        "                    o = outputs[j].tolist().index(max(outputs[j]))\n",
        "                    l = l.item()\n",
        "                    if o == 1 and l == 1:\n",
        "                        tpos += 1\n",
        "                    elif o == 1 and l == 0:\n",
        "                        fpos += 1\n",
        "                    elif o == 0 and l == 0:\n",
        "                        tneg += 1\n",
        "                    elif o == 0 and l == 1:\n",
        "                        fneg += 1\n",
        "                    else:\n",
        "                        raise ValueError(f'Output or label is not rounded to zero: {o = } {l = }')\n",
        "        accuracy = (tpos + tneg) / (tpos + fpos + tneg + fneg)\n",
        "        precision = tpos / (tpos + fpos) if (tpos + fpos) > 0 else 0\n",
        "        recall = tpos / (tpos + fneg) if (tpos + fneg) > 0 else 0\n",
        "        fscore = 2 * tpos / (2*tpos + fpos + fneg) if (2*tpos + fpos + fneg) > 0 else 0\n",
        "        return accuracy, precision, recall, fscore, result_loss / (i+1)\n",
        "\n",
        "\n",
        "    def train(self, epochs, train_iter, test_iter):\n",
        "        for epoch in range(epochs):\n",
        "            train_acc, train_prec, train_rec, train_f, train_loss = self._train_one_epoch(train_iter)\n",
        "            test_acc, test_prec, test_rec, test_f, test_loss = self._test_one_epoch(test_iter)\n",
        "            print(f'{epoch = }\\t{train_loss=:.5f}\\t{test_loss=:.5f}\\t{train_acc=:.5f}\\t{test_acc=:.5f}\\t{test_prec=:.5f}\\t{test_rec=:.5f}\\t{test_f=:.5f}')\n",
        "\n"
      ],
      "metadata": {
        "id": "3Btdmdhj36zM"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Minimal model (does not work)"
      ],
      "metadata": {
        "id": "I2n78bgjw-uy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MinimalGOClassifierCNN(nn.Module):\n",
        "    def __init__(self, input_length: int, vocab_size : int=21,  num_filters: int=32, kernel_size: int=5):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, vocab_size)\n",
        "        self.conv_layer = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=vocab_size, out_channels=num_filters, kernel_size=kernel_size),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool1d(1)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LazyLinear(out_features=2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.conv_layer(x.transpose(1,2))\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        output = F.softmax(x, 1)\n",
        "        return output"
      ],
      "metadata": {
        "id": "6Agj87Dpbf-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Berry's model (works, go to for binary classification)"
      ],
      "metadata": {
        "id": "1wX4qCXZxCzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BerryCNN1D(nn.Module):\n",
        "    \"\"\"1D Convolutional Neural Network for protein function classification\"\"\"\n",
        "    def __init__(self, context_size: int, vocab_size: int = 21,  conv_channels: int = 128, use_bias: bool = False):\n",
        "        super().__init__()\n",
        "        assert context_size % 2 == 0, f'Invalid context_size, {context_size} is not an even number'\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "        # CNN model for binary classification\n",
        "        self.conv1 = nn.Sequential(\n",
        "            # conv block 1\n",
        "            nn.Conv1d(in_channels=vocab_size, out_channels=conv_channels, kernel_size=3, padding='same', bias=use_bias),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            # conv block 2\n",
        "            nn.Conv1d(in_channels=conv_channels, out_channels=conv_channels, kernel_size=3, padding='same', bias=use_bias),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveMaxPool1d(1)\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            # flatten + classification head\n",
        "            nn.Flatten(1, -1),\n",
        "            nn.LazyLinear(out_features=2, bias=use_bias)  # binary classification\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.tensor, targets: torch.tensor = None):\n",
        "        \"\"\"Predict protein function class (0 or 1)\"\"\"\n",
        "        x = self.embedding(x).permute(0,2,1)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "2V5v63DoLaLr"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BerryCNN1D(context_size=num_steps, conv_channels=128)\n",
        "model.apply(init_weights)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "670133ec-aa1b-4eab-ff40-8371957ca58c",
        "id": "-kFRoPlYpeKe"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BerryCNN1D(\n",
              "  (embedding): Embedding(21, 21)\n",
              "  (conv1): Sequential(\n",
              "    (0): Conv1d(21, 128, kernel_size=(3,), stride=(1,), padding=same, bias=False)\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (conv2): Sequential(\n",
              "    (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=same, bias=False)\n",
              "    (1): ReLU()\n",
              "    (2): AdaptiveMaxPool1d(output_size=1)\n",
              "  )\n",
              "  (fc): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): LazyLinear(in_features=0, out_features=2, bias=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(model, loss_fn, optimizer, device)\n",
        "trainer.train(epochs=50, train_iter=train_iter, test_iter=test_iter)"
      ],
      "metadata": {
        "id": "AMz0guzKptRZ",
        "outputId": "9a828106-b06e-48dc-f64c-e2129f347340",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch = 0\ttrain_loss=0.59139\ttest_loss=0.55797\ttrain_acc=0.73157\ttest_acc=0.75354\ttest_prec=0.00000\ttest_rec=0.00000\ttest_f=0.00000\n",
            "epoch = 1\ttrain_loss=0.56660\ttest_loss=0.56278\ttrain_acc=0.73724\ttest_acc=0.75354\ttest_prec=0.00000\ttest_rec=0.00000\ttest_f=0.00000\n",
            "epoch = 2\ttrain_loss=0.56258\ttest_loss=0.55463\ttrain_acc=0.73724\ttest_acc=0.75354\ttest_prec=0.00000\ttest_rec=0.00000\ttest_f=0.00000\n",
            "epoch = 3\ttrain_loss=0.54069\ttest_loss=0.54407\ttrain_acc=0.74669\ttest_acc=0.75354\ttest_prec=0.00000\ttest_rec=0.00000\ttest_f=0.00000\n",
            "epoch = 4\ttrain_loss=0.52345\ttest_loss=0.54486\ttrain_acc=0.73724\ttest_acc=0.75354\ttest_prec=0.00000\ttest_rec=0.00000\ttest_f=0.00000\n",
            "epoch = 5\ttrain_loss=0.51655\ttest_loss=0.57330\ttrain_acc=0.73913\ttest_acc=0.79037\ttest_prec=0.88235\ttest_rec=0.17241\ttest_f=0.28846\n",
            "epoch = 6\ttrain_loss=0.51245\ttest_loss=0.51653\ttrain_acc=0.78450\ttest_acc=0.75354\ttest_prec=0.00000\ttest_rec=0.00000\ttest_f=0.00000\n",
            "epoch = 7\ttrain_loss=0.47873\ttest_loss=0.53008\ttrain_acc=0.77883\ttest_acc=0.75354\ttest_prec=0.00000\ttest_rec=0.00000\ttest_f=0.00000\n",
            "epoch = 8\ttrain_loss=0.45053\ttest_loss=0.49675\ttrain_acc=0.77694\ttest_acc=0.75354\ttest_prec=0.00000\ttest_rec=0.00000\ttest_f=0.00000\n",
            "epoch = 9\ttrain_loss=0.41333\ttest_loss=0.49908\ttrain_acc=0.79395\ttest_acc=0.75354\ttest_prec=0.00000\ttest_rec=0.00000\ttest_f=0.00000\n",
            "epoch = 10\ttrain_loss=0.39641\ttest_loss=0.48046\ttrain_acc=0.82231\ttest_acc=0.76204\ttest_prec=1.00000\ttest_rec=0.03448\ttest_f=0.06667\n",
            "epoch = 11\ttrain_loss=0.36174\ttest_loss=0.48546\ttrain_acc=0.84688\ttest_acc=0.82720\ttest_prec=0.82500\ttest_rec=0.37931\ttest_f=0.51969\n",
            "epoch = 12\ttrain_loss=0.34788\ttest_loss=0.47390\ttrain_acc=0.86957\ttest_acc=0.76771\ttest_prec=0.85714\ttest_rec=0.06897\ttest_f=0.12766\n",
            "epoch = 13\ttrain_loss=0.31280\ttest_loss=0.46188\ttrain_acc=0.90170\ttest_acc=0.81020\ttest_prec=0.91667\ttest_rec=0.25287\ttest_f=0.39640\n",
            "epoch = 14\ttrain_loss=0.29650\ttest_loss=0.47666\ttrain_acc=0.90926\ttest_acc=0.76204\ttest_prec=1.00000\ttest_rec=0.03448\ttest_f=0.06667\n",
            "epoch = 15\ttrain_loss=0.26968\ttest_loss=0.45367\ttrain_acc=0.92628\ttest_acc=0.83286\ttest_prec=0.76923\ttest_rec=0.45977\ttest_f=0.57554\n",
            "epoch = 16\ttrain_loss=0.24919\ttest_loss=0.48458\ttrain_acc=0.94140\ttest_acc=0.76487\ttest_prec=1.00000\ttest_rec=0.04598\ttest_f=0.08791\n",
            "epoch = 17\ttrain_loss=0.24321\ttest_loss=0.44699\ttrain_acc=0.92628\ttest_acc=0.79603\ttest_prec=0.94118\ttest_rec=0.18391\ttest_f=0.30769\n",
            "epoch = 18\ttrain_loss=0.21201\ttest_loss=0.42957\ttrain_acc=0.95085\ttest_acc=0.83003\ttest_prec=0.75472\ttest_rec=0.45977\ttest_f=0.57143\n",
            "epoch = 19\ttrain_loss=0.18880\ttest_loss=0.43507\ttrain_acc=0.95841\ttest_acc=0.80453\ttest_prec=0.95000\ttest_rec=0.21839\ttest_f=0.35514\n",
            "epoch = 20\ttrain_loss=0.17758\ttest_loss=0.42143\ttrain_acc=0.96975\ttest_acc=0.82720\ttest_prec=0.74074\ttest_rec=0.45977\ttest_f=0.56738\n",
            "epoch = 21\ttrain_loss=0.16227\ttest_loss=0.42715\ttrain_acc=0.97732\ttest_acc=0.83569\ttest_prec=0.79592\ttest_rec=0.44828\ttest_f=0.57353\n",
            "epoch = 22\ttrain_loss=0.14817\ttest_loss=0.41158\ttrain_acc=0.98488\ttest_acc=0.83569\ttest_prec=0.79592\ttest_rec=0.44828\ttest_f=0.57353\n",
            "epoch = 23\ttrain_loss=0.14057\ttest_loss=0.40954\ttrain_acc=0.98110\ttest_acc=0.83569\ttest_prec=0.80851\ttest_rec=0.43678\ttest_f=0.56716\n",
            "epoch = 24\ttrain_loss=0.12259\ttest_loss=0.41269\ttrain_acc=0.98677\ttest_acc=0.82436\ttest_prec=0.72727\ttest_rec=0.45977\ttest_f=0.56338\n",
            "epoch = 25\ttrain_loss=0.11603\ttest_loss=0.41532\ttrain_acc=0.99622\ttest_acc=0.83003\ttest_prec=0.80000\ttest_rec=0.41379\ttest_f=0.54545\n",
            "epoch = 26\ttrain_loss=0.10453\ttest_loss=0.41346\ttrain_acc=0.99244\ttest_acc=0.83569\ttest_prec=0.80851\ttest_rec=0.43678\ttest_f=0.56716\n",
            "epoch = 27\ttrain_loss=0.09462\ttest_loss=0.42377\ttrain_acc=0.99811\ttest_acc=0.83003\ttest_prec=0.88571\ttest_rec=0.35632\ttest_f=0.50820\n",
            "epoch = 28\ttrain_loss=0.08658\ttest_loss=0.42765\ttrain_acc=0.99811\ttest_acc=0.84136\ttest_prec=0.86047\ttest_rec=0.42529\ttest_f=0.56923\n",
            "epoch = 29\ttrain_loss=0.08005\ttest_loss=0.41718\ttrain_acc=1.00000\ttest_acc=0.83003\ttest_prec=0.72881\ttest_rec=0.49425\ttest_f=0.58904\n",
            "epoch = 30\ttrain_loss=0.07473\ttest_loss=0.41656\ttrain_acc=1.00000\ttest_acc=0.83569\ttest_prec=0.85366\ttest_rec=0.40230\ttest_f=0.54688\n",
            "epoch = 31\ttrain_loss=0.06769\ttest_loss=0.40171\ttrain_acc=1.00000\ttest_acc=0.82720\ttest_prec=0.70968\ttest_rec=0.50575\ttest_f=0.59060\n",
            "epoch = 32\ttrain_loss=0.06448\ttest_loss=0.42349\ttrain_acc=1.00000\ttest_acc=0.83853\ttest_prec=0.78846\ttest_rec=0.47126\ttest_f=0.58993\n",
            "epoch = 33\ttrain_loss=0.05793\ttest_loss=0.41549\ttrain_acc=1.00000\ttest_acc=0.83853\ttest_prec=0.85714\ttest_rec=0.41379\ttest_f=0.55814\n",
            "epoch = 34\ttrain_loss=0.05436\ttest_loss=0.40301\ttrain_acc=1.00000\ttest_acc=0.83003\ttest_prec=0.72881\ttest_rec=0.49425\ttest_f=0.58904\n",
            "epoch = 35\ttrain_loss=0.05245\ttest_loss=0.41839\ttrain_acc=1.00000\ttest_acc=0.83853\ttest_prec=0.80000\ttest_rec=0.45977\ttest_f=0.58394\n",
            "epoch = 36\ttrain_loss=0.04913\ttest_loss=0.40499\ttrain_acc=1.00000\ttest_acc=0.83853\ttest_prec=0.78846\ttest_rec=0.47126\ttest_f=0.58993\n",
            "epoch = 37\ttrain_loss=0.04512\ttest_loss=0.39843\ttrain_acc=1.00000\ttest_acc=0.83003\ttest_prec=0.71429\ttest_rec=0.51724\ttest_f=0.60000\n",
            "epoch = 38\ttrain_loss=0.04252\ttest_loss=0.42084\ttrain_acc=1.00000\ttest_acc=0.84136\ttest_prec=0.86047\ttest_rec=0.42529\ttest_f=0.56923\n",
            "epoch = 39\ttrain_loss=0.03971\ttest_loss=0.40801\ttrain_acc=1.00000\ttest_acc=0.83853\ttest_prec=0.78846\ttest_rec=0.47126\ttest_f=0.58993\n",
            "epoch = 40\ttrain_loss=0.03775\ttest_loss=0.42309\ttrain_acc=1.00000\ttest_acc=0.84136\ttest_prec=0.79245\ttest_rec=0.48276\ttest_f=0.60000\n",
            "epoch = 41\ttrain_loss=0.03551\ttest_loss=0.45340\ttrain_acc=1.00000\ttest_acc=0.82720\ttest_prec=0.90625\ttest_rec=0.33333\ttest_f=0.48739\n",
            "epoch = 42\ttrain_loss=0.03501\ttest_loss=0.40439\ttrain_acc=1.00000\ttest_acc=0.83853\ttest_prec=0.78846\ttest_rec=0.47126\ttest_f=0.58993\n",
            "epoch = 43\ttrain_loss=0.03211\ttest_loss=0.44457\ttrain_acc=1.00000\ttest_acc=0.83853\ttest_prec=0.80000\ttest_rec=0.45977\ttest_f=0.58394\n",
            "epoch = 44\ttrain_loss=0.03017\ttest_loss=0.41275\ttrain_acc=1.00000\ttest_acc=0.84136\ttest_prec=0.79245\ttest_rec=0.48276\ttest_f=0.60000\n",
            "epoch = 45\ttrain_loss=0.02920\ttest_loss=0.41483\ttrain_acc=1.00000\ttest_acc=0.84136\ttest_prec=0.80392\ttest_rec=0.47126\ttest_f=0.59420\n",
            "epoch = 46\ttrain_loss=0.02739\ttest_loss=0.42618\ttrain_acc=1.00000\ttest_acc=0.84136\ttest_prec=0.84444\ttest_rec=0.43678\ttest_f=0.57576\n",
            "epoch = 47\ttrain_loss=0.02633\ttest_loss=0.40286\ttrain_acc=1.00000\ttest_acc=0.83003\ttest_prec=0.72881\ttest_rec=0.49425\ttest_f=0.58904\n",
            "epoch = 48\ttrain_loss=0.02520\ttest_loss=0.41104\ttrain_acc=1.00000\ttest_acc=0.83853\ttest_prec=0.78846\ttest_rec=0.47126\ttest_f=0.58993\n",
            "epoch = 49\ttrain_loss=0.02391\ttest_loss=0.41847\ttrain_acc=1.00000\ttest_acc=0.83853\ttest_prec=0.80000\ttest_rec=0.45977\ttest_f=0.58394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extra convolutional layer (does not improve)"
      ],
      "metadata": {
        "id": "_dsG7ooLxI2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MoreCNN1D(nn.Module):\n",
        "    \"\"\"1D Convolutional Neural Network for protein function classification\"\"\"\n",
        "    def __init__(self, context_size: int, vocab_size: int = 21,  conv_channels: int = 128, use_bias: bool = False):\n",
        "        super().__init__()\n",
        "        assert context_size % 2 == 0, f'Invalid context_size, {context_size} is not an even number'\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "        # CNN model for binary classification\n",
        "        self.conv1 = nn.Sequential(\n",
        "            # conv block 1\n",
        "            nn.Conv1d(in_channels=vocab_size, out_channels=conv_channels, kernel_size=3, padding='same', bias=use_bias),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            # conv block 2\n",
        "            nn.Conv1d(in_channels=conv_channels, out_channels=conv_channels, kernel_size=3, padding='same', bias=use_bias),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            # conv block 3\n",
        "            nn.Conv1d(in_channels=conv_channels, out_channels=conv_channels, kernel_size=3, padding='same', bias=use_bias),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveMaxPool1d(1)\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            # flatten + classification head\n",
        "            nn.Flatten(1, -1),\n",
        "            nn.LazyLinear(out_features=2, bias=use_bias)  # binary classification\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.tensor, targets: torch.tensor = None):\n",
        "        \"\"\"Predict protein function class (0 or 1)\"\"\"\n",
        "        x = self.embedding(x).permute(0,2,1)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        return self.fc(x)"
      ],
      "metadata": {
        "id": "M6H_ib65yCPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MoreCNN1D(context_size=num_steps, conv_channels=128)\n",
        "model.apply(init_weights)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBZC4GHeLUHz",
        "outputId": "4cb3b9e9-935f-47da-e0d0-d87df22ab1e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MoreCNN1D(\n",
              "  (embedding): Embedding(21, 21)\n",
              "  (conv1): Sequential(\n",
              "    (0): Conv1d(21, 128, kernel_size=(3,), stride=(1,), padding=same, bias=False)\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (conv2): Sequential(\n",
              "    (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=same, bias=False)\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (conv3): Sequential(\n",
              "    (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=same, bias=False)\n",
              "    (1): ReLU()\n",
              "    (2): AdaptiveMaxPool1d(output_size=1)\n",
              "  )\n",
              "  (fc): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): LazyLinear(in_features=0, out_features=2, bias=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(model, loss_fn, optimizer)\n",
        "trainer.train(epochs=10, train_iter=train_iter, test_iter=test_iter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fh2DuxL8e7n",
        "outputId": "3f6296b1-1e39-4491-b330-d21c3136e2ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch = 0\ttrain_loss=0.35404\ttest_loss=0.33095\n",
            "epoch = 1\ttrain_loss=0.33558\ttest_loss=0.32332\n",
            "epoch = 2\ttrain_loss=0.32925\ttest_loss=0.32346\n",
            "epoch = 3\ttrain_loss=0.31973\ttest_loss=0.31417\n",
            "epoch = 4\ttrain_loss=0.30916\ttest_loss=0.32241\n",
            "epoch = 5\ttrain_loss=0.30079\ttest_loss=0.30237\n",
            "epoch = 6\ttrain_loss=0.28817\ttest_loss=0.29379\n",
            "epoch = 7\ttrain_loss=0.27623\ttest_loss=0.28432\n",
            "epoch = 8\ttrain_loss=0.25552\ttest_loss=0.27249\n",
            "epoch = 9\ttrain_loss=0.23447\ttest_loss=0.29422\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data multiple labels"
      ],
      "metadata": {
        "id": "i1r1FMrdxNj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_multiple_pos(seqfile: str, posfiles: list[str]) -> tuple[list, list]:\n",
        "    \"\"\"Read file with sequences and file with positive cases into lists.\n",
        "\n",
        "    :param seqfile: file with sequences\n",
        "    :type seqfile: str\n",
        "    :param posfiles: files with positive cases (annotated with function)\n",
        "    :type posfile: str\n",
        "    :return: two lists, first with sequences and second with integer labels\n",
        "    :rtype: tuple[list, list]\n",
        "    \"\"\"\n",
        "    idlist = []\n",
        "    datalist = []\n",
        "    labellist = []\n",
        "    with open(seqfile, 'r') as f:\n",
        "        for line in f.readlines():\n",
        "            line = line.rstrip().split('\\t')\n",
        "            idlist.append(line[0])\n",
        "            datalist.append(line[1])\n",
        "            labellist.append([0 for _ in posfiles])\n",
        "    for l, posfile in enumerate(posfiles):\n",
        "        with open(posfile, 'r') as f:\n",
        "            for line in f.readlines():\n",
        "                id = line.rstrip()\n",
        "                try:\n",
        "                    i = idlist.index(id)\n",
        "                    # if labellist[i] != 0:\n",
        "                    #     raise Exception(f'Sequence with multiple labels. {id = }, {labellist[i] = }, {l = }')\n",
        "                    labellist[i][l] = 1\n",
        "                except ValueError:\n",
        "                    continue\n",
        "    return datalist, labellist\n",
        "\n",
        "\n",
        "def split_labelled_multiple_pos(datalist: list, labellist: list):\n",
        "    pos_datalist = []\n",
        "    pos_labellist = []\n",
        "    neg_datalist = []\n",
        "    neg_labellist = []\n",
        "    for i, labels in enumerate(labellist):\n",
        "        is_pos = False\n",
        "        for label in labels:\n",
        "            if label:\n",
        "                is_pos = True\n",
        "        if is_pos:\n",
        "            pos_datalist.append(datalist[i])\n",
        "            pos_labellist.append(labels)\n",
        "        else:\n",
        "            neg_datalist.append(datalist[i])\n",
        "            neg_labellist.append(labels)\n",
        "    return pos_datalist, pos_labellist, neg_datalist, neg_labellist\n",
        "\n",
        "\n",
        "def zip_n_shuffle(list1: list, list2: list) -> tuple[list, list]:\n",
        "    assert len(list1) == len(list2)\n",
        "    combined = list(zip(list1, list2))\n",
        "    random.shuffle(combined)\n",
        "    list1, list2 = zip(*combined)\n",
        "    return list(list1), list(list2)\n",
        "\n",
        "\n",
        "def remove_sequences_multiple_pos(datalist: list, labellist, fraction=0.5):\n",
        "    datalist, labellist = zip_n_shuffle(datalist, labellist)\n",
        "    i = round(len(datalist) * fraction)\n",
        "    return datalist[:i], labellist[:i]\n",
        "\n",
        "\n",
        "def remove_sequences_equal_multiple_pos(reduced_datalist: list, reduced_labellist: list, compared_datalist: list):\n",
        "    reduced_datalist, reduced_labellist = zip_n_shuffle(reduced_datalist, reduced_labellist)\n",
        "    reduced_datalist = reduced_datalist[:len(compared_datalist)]\n",
        "    reduced_labellist = reduced_labellist[:len(compared_datalist)]\n",
        "    if len(compared_datalist) != len(reduced_datalist) or len(compared_datalist) != len(reduced_labellist):\n",
        "        raise ValueError\n",
        "    return reduced_datalist, reduced_labellist\n",
        "\n",
        "\n",
        "def fuse_sequence_lists_multiple_pos(pos_datalist: list, pos_labellist:list, neg_datalist: list, neg_labellist):\n",
        "    datalist = pos_datalist + neg_datalist\n",
        "    labellist = pos_labellist + neg_labellist\n",
        "    return datalist, labellist\n",
        "\n",
        "\n",
        "def calculate_pos_weights(labellist: list):\n",
        "    total_samples = len(labellist)\n",
        "    label_counts = torch.Tensor(labellist).sum(0)\n",
        "    pos_weights = (total_samples - label_counts) / (label_counts + 1e-5)\n",
        "    return pos_weights"
      ],
      "metadata": {
        "id": "g9TF3MfTr1o6"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trainer multiple labels"
      ],
      "metadata": {
        "id": "o0TJEbhQxRLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "class TrainerMultipleClasses:\n",
        "    def __init__(self, model, loss_fn, optimizer, device):\n",
        "        self.model = model.to(device)\n",
        "        self.loss_fn = loss_fn\n",
        "        self.optimizer = optimizer\n",
        "        self.device = device\n",
        "\n",
        "\n",
        "    def _train_one_epoch(self, train_iter):\n",
        "        result_loss = 0\n",
        "        tpos = 0\n",
        "        fpos = 0\n",
        "        tneg = 0\n",
        "        fneg = 0\n",
        "        self.model.train(True)\n",
        "        for i, (inputs, labels) in enumerate(train_iter):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            outputs = self.model(inputs).to('cpu')\n",
        "            labels = labels.type(torch.float32)\n",
        "\n",
        "            loss = loss_fn(input=outputs, target=labels)\n",
        "            result_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            self.optimizer.step()\n",
        "            for b, lab in enumerate(labels):\n",
        "                out = torch.round(torch.sigmoid(outputs[b]))\n",
        "\n",
        "                for j, o in enumerate(out):\n",
        "                    # print(f'{o=}\\t{l=}')\n",
        "                    l = lab[j]\n",
        "                    if o == 1 and l == 1:\n",
        "                        tpos += 1\n",
        "                    elif o == 1 and l == 0:\n",
        "                        fpos += 1\n",
        "                    elif o == 0 and l == 0:\n",
        "                        tneg += 1\n",
        "                    elif o == 0 and l == 1:\n",
        "                        fneg += 1\n",
        "                    else:\n",
        "                        raise ValueError(f'Output or label is not rounded to zero: {o = } {l = }')\n",
        "                    # print(f'{tpos=}\\t{fpos=}\\t{tneg=}\\t{fneg=}')\n",
        "        accuracy = (tpos + tneg) / (tpos + fpos + tneg + fneg)\n",
        "        precision = tpos / (tpos + fpos) if (tpos + fpos) > 0 else 0\n",
        "        recall = tpos / (tpos + fneg) if (tpos + fneg) > 0 else 0\n",
        "        fscore = 2 * tpos / (2*tpos + fpos + fneg) if (2*tpos + fpos + fneg) > 0 else 0\n",
        "        return accuracy, precision, recall, fscore, result_loss / (i+1)\n",
        "\n",
        "\n",
        "    def _test_one_epoch(self, test_iter):\n",
        "        result_loss = 0\n",
        "        tpos = 0\n",
        "        fpos = 0\n",
        "        tneg = 0\n",
        "        fneg = 0\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for i, (inputs, labels) in enumerate(test_iter):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels\n",
        "                outputs = self.model(inputs).to('cpu')\n",
        "                labels = labels.type(torch.float32)\n",
        "                loss = loss_fn(input=outputs, target=labels)\n",
        "                result_loss += loss.item()\n",
        "                for b, lab in enumerate(labels):\n",
        "                    out = torch.round(torch.sigmoid(outputs[b]))\n",
        "                    for j, o in enumerate(out):\n",
        "                        l = lab[j]\n",
        "                        if o == 1 and l == 1:\n",
        "                            tpos += 1\n",
        "                        elif o == 1 and l == 0:\n",
        "                            fpos += 1\n",
        "                        elif o == 0 and l == 0:\n",
        "                            tneg += 1\n",
        "                        elif o == 0 and l == 1:\n",
        "                            fneg += 1\n",
        "                        else:\n",
        "                            raise ValueError(f'Output or label is not rounded to zero: {o = } {l = }')\n",
        "        # print(f'{tpos=}\\t{fpos=}\\t{tneg=}\\t{fneg=}')\n",
        "        accuracy = (tpos + tneg) / (tpos + fpos + tneg + fneg)\n",
        "        precision = tpos / (tpos + fpos) if (tpos + fpos) > 0 else 0\n",
        "        recall = tpos / (tpos + fneg) if (tpos + fneg) > 0 else 0\n",
        "        fscore = 2 * tpos / (2*tpos + fpos + fneg) if (2*tpos + fpos + fneg) > 0 else 0\n",
        "        return accuracy, precision, recall, fscore, result_loss / (i+1)\n",
        "\n",
        "\n",
        "    def train(self, epochs, train_iter, test_iter):\n",
        "        for epoch in range(epochs):\n",
        "            train_acc, train_prec, train_rec, train_f, train_loss = self._train_one_epoch(train_iter)\n",
        "            test_acc, test_prec, test_rec, test_f, test_loss = self._test_one_epoch(test_iter)\n",
        "            print(f'{epoch = }\\t{train_loss=:.5f}\\t{test_loss=:.5f}\\t{train_acc=:.5f}\\t{test_acc=:.5f}\\t{test_prec=:.5f}\\t{test_rec=:.5f}\\t{test_f=:.5f}')"
      ],
      "metadata": {
        "id": "7xssSozK0uO3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model multiple labels"
      ],
      "metadata": {
        "id": "xsJ8Xb1zxVYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiClassifierCNN1D(nn.Module):\n",
        "    \"\"\"1D Convolutional Neural Network for protein function classification\"\"\"\n",
        "    def __init__(self, num_classes: int, context_size: int, vocab_size: int = 21,  conv_channels: int = 128, use_bias: bool = False):\n",
        "        super().__init__()\n",
        "        assert context_size % 2 == 0, f'Invalid context_size, {context_size} is not an even number'\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "        # CNN model for binary classification\n",
        "        self.conv1 = nn.Sequential(\n",
        "            # conv block 1\n",
        "            nn.Conv1d(in_channels=vocab_size, out_channels=conv_channels, kernel_size=3, padding='same', bias=use_bias),\n",
        "            nn.BatchNorm1d(conv_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
        "            nn.Dropout(0)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            # conv block 2\n",
        "            nn.Conv1d(in_channels=conv_channels, out_channels=conv_channels, kernel_size=3, padding='same', bias=use_bias),\n",
        "            nn.BatchNorm1d(conv_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(1),\n",
        "            nn.Dropout(0)\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            # conv block 3\n",
        "            nn.Conv1d(in_channels=conv_channels, out_channels=conv_channels, kernel_size=3, padding='same', bias=use_bias),\n",
        "            nn.BatchNorm1d(conv_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveMaxPool1d(1)\n",
        "        )\n",
        "        self.fc1 = nn.Sequential(\n",
        "            # flatten + classification head\n",
        "            nn.Flatten(1, -1),\n",
        "            nn.LazyLinear(out_features=64, bias=use_bias),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0)\n",
        "        )\n",
        "        self.fc2 = nn.Sequential(\n",
        "            # flatten + classification head\n",
        "            nn.LazyLinear(out_features=num_classes, bias=use_bias)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.tensor, targets: torch.tensor = None):\n",
        "        \"\"\"Predict protein function class (0 or 1)\"\"\"\n",
        "        x = self.embedding(x).permute(0,2,1)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.fc1(x)\n",
        "        return self.fc2(x)\n"
      ],
      "metadata": {
        "id": "DvXk1P5yzsRZ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load GO data multiple labels"
      ],
      "metadata": {
        "id": "5H-PP8bUxpCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "annot_files = [\n",
        "    \"GO_3A0005576.annotprot\",\n",
        "    \"GO_3A0005739.annotprot\",\n",
        "    \"GO_3A0007165.annotprot\",\n",
        "    \"GO_3A0043066.annotprot\",\n",
        "    \"GO_3A0055085.annotprot\"\n",
        "]\n",
        "\n",
        "batch_size = 10\n",
        "num_steps = 1000\n",
        "\n",
        "dl, ll = read_multiple_pos(\"expr5Tseq_filtGO_100-1000.lis\", annot_files)\n",
        "\n",
        "p_dl, p_ll, n_dl, n_ll = split_labelled_multiple_pos(dl, ll)\n",
        "n_dl, n_ll = remove_sequences_multiple_pos(n_dl, n_ll, 0.1)\n",
        "dl, ll = fuse_sequence_lists_multiple_pos(p_dl, p_ll, n_dl, n_ll)\n",
        "\n",
        "train_dl, train_ll, test_dl, test_ll = generate_train_test(dl, ll, 0.6)\n",
        "train_ds = [train_dl, train_ll]\n",
        "test_ds = [test_dl, test_ll]\n",
        "\n",
        "traindataloader = load_data(batch_size, num_steps, train_ds)\n",
        "testdataloader = load_data(batch_size, num_steps, test_ds)\n",
        "\n",
        "pos_weights = calculate_pos_weights(train_ll)\n",
        "\n",
        "print(next(iter(traindataloader)))"
      ],
      "metadata": {
        "id": "CId5XjpFtW82",
        "outputId": "b35be285-9888-4abf-98b2-5246733d05d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[10,  0,  0,  ..., 20, 20, 20],\n",
            "        [10, 19, 14,  ..., 20, 20, 20],\n",
            "        [10,  7, 15,  ..., 20, 20, 20],\n",
            "        ...,\n",
            "        [10,  0, 13,  ..., 20, 20, 20],\n",
            "        [10, 14,  7,  ..., 20, 20, 20],\n",
            "        [10, 15,  5,  ..., 20, 20, 20]]), tensor([[0, 0, 1, 0, 0],\n",
            "        [0, 0, 1, 0, 0],\n",
            "        [0, 1, 0, 0, 0],\n",
            "        [0, 0, 0, 1, 0],\n",
            "        [0, 1, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 1],\n",
            "        [0, 0, 1, 0, 0],\n",
            "        [0, 1, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len_list = []\n",
        "for s in dl:\n",
        "    len_list.append(len(s))\n",
        "sns.histplot(len_list)\n",
        "\n",
        "p = 0\n",
        "n = 0\n",
        "for labels in ll:\n",
        "    found_pos = False\n",
        "    for l in labels:\n",
        "        if l:\n",
        "            p += 1\n",
        "            found_pos = True\n",
        "            break\n",
        "    if not found_pos:\n",
        "        n+=1\n",
        "print(f'{p = }\\n{n = }')"
      ],
      "metadata": {
        "id": "JrwN-K7Syzod",
        "outputId": "74cd13c4-5d28-4558-968f-188a8d47ceb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p = 1454\n",
            "n = 533\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train model multiple labels"
      ],
      "metadata": {
        "id": "bcZoEI7-xuop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultiClassifierCNN1D(num_classes=5, context_size=num_steps, conv_channels=256, use_bias=True)\n",
        "model.apply(init_weights)\n",
        "loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9)\n",
        "model"
      ],
      "metadata": {
        "id": "towK5DH_147q",
        "outputId": "c67cb366-f595-4c29-e2dc-dabdedb4be57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultiClassifierCNN1D(\n",
              "  (embedding): Embedding(21, 21)\n",
              "  (conv1): Sequential(\n",
              "    (0): Conv1d(21, 256, kernel_size=(3,), stride=(1,), padding=same)\n",
              "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (4): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (conv2): Sequential(\n",
              "    (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=same)\n",
              "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): MaxPool1d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
              "    (4): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (conv3): Sequential(\n",
              "    (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=same)\n",
              "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): AdaptiveMaxPool1d(output_size=1)\n",
              "  )\n",
              "  (fc1): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): LazyLinear(in_features=0, out_features=64, bias=True)\n",
              "    (2): ReLU()\n",
              "    (3): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (fc2): Sequential(\n",
              "    (0): LazyLinear(in_features=0, out_features=5, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = TrainerMultipleClasses(model, loss_fn, optimizer, device)\n",
        "trainer.train(epochs=50, train_iter=traindataloader, test_iter=testdataloader)"
      ],
      "metadata": {
        "id": "xv-k3v3e2Afo",
        "outputId": "bf245e32-45c2-4cf8-b91b-4cda5b446451",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch = 0\ttrain_loss=1.18734\ttest_loss=1.18547\ttrain_acc=0.57081\ttest_acc=0.55270\ttest_prec=0.14859\ttest_rec=0.36073\ttest_f=0.21048\n",
            "epoch = 1\ttrain_loss=1.16334\ttest_loss=1.18451\ttrain_acc=0.47315\ttest_acc=0.45509\ttest_prec=0.18365\ttest_rec=0.66667\ttest_f=0.28797\n",
            "epoch = 2\ttrain_loss=1.16443\ttest_loss=1.18143\ttrain_acc=0.50872\ttest_acc=0.45509\ttest_prec=0.18365\ttest_rec=0.66667\ttest_f=0.28797\n",
            "epoch = 3\ttrain_loss=1.16709\ttest_loss=1.18601\ttrain_acc=0.50268\ttest_acc=0.45509\ttest_prec=0.18365\ttest_rec=0.66667\ttest_f=0.28797\n",
            "epoch = 4\ttrain_loss=1.16091\ttest_loss=1.18219\ttrain_acc=0.58926\ttest_acc=0.71522\ttest_prec=0.20201\ttest_rec=0.24505\ttest_f=0.22146\n",
            "epoch = 5\ttrain_loss=1.16220\ttest_loss=1.18381\ttrain_acc=0.54362\ttest_acc=0.67220\ttest_prec=0.09524\ttest_rec=0.11568\ttest_f=0.10447\n",
            "epoch = 6\ttrain_loss=1.16423\ttest_loss=1.18129\ttrain_acc=0.42248\ttest_acc=0.67220\ttest_prec=0.09524\ttest_rec=0.11568\ttest_f=0.10447\n",
            "epoch = 7\ttrain_loss=1.16215\ttest_loss=1.18300\ttrain_acc=0.57483\ttest_acc=0.56553\ttest_prec=0.16394\ttest_rec=0.39726\ttest_f=0.23210\n",
            "epoch = 8\ttrain_loss=1.16235\ttest_loss=1.18137\ttrain_acc=0.43456\ttest_acc=0.40428\ttest_prec=0.14130\ttest_rec=0.51294\ttest_f=0.22156\n",
            "epoch = 9\ttrain_loss=1.16468\ttest_loss=1.18164\ttrain_acc=0.57299\ttest_acc=0.47673\ttest_prec=0.20168\ttest_rec=0.73212\ttest_f=0.31624\n",
            "epoch = 10\ttrain_loss=1.16411\ttest_loss=1.18329\ttrain_acc=0.61527\ttest_acc=0.58591\ttest_prec=0.18919\ttest_rec=0.45814\ttest_f=0.26779\n",
            "epoch = 11\ttrain_loss=1.16407\ttest_loss=1.18298\ttrain_acc=0.49295\ttest_acc=0.47673\ttest_prec=0.20168\ttest_rec=0.73212\ttest_f=0.31624\n",
            "epoch = 12\ttrain_loss=1.16201\ttest_loss=1.18239\ttrain_acc=0.39899\ttest_acc=0.58616\ttest_prec=0.18931\ttest_rec=0.45814\ttest_f=0.26791\n",
            "epoch = 13\ttrain_loss=1.16673\ttest_loss=1.18488\ttrain_acc=0.56460\ttest_acc=0.44604\ttest_prec=0.17610\ttest_rec=0.63927\ttest_f=0.27613\n",
            "epoch = 14\ttrain_loss=1.16097\ttest_loss=1.18207\ttrain_acc=0.54765\ttest_acc=0.32704\ttest_prec=0.18270\ttest_rec=0.88432\ttest_f=0.30284\n",
            "epoch = 15\ttrain_loss=1.16193\ttest_loss=1.18348\ttrain_acc=0.39060\ttest_acc=0.56604\ttest_prec=0.16415\ttest_rec=0.39726\ttest_f=0.23231\n",
            "epoch = 16\ttrain_loss=1.16552\ttest_loss=1.18182\ttrain_acc=0.56812\ttest_acc=0.58616\ttest_prec=0.18931\ttest_rec=0.45814\ttest_f=0.26791\n",
            "epoch = 17\ttrain_loss=1.16332\ttest_loss=1.18060\ttrain_acc=0.51980\ttest_acc=0.58616\ttest_prec=0.18931\ttest_rec=0.45814\ttest_f=0.26791\n",
            "epoch = 18\ttrain_loss=1.16306\ttest_loss=1.18468\ttrain_acc=0.46409\ttest_acc=0.56604\ttest_prec=0.16415\ttest_rec=0.39726\ttest_f=0.23231\n",
            "epoch = 19\ttrain_loss=1.16337\ttest_loss=1.18132\ttrain_acc=0.51309\ttest_acc=0.56604\ttest_prec=0.16415\ttest_rec=0.39726\ttest_f=0.23231\n",
            "epoch = 20\ttrain_loss=1.16700\ttest_loss=1.18185\ttrain_acc=0.41812\ttest_acc=0.58616\ttest_prec=0.18931\ttest_rec=0.45814\ttest_f=0.26791\n",
            "epoch = 21\ttrain_loss=1.16319\ttest_loss=1.18534\ttrain_acc=0.49430\ttest_acc=0.61686\ttest_prec=0.22767\ttest_rec=0.55099\ttest_f=0.32221\n",
            "epoch = 22\ttrain_loss=1.16111\ttest_loss=1.18389\ttrain_acc=0.48389\ttest_acc=0.31497\ttest_prec=0.17516\ttest_rec=0.84779\ttest_f=0.29033\n",
            "epoch = 23\ttrain_loss=1.16785\ttest_loss=1.18269\ttrain_acc=0.50369\ttest_acc=0.46717\ttest_prec=0.19371\ttest_rec=0.70320\ttest_f=0.30375\n",
            "epoch = 24\ttrain_loss=1.16095\ttest_loss=1.18222\ttrain_acc=0.36812\ttest_acc=0.43396\ttest_prec=0.16604\ttest_rec=0.60274\ttest_f=0.26036\n",
            "epoch = 25\ttrain_loss=1.16437\ttest_loss=1.18270\ttrain_acc=0.51510\ttest_acc=0.41384\ttest_prec=0.14927\ttest_rec=0.54186\ttest_f=0.23406\n",
            "epoch = 26\ttrain_loss=1.16557\ttest_loss=1.18083\ttrain_acc=0.46074\ttest_acc=0.58616\ttest_prec=0.18931\ttest_rec=0.45814\ttest_f=0.26791\n",
            "epoch = 27\ttrain_loss=1.16082\ttest_loss=1.18346\ttrain_acc=0.47852\ttest_acc=0.58616\ttest_prec=0.18931\ttest_rec=0.45814\ttest_f=0.26791\n",
            "epoch = 28\ttrain_loss=1.16320\ttest_loss=1.18119\ttrain_acc=0.50436\ttest_acc=0.56604\ttest_prec=0.16415\ttest_rec=0.39726\ttest_f=0.23231\n",
            "epoch = 29\ttrain_loss=1.16313\ttest_loss=1.18141\ttrain_acc=0.51913\ttest_acc=0.46717\ttest_prec=0.19371\ttest_rec=0.70320\ttest_f=0.30375\n",
            "epoch = 30\ttrain_loss=1.16563\ttest_loss=1.18116\ttrain_acc=0.50906\ttest_acc=0.45509\ttest_prec=0.18365\ttest_rec=0.66667\ttest_f=0.28797\n",
            "epoch = 31\ttrain_loss=1.16475\ttest_loss=1.18303\ttrain_acc=0.49128\ttest_acc=0.58616\ttest_prec=0.18931\ttest_rec=0.45814\ttest_f=0.26791\n",
            "epoch = 32\ttrain_loss=1.16613\ttest_loss=1.18568\ttrain_acc=0.47987\ttest_acc=0.53283\ttest_prec=0.12264\ttest_rec=0.29680\ttest_f=0.17356\n",
            "epoch = 33\ttrain_loss=1.16586\ttest_loss=1.18302\ttrain_acc=0.33322\ttest_acc=0.38314\ttest_prec=0.12369\ttest_rec=0.44901\ttest_f=0.19395\n",
            "epoch = 34\ttrain_loss=1.16307\ttest_loss=1.18536\ttrain_acc=0.50503\ttest_acc=0.43396\ttest_prec=0.16604\ttest_rec=0.60274\ttest_f=0.26036\n",
            "epoch = 35\ttrain_loss=1.16326\ttest_loss=1.18250\ttrain_acc=0.44497\ttest_acc=0.41384\ttest_prec=0.14927\ttest_rec=0.54186\ttest_f=0.23406\n",
            "epoch = 36\ttrain_loss=1.16566\ttest_loss=1.18441\ttrain_acc=0.40940\ttest_acc=0.44604\ttest_prec=0.17610\ttest_rec=0.63927\ttest_f=0.27613\n",
            "epoch = 37\ttrain_loss=1.16629\ttest_loss=1.18241\ttrain_acc=0.44564\ttest_acc=0.44604\ttest_prec=0.17610\ttest_rec=0.63927\ttest_f=0.27613\n",
            "epoch = 38\ttrain_loss=1.16413\ttest_loss=1.18139\ttrain_acc=0.46913\ttest_acc=0.40428\ttest_prec=0.14130\ttest_rec=0.51294\ttest_f=0.22156\n",
            "epoch = 39\ttrain_loss=1.16604\ttest_loss=1.18269\ttrain_acc=0.38020\ttest_acc=0.42591\ttest_prec=0.15933\ttest_rec=0.57839\ttest_f=0.24984\n",
            "epoch = 40\ttrain_loss=1.16222\ttest_loss=1.18431\ttrain_acc=0.38691\ttest_acc=0.42440\ttest_prec=0.15807\ttest_rec=0.57382\ttest_f=0.24786\n",
            "epoch = 41\ttrain_loss=1.16424\ttest_loss=1.18155\ttrain_acc=0.60856\ttest_acc=0.58616\ttest_prec=0.18931\ttest_rec=0.45814\ttest_f=0.26791\n",
            "epoch = 42\ttrain_loss=1.16555\ttest_loss=1.18278\ttrain_acc=0.50906\ttest_acc=0.57560\ttest_prec=0.17610\ttest_rec=0.42618\ttest_f=0.24922\n",
            "epoch = 43\ttrain_loss=1.16209\ttest_loss=1.18166\ttrain_acc=0.40570\ttest_acc=0.42440\ttest_prec=0.15807\ttest_rec=0.57382\ttest_f=0.24786\n",
            "epoch = 44\ttrain_loss=1.16227\ttest_loss=1.18278\ttrain_acc=0.47047\ttest_acc=0.58616\ttest_prec=0.18931\ttest_rec=0.45814\ttest_f=0.26791\n",
            "epoch = 45\ttrain_loss=1.16100\ttest_loss=1.18285\ttrain_acc=0.54765\ttest_acc=0.58616\ttest_prec=0.18931\ttest_rec=0.45814\ttest_f=0.26791\n",
            "epoch = 46\ttrain_loss=1.16212\ttest_loss=1.18254\ttrain_acc=0.44329\ttest_acc=0.58616\ttest_prec=0.18931\ttest_rec=0.45814\ttest_f=0.26791\n",
            "epoch = 47\ttrain_loss=1.16214\ttest_loss=1.18372\ttrain_acc=0.56812\ttest_acc=0.58616\ttest_prec=0.18931\ttest_rec=0.45814\ttest_f=0.26791\n",
            "epoch = 48\ttrain_loss=1.16706\ttest_loss=1.18570\ttrain_acc=0.43054\ttest_acc=0.73509\ttest_prec=0.25250\ttest_rec=0.30746\ttest_f=0.27728\n",
            "epoch = 49\ttrain_loss=1.16379\ttest_loss=1.18265\ttrain_acc=0.61913\ttest_acc=0.57409\ttest_prec=0.17421\ttest_rec=0.42161\ttest_f=0.24655\n"
          ]
        }
      ]
    }
  ]
}