{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QrywNfRlazm-"
      },
      "outputs": [],
      "source": [
        "! git clone https://git.wur.nl/bioinformatics/grs34806-deep-learning-project-data.git -q\n",
        "! git clone https://github.com/maussn/GRS34806-project.git -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from datetime import datetime\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import random\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "3ctb6CE_bWXP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(Path('grs34806-deep-learning-project-data'))"
      ],
      "metadata": {
        "id": "SEpH6j4dbJuO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read(seqfile: str, posfile: str) -> tuple[list, list]:\n",
        "    \"\"\"Read file with sequences and file with positive cases into lists.\n",
        "\n",
        "    :param seqfile: file with sequences\n",
        "    :type seqfile: str\n",
        "    :param posfile: file with positive cases (annotated with function)\n",
        "    :type posfile: str\n",
        "    :return: two lists, first with sequences and second with boolean labels\n",
        "    :rtype: tuple[list, list]\n",
        "    \"\"\"\n",
        "    idlist = []\n",
        "    datalist = []\n",
        "    labellist = []\n",
        "    with open(seqfile, 'r') as f:\n",
        "        for line in f.readlines():\n",
        "            line = line.rstrip().split('\\t')\n",
        "            idlist.append(line[0])\n",
        "            datalist.append(line[1])\n",
        "            labellist.append(False)\n",
        "    with open(posfile, 'r') as f:\n",
        "        for line in f.readlines():\n",
        "            id = line.rstrip()\n",
        "            try:\n",
        "                i = idlist.index(id)\n",
        "                labellist[i] = True\n",
        "            except ValueError:\n",
        "                continue\n",
        "    return datalist, labellist\n",
        "\n",
        "\n",
        "def split_labelled(datalist: list, labellist: list):\n",
        "    pos_datalist = []\n",
        "    neg_datalist = []\n",
        "    for i, label in enumerate(labellist):\n",
        "        if label:\n",
        "            pos_datalist.append(datalist[i])\n",
        "        else:\n",
        "            neg_datalist.append(datalist[i])\n",
        "    return pos_datalist, neg_datalist\n",
        "\n",
        "\n",
        "def remove_sequences(datalist: list, fraction=0.5):\n",
        "    random.shuffle(datalist)\n",
        "    i = round(len(datalist) * fraction)\n",
        "    return datalist[:i]\n",
        "\n",
        "\n",
        "def fuse_sequence_lists(pos_datalist: list, neg_datalist: list):\n",
        "    pos_labels = [True for _ in pos_datalist]\n",
        "    neg_labels = [False for _ in neg_datalist]\n",
        "    datalist = pos_datalist + neg_datalist\n",
        "    labellist = pos_labels + neg_labels\n",
        "    return datalist, labellist\n",
        "\n",
        "\n",
        "def generate_train_test(datalist: list, labellist: list, fraction: float=0.8):\n",
        "    \"\"\"Split up dataset in training set and test set\n",
        "\n",
        "    :param datalist: list with sequences\n",
        "    :type datalist: list\n",
        "    :param labellist: list with labels\n",
        "    :type labellist: list\n",
        "    :param ratio: fraction to be added to the training set, remainder is added to the test set, defaults to 0.8\n",
        "    :type ratio: float, optional\n",
        "    :return: four lists, first two the training data and labels, second two the test data and labels\n",
        "    :rtype: tuple[list, list, list, list]\n",
        "    \"\"\"\n",
        "    c = list(zip(datalist, labellist))\n",
        "    random.shuffle(c)\n",
        "    datalist[:], labellist[:] = zip(*c)\n",
        "    i = round(len(datalist) * fraction)\n",
        "    traindatalist = datalist[:i]\n",
        "    trainlabellist = labellist[:i]\n",
        "    testdatalist = datalist[i:]\n",
        "    testlabellist = labellist[i:]\n",
        "    return traindatalist, trainlabellist,testdatalist,testlabellist\n",
        "\n",
        "\n",
        "def tokenize(data: list, map2num: dict, non_aa_num: int=20) -> list:\n",
        "    \"\"\"Tokenize all sequences in a list\n",
        "\n",
        "    :param data: list of sequences to tokenize\n",
        "    :type data: list\n",
        "    :param map2num: ammino acid -> integer token mapping\n",
        "    :type map2num: dict\n",
        "    :param non_aa_num: token for non amino acid characters, defaults to 20\n",
        "    :type non_aa_num: int, optional\n",
        "    :return: list of tokenized sequences\n",
        "    :rtype: list\n",
        "    \"\"\"\n",
        "    seq = []\n",
        "    for count, i in enumerate(data):\n",
        "        seq.append([map2num.get(j,non_aa_num) for j in list(i)])\n",
        "    return seq\n",
        "\n",
        "\n",
        "def truncate_pad(line: list, num_steps: int, padding_token: int) -> list:\n",
        "    \"\"\"Truncate or pad a tokenized sequence\n",
        "\n",
        "    :param line: tokenized sequence\n",
        "    :type line: list\n",
        "    :param num_steps: maximum sequence length\n",
        "    :type num_steps: int\n",
        "    :param padding_token: token to be used for padding\n",
        "    :type padding_token: int\n",
        "    :return: truncated/padded sequence\n",
        "    :rtype: list\n",
        "    \"\"\"\n",
        "    if len(line) > num_steps:\n",
        "        return line[:num_steps] # Truncate\n",
        "    return line + [padding_token] * (num_steps - len(line)) # Pad\n",
        "\n",
        "\n",
        "def build_seq_array(lines: list, num_steps: int, non_aa_num: int=20) -> torch.tensor:\n",
        "    \"\"\"Truncate or pad tokenized sequences and convert to tensor\n",
        "\n",
        "    :param lines: tokenized sequences\n",
        "    :type lines: list\n",
        "    :param num_steps: maximum sequence length\n",
        "    :type num_steps: int\n",
        "    :param non_aa_num: token for padding, defaults to 20\n",
        "    :type non_aa_num: int, optional\n",
        "    :return: tensor with truncated/padded tokenized sequences\n",
        "    :rtype: torch.tensor\n",
        "    \"\"\"\n",
        "    return torch.tensor([truncate_pad(l, num_steps, non_aa_num) for l in lines], dtype=torch.long)\n",
        "\n",
        "\n",
        "def load_array(data_arrays: tuple[torch.tensor, torch.tensor], batch_size: int, is_train: bool=True) -> torch.utils.data.DataLoader:\n",
        "    \"\"\"Construct a PyTorch data iterator.\n",
        "\n",
        "    Taken from d2l package\"\"\"\n",
        "    dataset = torch.utils.data.TensorDataset(*data_arrays)\n",
        "    return torch.utils.data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
        "\n",
        "\n",
        "def load_data(batch_size: int, num_steps: int, dataset: tuple[list, list]) -> torch.utils.data.DataLoader:\n",
        "    \"\"\"Tokenize sequence/label dataset and load into dataloader.\n",
        "\n",
        "    :param batch_size: size of each batch\n",
        "    :type batch_size: int\n",
        "    :param num_steps: maximum sequence length\n",
        "    :type num_steps: int\n",
        "    :param dataset: first list contains sequences, second labels\n",
        "    :type dataset: tuple[list, list]\n",
        "    :return: torch dataloader which gives a tensor of sequences in a batch and a tensor with their labels\n",
        "    :rtype: torch.utils.data.DataLoader\n",
        "    \"\"\"\n",
        "    mapaa2num = {aa: i for (i, aa) in enumerate(list(\"ACDEFGHIKLMNPQRSTVWY\"))}\n",
        "    seq,lab = dataset\n",
        "    seq = tokenize(seq, mapaa2num)\n",
        "    seq_array = build_seq_array(seq, num_steps)\n",
        "    data_arrays = (seq_array, torch.tensor(lab, dtype=torch.long))\n",
        "    data_iter = load_array(data_arrays, batch_size)\n",
        "    return data_iter"
      ],
      "metadata": {
        "id": "eHpedW0hqx1V"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 10\n",
        "num_steps = 500\n",
        "\n",
        "# Example for one of the simulated datasets\n",
        "datalist, labellist = read(\"len200_500_n5000nr4.seq\", \"len200_500_n5000nr4.pos\")\n",
        "\n",
        "# # Remove negatives\n",
        "# pos_datalist, neg_datalist = split_labelled(datalist, labellist)\n",
        "# neg_datalist = remove_sequences(neg_datalist, 0.5)\n",
        "# datalist, labellist = fuse_sequence_lists(pos_datalist, neg_datalist)\n",
        "\n",
        "traindatalist, trainlabellist, testdatalist, testlabellist = generate_train_test(datalist, labellist, 0.8)\n",
        "traindataset = [traindatalist, trainlabellist]\n",
        "testdataset = [testdatalist, testlabellist]\n",
        "\n",
        "# Set batch_size and num_steps (maximum sequence length)\n",
        "train_iter = load_data(batch_size, num_steps, traindataset)\n",
        "test_iter = load_data(batch_size, num_steps, testdataset)\n",
        "\n",
        "print(next(iter(train_iter)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyRvhoyiwv_g",
        "outputId": "5e6f500a-55bf-42f8-9e4d-80790d6bd8f6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[10,  3,  1,  ..., 20, 20, 20],\n",
            "        [10,  9, 13,  ..., 20, 20, 20],\n",
            "        [10,  4, 10,  ..., 20, 20, 20],\n",
            "        ...,\n",
            "        [10, 10,  9,  ..., 20, 20, 20],\n",
            "        [10, 10, 14,  ..., 20, 20, 20],\n",
            "        [10,  2, 12,  ..., 20, 20, 20]]), tensor([1, 1, 0, 0, 1, 0, 1, 1, 1, 1])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len_list = []\n",
        "for s in datalist:\n",
        "    len_list.append(len(s))\n",
        "sns.histplot(len_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "26FSv9_pGhWY",
        "outputId": "9e625811-5808-4453-e97c-f4aeff874ce8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: ylabel='Count'>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p = 0\n",
        "n = 0\n",
        "for l in labellist:\n",
        "    if l:\n",
        "        p += 1\n",
        "    else:\n",
        "        n+=1\n",
        "print(f'{p = }\\n{n = }')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hMgfQJIFjb5",
        "outputId": "b5ef4ddf-0ff5-4e39-b542-f617882f500e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p = 2490\n",
            "n = 2510\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(layer):\n",
        "    if type(layer) == nn.Linear or type(layer) == nn.Conv1d:\n",
        "        nn.init.xavier_uniform_(layer.weight)\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model, loss_fn, optimizer):\n",
        "        self.model = model\n",
        "        self.loss_fn = loss_fn\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "\n",
        "    def _train_one_epoch(self, epoch_index, train_iter):\n",
        "        result_loss = 0\n",
        "        for i, (inputs, labels) in enumerate(train_iter):\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            outputs = self.model(inputs)\n",
        "\n",
        "            loss = self.loss_fn(outputs, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            self.optimizer.step()\n",
        "            result_loss += loss.item()\n",
        "        return result_loss / (i + 1)\n",
        "\n",
        "\n",
        "    def train(self, epochs, train_iter, test_iter):\n",
        "        for epoch in range(epochs):\n",
        "            self.model.train(True)\n",
        "            train_loss = self._train_one_epoch(epoch, train_iter)\n",
        "            self.model.eval()\n",
        "            result_loss = 0\n",
        "            with torch.no_grad():\n",
        "                for i, (test_inputs, test_labels) in enumerate(test_iter):\n",
        "                    test_outputs = self.model(test_inputs)\n",
        "                    loss = self.loss_fn(test_outputs, test_labels)\n",
        "                    # print(f'{loss = }\\t{test_outputs = }\\t{test_labels = }')\n",
        "                    result_loss += loss.item()\n",
        "            test_loss = result_loss / (i + 1)\n",
        "            print(f'{epoch = }\\t{train_loss=:.5f}\\t{test_loss=:.5f}')\n",
        "\n"
      ],
      "metadata": {
        "id": "3Btdmdhj36zM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MinimalGOClassifierCNN(nn.Module):\n",
        "    def __init__(self, input_length: int, vocab_size : int=21,  num_filters: int=32, kernel_size: int=5):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, vocab_size)\n",
        "        self.conv_layer = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=vocab_size, out_channels=num_filters, kernel_size=kernel_size),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool1d(1)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LazyLinear(out_features=2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.conv_layer(x.transpose(1,2))\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        output = F.softmax(x, 1)\n",
        "        return output"
      ],
      "metadata": {
        "id": "6Agj87Dpbf-Y"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BerryCNN1D(nn.Module):\n",
        "    \"\"\"1D Convolutional Neural Network for protein function classification\"\"\"\n",
        "    def __init__(self, context_size: int, vocab_size: int = 21,  conv_channels: int = 128, use_bias: bool = False):\n",
        "        super().__init__()\n",
        "        assert context_size % 2 == 0, f'Invalid context_size, {context_size} is not an even number'\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "        # CNN model for binary classification\n",
        "        self.conv1 = nn.Sequential(\n",
        "            # conv block 1\n",
        "            nn.Conv1d(in_channels=vocab_size, out_channels=conv_channels, kernel_size=3, padding='same', bias=use_bias),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            # conv block 2\n",
        "            nn.Conv1d(in_channels=conv_channels, out_channels=conv_channels, kernel_size=3, padding='same', bias=use_bias),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveMaxPool1d(1)\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            # flatten + classification head\n",
        "            nn.Flatten(1, -1),\n",
        "            nn.LazyLinear(out_features=2, bias=use_bias)  # binary classification\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.tensor, targets: torch.tensor = None):\n",
        "        \"\"\"Predict protein function class (0 or 1)\"\"\"\n",
        "        x = self.embedding(x).permute(0,2,1)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        return self.fc(x)\n"
      ],
      "metadata": {
        "id": "2V5v63DoLaLr"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MoreCNN1D(nn.Module):\n",
        "    \"\"\"1D Convolutional Neural Network for protein function classification\"\"\"\n",
        "    def __init__(self, context_size: int, vocab_size: int = 21,  conv_channels: int = 128, use_bias: bool = False):\n",
        "        super().__init__()\n",
        "        assert context_size % 2 == 0, f'Invalid context_size, {context_size} is not an even number'\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "        # CNN model for binary classification\n",
        "        self.conv1 = nn.Sequential(\n",
        "            # conv block 1\n",
        "            nn.Conv1d(in_channels=vocab_size, out_channels=conv_channels, kernel_size=3, padding='same', bias=use_bias),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            # conv block 2\n",
        "            nn.Conv1d(in_channels=conv_channels, out_channels=conv_channels, kernel_size=3, padding='same', bias=use_bias),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            # conv block 3\n",
        "            nn.Conv1d(in_channels=conv_channels, out_channels=conv_channels, kernel_size=3, padding='same', bias=use_bias),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveMaxPool1d(1)\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            # flatten + classification head\n",
        "            nn.Flatten(1, -1),\n",
        "            nn.LazyLinear(out_features=2, bias=use_bias)  # binary classification\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.tensor, targets: torch.tensor = None):\n",
        "        \"\"\"Predict protein function class (0 or 1)\"\"\"\n",
        "        x = self.embedding(x).permute(0,2,1)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        return self.fc(x)"
      ],
      "metadata": {
        "id": "M6H_ib65yCPk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MoreCNN1D(context_size=num_steps, conv_channels=128)\n",
        "model.apply(init_weights)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBZC4GHeLUHz",
        "outputId": "ab9f72a7-72cf-4ffe-facc-acaf46fcaad8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MoreCNN1D(\n",
              "  (embedding): Embedding(21, 21)\n",
              "  (conv1): Sequential(\n",
              "    (0): Conv1d(21, 128, kernel_size=(3,), stride=(1,), padding=same, bias=False)\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (conv2): Sequential(\n",
              "    (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=same, bias=False)\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (conv3): Sequential(\n",
              "    (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=same, bias=False)\n",
              "    (1): ReLU()\n",
              "    (2): AdaptiveMaxPool1d(output_size=1)\n",
              "  )\n",
              "  (fc): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): LazyLinear(in_features=0, out_features=2, bias=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(model, loss_fn, optimizer)\n",
        "trainer.train(epochs=10, train_iter=train_iter, test_iter=test_iter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fh2DuxL8e7n",
        "outputId": "e4ee91bd-9abf-4661-c9eb-d4adda72fc6c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch = 0\ttrain_loss=0.70277\ttest_loss=0.72962\n",
            "epoch = 1\ttrain_loss=0.68949\ttest_loss=0.69663\n",
            "epoch = 2\ttrain_loss=0.68289\ttest_loss=0.69551\n",
            "epoch = 3\ttrain_loss=0.66721\ttest_loss=0.69801\n",
            "epoch = 4\ttrain_loss=0.53815\ttest_loss=0.28768\n",
            "epoch = 5\ttrain_loss=0.09282\ttest_loss=0.03446\n",
            "epoch = 6\ttrain_loss=0.01652\ttest_loss=0.01157\n",
            "epoch = 7\ttrain_loss=0.00750\ttest_loss=0.00651\n",
            "epoch = 8\ttrain_loss=0.00431\ttest_loss=0.00464\n",
            "epoch = 9\ttrain_loss=0.00280\ttest_loss=0.00369\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_iter))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G57lnNJ2xKmf",
        "outputId": "537f85a2-3a64-4041-e976-ab2179a4e098"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "160\n"
          ]
        }
      ]
    }
  ]
}